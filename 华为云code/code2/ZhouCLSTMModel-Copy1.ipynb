{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport os\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model/freenli/model\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\nsplit_info = {\n    \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\n\ndef dataset_split(info):\n    if info:\n        [num, pi] = info\n        train_data = [[] for i in range(num)]\n        with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                train_data[index].append([content,label])\n\n        for i in range(num):\n            np.random.shuffle(train_data[i])\n            train_data[i] = np.asarray(train_data[i])\n\n        np.random.shuffle(train_data)   \n        return train_data\n    \n    \n    train_data = []\n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data)\n\n\ndef mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(train_data, split_type):\n    \n    print(split_type)\n    \n    train_acc = []\n    test_acc = []\n    fold_id = 0\n    \n    if split_type != \"random\":\n        tx = []\n        ty = []\n        for ti in train_data:\n            x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            tx.append(x_train)\n            ty.append(y_train)\n\n        tx = np.asarray(tx)\n        ty = np.asarray(ty)\n\n        print(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n        \n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_x, train_y = mergeData(tx[train_i],ty[train_i])\n            test_x, test_y = mergeData(tx[test_i],ty[test_i])\n            train_acc.append(model_train(train_x, train_y,split_type,fold_id))\n            test_acc.append(model_evaluate(test_x, test_y,split_type,fold_id,categories))\n        \n    else:\n        tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n        print(len(tx),len(tx[0]),len(tx[1]))\n\n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_acc.append(model_train(tx[train_i], ty[train_i],split_type,fold_id))\n            test_acc.append(model_evaluate(tx[test_i], ty[test_i],split_type,fold_id,categories))\n        \n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class ZhouCLSTMModel:\n    '''\n    Implementation proposal of: https://arxiv.org/pdf/1511.08630\n    '''\n    def __init__(self, embedding,\n        conv_size    = 3,\n        conv_filters = 300,\n        drop_rate    = 0.5,\n        lstm_units   = 150):\n        '''Constructor.\n        # Parameters:\n        conv_size: Size of the convolutions. Number of words that takes each\n            convolution step.\n        conv_filters: Number of convolution filters.\n        drop_rate: Drop rate for the final output of the LSTM layer.\n        lstm_units: Size of the states of the LSTM layer.\n        '''\n        self._embedding    = embedding\n        self._conv_size    = conv_size\n        self._conv_filters = conv_filters\n        self._drop_rate    = drop_rate\n        self._lstm_units   = lstm_units\n\n    def __call__(self, x_input):\n        self._embedding_tf = self._create_embedding_layer(\n            self._embedding, x_input)\n        \n        self._convolution_tf = self._create_convolutional_layers(\n            self._conv_size,\n            self._conv_filters,\n            self._drop_rate,\n            self._embedding_tf)\n        \n        self._lstm_tf = self._create_lstm_layer(\n            self._lstm_units,\n            self._convolution_tf)\n        \n        return self._lstm_tf\n\n    def summary(self):\n        print('embedding:', str(self._embedding_tf.shape))\n        print('conv:', str(self._convolution_tf.shape))\n        print('lstm:', str(self._lstm_tf.shape))\n\n    def _create_embedding_layer(self, embedding, input_x):\n        embedding = tf.Variable(initial_value=embedding)\n\n        embedded_chars = tf.nn.embedding_lookup(\n            embedding, tf.cast(input_x, 'int32'))\n\n        return embedded_chars\n\n    def _create_convolutional_layers(self,\n        conv_size, num_filters, drop_rate, embedding):\n        \n        filter_height = conv_size\n        filter_width = embedding.shape[2].value\n\n        filter_shape = [filter_height, filter_width, 1, num_filters]\n\n        W = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=filter_shape,\n                stddev=0.1))\n        b = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=[num_filters]))\n\n        embedding_expanded = tf.expand_dims(embedding, -1)\n        conv = tf.nn.conv2d(\n            input=embedding_expanded,\n            filter=W,\n            strides=[1,1,1,1],\n            padding='VALID')\n        conv_reduced = tf.reshape(\n            tensor=conv,\n            shape=[-1, conv.shape[1], conv.shape[3]])\n\n        bias = tf.nn.bias_add(conv_reduced, b)\n        c = tf.nn.relu(bias)\n\n        d = tf.nn.dropout(c, keep_prob=drop_rate)\n        return d\n\n    def _create_lstm_layer(self, lstm_units, conv_input):\n        lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_units)\n        sequence = tf.unstack(conv_input, axis=1)\n        (_, (h, _)) = tf.nn.static_rnn(lstm_cell, sequence, dtype=tf.float32)\n\n        return h", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "contents_all = contents_train + contents_dev + contents_test\nseq_length = 0\nfor content in contents_all:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 35"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length = 41", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocab, embd = loadGloVe(vocabPath, 100)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\n# embedding = np.asarray(embd)\nembedding = embd", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id = dict(zip(vocab, range(vocab_size)))\nlen(embedding),embedding_dim,vocab_size", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "(400000, 100, 400000)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "learning_rate = 1e-3\ndropout_keep_prob = 0.5\n\n# \u8f93\u5165\u5185\u5bb9\u53ca\u5bf9\u5e94\u7684\u6807\u7b7e\ninput_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\ninput_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\nkeep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\nmodel = ZhouCLSTMModel(embedding, drop_rate = keep_prob)\nfc = model(input_x)\nmodel.summary()\n\n# \u5206\u7c7b\u5668\nlogits = tf.layers.dense(fc, num_classes, name='fc2')\ny_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n\n# \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\nloss = tf.reduce_mean(cross_entropy)\n# \u4f18\u5316\u5668\noptim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n# \u51c6\u786e\u7387\ncorrect_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\nacc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n\nsaver = tf.train.Saver()", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "embedding: (?, 41, 100)\nconv: (?, 39, 300)\nlstm: (?, 150)\nWARNING:tensorflow:From <ipython-input-9-0b90efce3c4c>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-9-0b90efce3c4c>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {input_x: x_batch1, input_y: y_batch1, keep_prob: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len", "execution_count": 12, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data = dataset_split(split_info[\"random\"])", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(train_data), len(train_data[0])", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "(14035, 2)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "x_train, y_train = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)", "execution_count": 16, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(x_train)", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "14035"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u521b\u5efasession\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\n\nprint('Training and evaluating...')\nstart_time = time.time()\ntotal_batch = 0  # \u603b\u6279\u6b21\nbest_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\nlast_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\nnum_epochs = 50\nbatch_size = 64\nrequire_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\nflag = False\n\nfor epoch in range(num_epochs):  # 20\n    print('Epoch:', epoch + 1)\n    batch_train = batch_iter(x_train, y_train, batch_size)\n    for x_batch, y_batch in batch_train:\n        feed_dict = {input_x: x_batch, input_y: y_batch, keep_prob: dropout_keep_prob}\n        session.run(optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n        total_batch += 1\n\n        if total_batch % print_per_batch == 0:\n            # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n            feed_dict[keep_prob] = 1.0\n            loss_train, acc_train = session.run([loss, acc], feed_dict=feed_dict)\n            # loss_val, acc_val = evaluate(session, x_dev, y_dev, loss, acc)\n            if acc_train > best_acc_train:\n                # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                best_acc_train = acc_train\n                last_improved = total_batch\n                saver.save(sess=session, save_path=savePath)\n                improved_str = '*'\n            else:\n                improved_str = ''\n\n            time_dif = get_time_dif(start_time)\n            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, Time: {3} {4}'\n            print(msg.format(total_batch, loss_train, acc_train, time_dif, improved_str))\n\n        if total_batch - last_improved > require_improvement:\n            # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n            print(\"No optimization for a long time, auto-stopping...\")\n            flag = True\n            break  # \u8df3\u51fa\u5faa\u73af\n    if flag:  # \u540c\u4e0a\n        break\n\nsession.close()", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    2.0, Train Acc:  43.75%, Time: 0:00:14 *\nIter:     60, Train Loss:    1.7, Train Acc:  40.62%, Time: 0:00:17 \nIter:     90, Train Loss:    1.4, Train Acc:  46.88%, Time: 0:00:29 *\nIter:    120, Train Loss:    1.3, Train Acc:  51.56%, Time: 0:00:44 *\nIter:    150, Train Loss:   0.77, Train Acc:  73.44%, Time: 0:00:54 *\nIter:    180, Train Loss:   0.79, Train Acc:  70.31%, Time: 0:00:58 \nIter:    210, Train Loss:   0.74, Train Acc:  73.44%, Time: 0:01:02 \nEpoch: 2\nIter:    240, Train Loss:   0.36, Train Acc:  87.50%, Time: 0:01:13 *\nIter:    270, Train Loss:   0.57, Train Acc:  81.25%, Time: 0:01:17 \nIter:    300, Train Loss:   0.35, Train Acc:  85.94%, Time: 0:01:20 \nIter:    330, Train Loss:    0.4, Train Acc:  89.06%, Time: 0:01:36 *\nIter:    360, Train Loss:   0.31, Train Acc:  89.06%, Time: 0:01:39 \nIter:    390, Train Loss:   0.33, Train Acc:  89.06%, Time: 0:01:43 \nIter:    420, Train Loss:   0.47, Train Acc:  84.38%, Time: 0:01:47 \nEpoch: 3\nIter:    450, Train Loss:   0.15, Train Acc:  96.88%, Time: 0:01:59 *\nIter:    480, Train Loss:  0.071, Train Acc:  98.44%, Time: 0:02:13 *\nIter:    510, Train Loss:   0.17, Train Acc:  95.31%, Time: 0:02:16 \nIter:    540, Train Loss:   0.16, Train Acc:  96.88%, Time: 0:02:20 \nIter:    570, Train Loss:   0.16, Train Acc:  93.75%, Time: 0:02:24 \nIter:    600, Train Loss:   0.24, Train Acc:  92.19%, Time: 0:02:28 \nIter:    630, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:02:31 \nIter:    660, Train Loss:    0.1, Train Acc: 100.00%, Time: 0:02:45 *\nEpoch: 4\nIter:    690, Train Loss:  0.044, Train Acc: 100.00%, Time: 0:02:49 \nIter:    720, Train Loss:    0.3, Train Acc:  92.19%, Time: 0:02:53 \nIter:    750, Train Loss:  0.082, Train Acc:  98.44%, Time: 0:02:56 \nIter:    780, Train Loss:  0.079, Train Acc:  98.44%, Time: 0:03:00 \nIter:    810, Train Loss:   0.11, Train Acc:  95.31%, Time: 0:03:04 \nIter:    840, Train Loss:  0.035, Train Acc: 100.00%, Time: 0:03:08 \nIter:    870, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:03:11 \nEpoch: 5\nIter:    900, Train Loss:  0.024, Train Acc: 100.00%, Time: 0:03:15 \nIter:    930, Train Loss:  0.026, Train Acc: 100.00%, Time: 0:03:19 \nIter:    960, Train Loss:  0.063, Train Acc:  98.44%, Time: 0:03:22 \nIter:    990, Train Loss:  0.056, Train Acc:  98.44%, Time: 0:03:26 \nIter:   1020, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:03:30 \nIter:   1050, Train Loss:  0.028, Train Acc: 100.00%, Time: 0:03:34 \nIter:   1080, Train Loss:   0.02, Train Acc: 100.00%, Time: 0:03:37 \nEpoch: 6\nIter:   1110, Train Loss:  0.041, Train Acc:  98.44%, Time: 0:03:41 \nIter:   1140, Train Loss:   0.03, Train Acc: 100.00%, Time: 0:03:45 \nNo optimization for a long time, auto-stopping...\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def preprocess_sentence(sent):\n    new_sent = ''\n    for i in range(len(sent)):\n        if sent[i] in string.punctuation:\n            if i > 0 and i < len(sent) - 1:\n                if sent[i] in \",.\" and sent[i-1].isdigit() and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and (sent[i-1].isdigit() or sent[i+1].isdigit()):\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                elif sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            elif i == 0:\n                if sent[i] == \"$\" and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            else:\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                else:\n                    new_sent += sent[i]\n        else:\n            new_sent += sent[i]\n    return new_sent.strip().lower()\n\n\ndef predict11(predict_sentences, probability_threshold=0.3):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for psi in predict_sentences:\n\n        data_id.append([word_to_id[x] for x in preprocess_sentence(psi).split() if x in word_to_id])\n\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, seq_length)\n    feed_dict = {\n        input_x: x_pad,\n        keep_prob: 1.0\n    }\n    predict_result = session.run(tf.nn.softmax(logits), feed_dict=feed_dict)\n    print(predict_result)\n    result = []\n    for i in predict_result:\n        if max(i) > probability_threshold:\n            result.append(i.argmax()+1)\n        else:\n            result.append(0)\n    return result", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import string", "execution_count": 22, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "session = tf.Session()\nsaver.restore(sess=session, save_path=savePath)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/freenli/model\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/freenli/model\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "sentences = [\"can you tell me what is arkansas 's population on the date july 1st of 2002 ?\",\n\"show the way the number of likes were distributed .\", \"is it true that people living on average depends on higher gdp of a country\"]  # 1,7,10\n\npredict11(sentences)", "execution_count": 40, "outputs": [{"output_type": "stream", "text": "[[9.9565226e-01 3.8410417e-05 3.1449313e-03 7.6289487e-04 3.6055729e-04\n  9.4450970e-06 1.7177252e-05 5.7368757e-06 1.2711318e-07 8.3821969e-06]\n [4.2775994e-07 2.0520065e-06 6.1646038e-05 4.1338234e-07 1.9917552e-07\n  3.5080964e-05 9.9974602e-01 4.9381674e-07 1.3718965e-04 1.6582457e-05]\n [1.4377131e-05 1.9067187e-04 2.9857219e-03 6.4738197e-06 2.6175562e-06\n  6.4397642e-07 2.5312585e-04 9.0113713e-04 2.2385353e-05 9.9562281e-01]]\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 40, "data": {"text/plain": "[1, 7, 10]"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}