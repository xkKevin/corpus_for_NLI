{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_hf.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model/ad_biLSTM2/lstm_model\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def dataset_split(trainDataPath):\n\n    train_data = []\n    seq_length = 0\n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n            \n            if len(content) > seq_length:\n                seq_length = len(content)\n            \n    np.random.shuffle(train_data)\n    return np.asarray(train_data), seq_length\n\n\ndef loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\ntrain_data, seq_length = dataset_split(trainDataPath)\nx_train, y_train = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n\nprint(len(embedding),embedding_dim,vocab_size, seq_length)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000 41\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    \n    saver = tf.train.Saver()\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 200  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    print_per_batch = 100\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                # feed_dict[model.dropoutKeepProb] = 1.0\n                # loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                \n                loss_train, acc_train = evaluate(session, model, x_train, y_train, model.loss, model.acc, batch_size)\n                \n                if acc_train > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_train\n                    last_improved = total_batch\n                    improved_str = '*'\n                    \n                    if best_acc_train > 0.9:\n                        saver.save(sess=session, save_path=savePath)\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, duration, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                train_data_len = len(x_train)\n                train_num_batch = int((train_data_len - 1) / batch_size) + 1\n\n                y_train_cls = np.argmax(y_train, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_train_pred_cls = np.zeros(shape=len(x_train), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(train_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, train_data_len)\n                    feed_dict = {\n                        model.inputX: x_train[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_train_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_train_cls, y_train_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_train_cls, y_train_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_train_cls, y_train_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            wordEmbedding = tf.Variable(initial_value=wordEmbedding)\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nlstm = AdversarailLSTM(embedding)", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-6-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:128\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_train(lstm, x_train, y_train, categories)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.929, Train Acc: 53.89%, Time: 28.94s *\nIter: 200, Train Loss:  1.748, Train Acc: 73.14%, Time: 56.43s *\nEpoch: 2\nIter: 300, Train Loss:  1.337, Train Acc: 79.07%, Time: 24.64s *\nIter: 400, Train Loss: 0.9882, Train Acc: 85.27%, Time: 52.17s *\nEpoch: 3\nIter: 500, Train Loss: 0.7826, Train Acc: 88.14%, Time: 21.94s *\nIter: 600, Train Loss: 0.6662, Train Acc: 90.35%, Time: 66.12s *\nEpoch: 4\nIter: 700, Train Loss: 0.5454, Train Acc: 92.23%, Time: 39.04s *\nIter: 800, Train Loss: 0.4667, Train Acc: 93.35%, Time: 98.81s *\nEpoch: 5\nIter: 900, Train Loss: 0.4527, Train Acc: 93.44%, Time: 28.79s *\nIter: 1000, Train Loss: 0.2972, Train Acc: 96.15%, Time: 76.83s *\nIter: 1100, Train Loss: 0.3076, Train Acc: 95.75%, Time: 104.19s \nEpoch: 6\nIter: 1200, Train Loss: 0.2374, Train Acc: 96.76%, Time: 44.35s *\nIter: 1300, Train Loss: 0.2157, Train Acc: 97.06%, Time: 93.98s *\nEpoch: 7\nIter: 1400, Train Loss: 0.2283, Train Acc: 96.94%, Time: 24.49s \nIter: 1500, Train Loss: 0.1464, Train Acc: 98.05%, Time: 84.14s *\nEpoch: 8\nIter: 1600, Train Loss: 0.1423, Train Acc: 98.15%, Time: 51.26s *\nIter: 1700, Train Loss: 0.1508, Train Acc: 97.94%, Time: 78.56s \nEpoch: 9\nIter: 1800, Train Loss: 0.09688, Train Acc: 98.85%, Time: 40.49s *\nIter: 1900, Train Loss: 0.09836, Train Acc: 98.86%, Time: 84.62s *\nEpoch: 10\nIter: 2000, Train Loss: 0.09229, Train Acc: 98.68%, Time: 16.13s \nIter: 2100, Train Loss: 0.08614, Train Acc: 98.99%, Time: 72.90s *\nIter: 2200, Train Loss: 0.0881, Train Acc: 98.82%, Time: 100.26s \nEpoch: 11\n", "name": "stdout"}, {"output_type": "error", "ename": "InternalError", "evalue": ": Unable to connect to endpoint\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/_85, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_87, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_89, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/_91, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_93, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_95, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/_97, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_99, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_101, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/_103, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_105, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_107, Bi-LSTM/outputW/_109, Bi-LSTM/outputW/Adam/_111, Bi-LSTM/outputW/Adam_1/_113, beta1_power/_115, beta2_power/_117, globalStep, loss/Bi-LSTM/Attention/Variable/_119, loss/Bi-LSTM/Attention/Variable/Adam/_121, loss/Bi-LSTM/Attention/Variable/Adam_1/_123, loss/Bi-LSTM/output/outputB/_125, loss/Bi-LSTM/output/outputB/Adam/_127, loss/Bi-LSTM/output/outputB/Adam_1/_129, perturloss/Bi-LSTM/Attention/Variable/_131, perturloss/Bi-LSTM/Attention/Variable/Adam/_133, perturloss/Bi-LSTM/Attention/Variable/Adam_1/_135, perturloss/Bi-LSTM/output/outputB/_137, perturloss/Bi-LSTM/output/outputB/Adam/_139, perturloss/Bi-LSTM/output/outputB/Adam_1/_141, wordEmbedding/Variable/_143, wordEmbedding/Variable/Adam/_145, wordEmbedding/Variable/Adam_1/_147)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-fbcb6bdbf79a>\", line 1, in <module>\n    model_train(lstm, x_train, y_train, categories)\n  File \"<ipython-input-5-6470836b0686>\", line 41, in model_train\n    saver = tf.train.Saver()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 266, in save_op\n    tensors)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): : Unable to connect to endpoint\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/_85, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_87, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_89, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/_91, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_93, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_95, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/_97, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_99, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_101, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/_103, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_105, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_107, Bi-LSTM/outputW/_109, Bi-LSTM/outputW/Adam/_111, Bi-LSTM/outputW/Adam_1/_113, beta1_power/_115, beta2_power/_117, globalStep, loss/Bi-LSTM/Attention/Variable/_119, loss/Bi-LSTM/Attention/Variable/Adam/_121, loss/Bi-LSTM/Attention/Variable/Adam_1/_123, loss/Bi-LSTM/output/outputB/_125, loss/Bi-LSTM/output/outputB/Adam/_127, loss/Bi-LSTM/output/outputB/Adam_1/_129, perturloss/Bi-LSTM/Attention/Variable/_131, perturloss/Bi-LSTM/Attention/Variable/Adam/_133, perturloss/Bi-LSTM/Attention/Variable/Adam_1/_135, perturloss/Bi-LSTM/output/outputB/_137, perturloss/Bi-LSTM/output/outputB/Adam/_139, perturloss/Bi-LSTM/output/outputB/Adam_1/_141, wordEmbedding/Variable/_143, wordEmbedding/Variable/Adam/_145, wordEmbedding/Variable/Adam_1/_147)]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInternalError\u001b[0m: : Unable to connect to endpoint\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/_85, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_87, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_89, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/_91, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_93, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_95, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/_97, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_99, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_101, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/_103, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_105, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_107, Bi-LSTM/outputW/_109, Bi-LSTM/outputW/Adam/_111, Bi-LSTM/outputW/Adam_1/_113, beta1_power/_115, beta2_power/_117, globalStep, loss/Bi-LSTM/Attention/Variable/_119, loss/Bi-LSTM/Attention/Variable/Adam/_121, loss/Bi-LSTM/Attention/Variable/Adam_1/_123, loss/Bi-LSTM/output/outputB/_125, loss/Bi-LSTM/output/outputB/Adam/_127, loss/Bi-LSTM/output/outputB/Adam_1/_129, perturloss/Bi-LSTM/Attention/Variable/_131, perturloss/Bi-LSTM/Attention/Variable/Adam/_133, perturloss/Bi-LSTM/Attention/Variable/Adam_1/_135, perturloss/Bi-LSTM/output/outputB/_137, perturloss/Bi-LSTM/output/outputB/Adam/_139, perturloss/Bi-LSTM/output/outputB/Adam_1/_141, wordEmbedding/Variable/_143, wordEmbedding/Variable/Adam/_145, wordEmbedding/Variable/Adam_1/_147)]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m<ipython-input-8-fbcb6bdbf79a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-5-6470836b0686>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, categories)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbest_acc_train\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mimproved_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1702\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInternalError\u001b[0m: : Unable to connect to endpoint\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/_85, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_87, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_89, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/_91, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_93, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_95, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/_97, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_99, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_101, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/_103, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_105, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_107, Bi-LSTM/outputW/_109, Bi-LSTM/outputW/Adam/_111, Bi-LSTM/outputW/Adam_1/_113, beta1_power/_115, beta2_power/_117, globalStep, loss/Bi-LSTM/Attention/Variable/_119, loss/Bi-LSTM/Attention/Variable/Adam/_121, loss/Bi-LSTM/Attention/Variable/Adam_1/_123, loss/Bi-LSTM/output/outputB/_125, loss/Bi-LSTM/output/outputB/Adam/_127, loss/Bi-LSTM/output/outputB/Adam_1/_129, perturloss/Bi-LSTM/Attention/Variable/_131, perturloss/Bi-LSTM/Attention/Variable/Adam/_133, perturloss/Bi-LSTM/Attention/Variable/Adam_1/_135, perturloss/Bi-LSTM/output/outputB/_137, perturloss/Bi-LSTM/output/outputB/Adam/_139, perturloss/Bi-LSTM/output/outputB/Adam_1/_141, wordEmbedding/Variable/_143, wordEmbedding/Variable/Adam/_145, wordEmbedding/Variable/Adam_1/_147)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-fbcb6bdbf79a>\", line 1, in <module>\n    model_train(lstm, x_train, y_train, categories)\n  File \"<ipython-input-5-6470836b0686>\", line 41, in model_train\n    saver = tf.train.Saver()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 266, in save_op\n    tensors)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): : Unable to connect to endpoint\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/_85, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_87, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_89, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/_91, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_93, Bi-LSTM/bi-lstm0/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_95, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/_97, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_99, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_101, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/_103, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_105, Bi-LSTM/bi-lstm0/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_107, Bi-LSTM/outputW/_109, Bi-LSTM/outputW/Adam/_111, Bi-LSTM/outputW/Adam_1/_113, beta1_power/_115, beta2_power/_117, globalStep, loss/Bi-LSTM/Attention/Variable/_119, loss/Bi-LSTM/Attention/Variable/Adam/_121, loss/Bi-LSTM/Attention/Variable/Adam_1/_123, loss/Bi-LSTM/output/outputB/_125, loss/Bi-LSTM/output/outputB/Adam/_127, loss/Bi-LSTM/output/outputB/Adam_1/_129, perturloss/Bi-LSTM/Attention/Variable/_131, perturloss/Bi-LSTM/Attention/Variable/Adam/_133, perturloss/Bi-LSTM/Attention/Variable/Adam_1/_135, perturloss/Bi-LSTM/output/outputB/_137, perturloss/Bi-LSTM/output/outputB/Adam/_139, perturloss/Bi-LSTM/output/outputB/Adam_1/_141, wordEmbedding/Variable/_143, wordEmbedding/Variable/Adam/_145, wordEmbedding/Variable/Adam_1/_147)]]\n"]}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "with tf.Session() as session:\n    \n    session.run(tf.global_variables_initializer())\n    \n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=savePath)\n    \n    train_data_len = len(x_train)\n    train_num_batch = int((train_data_len - 1) / batch_size) + 1\n\n    y_train_cls = np.argmax(y_train, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_train_pred_cls = np.zeros(shape=len(x_train), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(train_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, train_data_len)\n        feed_dict = {\n            lstm.inputX: x_train[start_id:end_id],\n            lstm.dropoutKeepProb: 1.0\n        }\n        y_train_pred_cls[start_id:end_id] = session.run(lstm.y_pred_cls, feed_dict=feed_dict)\n\n    accuracy_score = metrics.accuracy_score(y_train_cls, y_train_pred_cls)\n    print(accuracy_score)\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_train_cls, y_train_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_train_cls, y_train_pred_cls)\n    print(cm)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM2/lstm_model\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM2/lstm_model\n", "name": "stderr"}, {"output_type": "error", "ename": "DataLossError", "evalue": "Checksum does not match: stored 999530730 vs. calculated on the restored bytes 2317565041\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save_1/RestoreV2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save_1/RestoreV2', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-7010d69d8e96>\", line 5, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Checksum does not match: stored 999530730 vs. calculated on the restored bytes 2317565041\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save_1/RestoreV2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mDataLossError\u001b[0m: Checksum does not match: stored 999530730 vs. calculated on the restored bytes 2317565041\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save_1/RestoreV2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m<ipython-input-9-7010d69d8e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_data_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1802\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mDataLossError\u001b[0m: Checksum does not match: stored 999530730 vs. calculated on the restored bytes 2317565041\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save_1/RestoreV2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save_1/RestoreV2', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-7010d69d8e96>\", line 5, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Checksum does not match: stored 999530730 vs. calculated on the restored bytes 2317565041\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_save_1/RestoreV2\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"]}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(predict_sentences, word_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    \n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(predict_sentences)):\n        data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n        \n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    feed_dict = {\n        lstm.inputX: x_test[start_id:end_id],\n        lstm.dropoutKeepProb: 1.0\n    }\n    predict_result = sess.run(lstm.y_pred_cls, feed_dict=feed_dict)\n    predict_result = [i+1 for i in predict_result]\n    return predict_result\n\nsession = tf.Session()\n\ndef predict11(predict_sentences, probability_threshold=0.26):  # 0.26189747\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for psi in predict_sentences:\n\n        data_id.append([word_to_id[x] for x in preprocess_sentence(psi).split() if x in word_to_id])\n\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, seq_length)\n    feed_dict = {\n        lstm.inputX: x_pad,\n        lstm.dropoutKeepProb: 1.0\n    }\n    predict_result = session.run(tf.nn.softmax(lstm.predictions), feed_dict=feed_dict)\n    # print(predict_result)\n    result = []\n    for i in predict_result:\n        if max(i) > probability_threshold:\n            result.append(i.argmax()+1)\n        else:\n            result.append(0)\n    return result", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def preprocess_sentence(sent):\n    new_sent = ''\n    for i in range(len(sent)):\n        if sent[i] in string.punctuation:\n            if i > 0 and i < len(sent) - 1:\n                if sent[i] in \",.\" and sent[i-1].isdigit() and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and (sent[i-1].isdigit() or sent[i+1].isdigit()):\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                elif sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            elif i == 0:\n                if sent[i] == \"$\" and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            else:\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                else:\n                    new_sent += sent[i]\n        else:\n            new_sent += sent[i]\n    return new_sent.strip().lower()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  # 7\n                         \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n                         \"List center - backs by what their transfer _ fee was .\",  # 5\n                         \"can you tell me what is arkansas 's population on the date july 1st of 2002 ?\",  # 1\n                         \"show the way the number of likes were distributed .\",  # 7\n                         \"is it true that people living on average depends on higher gdp of a country\"  # 10\n                         ]\n\nprint(predict11(predict_sentences))", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}