{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_hf.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model/ad_biLSTM3/lstm_model\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def dataset_split(trainDataPath):\n\n    train_data = []\n    seq_length = 0\n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n            \n            if len(content) > seq_length:\n                seq_length = len(content)\n            \n    np.random.shuffle(train_data)\n    return np.asarray(train_data), seq_length\n\n\ndef loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\ntrain_data, seq_length = dataset_split(trainDataPath)\nx_train, y_train = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n\nprint(len(embedding),embedding_dim,vocab_size, seq_length)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000 41\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    \n    saver = tf.train.Saver()\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 200  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    print_per_batch = 100\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                # feed_dict[model.dropoutKeepProb] = 1.0\n                # loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                \n                loss_train, acc_train = evaluate(session, model, x_train, y_train, model.loss, model.acc, batch_size)\n                \n                if acc_train > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_train\n                    last_improved = total_batch\n                    improved_str = '*'\n                    \n                    if best_acc_train > 0.9:\n                        saver.save(sess=session, save_path=savePath)\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, duration, improved_str))\n\n            if best_acc_train > 0.95 or total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                train_data_len = len(x_train)\n                train_num_batch = int((train_data_len - 1) / batch_size) + 1\n\n                y_train_cls = np.argmax(y_train, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_train_pred_cls = np.zeros(shape=len(x_train), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(train_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, train_data_len)\n                    feed_dict = {\n                        model.inputX: x_train[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_train_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_train_cls, y_train_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_train_cls, y_train_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_train_cls, y_train_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            wordEmbedding = tf.Variable(initial_value=wordEmbedding)\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [64]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09  128\nepsilon = 5\n\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nlstm = AdversarailLSTM(embedding)", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "outputSize:64\nWARNING:tensorflow:From <ipython-input-6-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-6-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:64\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_train(lstm, x_train, y_train, categories)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.619, Train Acc: 40.71%, Time: 28.56s *\nIter: 200, Train Loss:  2.173, Train Acc: 66.33%, Time: 55.53s *\nEpoch: 2\nIter: 300, Train Loss:  1.702, Train Acc: 73.18%, Time: 24.41s *\nIter: 400, Train Loss:  1.238, Train Acc: 81.88%, Time: 51.45s *\nEpoch: 3\nIter: 500, Train Loss:  1.008, Train Acc: 85.11%, Time: 21.74s *\nIter: 600, Train Loss: 0.8446, Train Acc: 87.20%, Time: 48.67s *\nEpoch: 4\nIter: 700, Train Loss: 0.6899, Train Acc: 90.11%, Time: 50.56s *\nIter: 800, Train Loss: 0.5842, Train Acc: 91.81%, Time: 110.25s *\nEpoch: 5\nIter: 900, Train Loss: 0.5058, Train Acc: 92.74%, Time: 34.06s *\nIter: 1000, Train Loss: 0.4972, Train Acc: 93.08%, Time: 77.94s *\nIter: 1100, Train Loss: 0.4216, Train Acc: 94.16%, Time: 117.88s *\nEpoch: 6\nIter: 1200, Train Loss: 0.3354, Train Acc: 95.53%, Time: 50.11s *\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.94      0.98      0.96      1364\n                   Filter       0.91      0.97      0.94      1452\n    Compute Derived Value       0.95      0.95      0.95      1535\n            Find Extremum       0.96      0.95      0.96      1660\n                     Sort       0.96      0.97      0.97      1206\n          Determine Range       0.94      0.90      0.92      1322\nCharacterize Distribution       0.98      0.93      0.96      1329\n           Find Anomalies       0.97      0.96      0.96      1373\n                  Cluster       0.97      0.97      0.97      1270\n                Correlate       0.98      0.98      0.98      1524\n\n                micro avg       0.96      0.96      0.96     14035\n                macro avg       0.96      0.96      0.96     14035\n             weighted avg       0.96      0.96      0.96     14035\n\nConfusion Matrix...\n[[1332    7   16    1    2    2    1    2    0    1]\n [   6 1402    7    8    1   16    1    7    0    4]\n [  37   13 1455    5    1   15    1    7    0    1]\n [  16   24   12 1585   12    4    1    3    0    3]\n [   4    1    0   12 1175    3    1    0   10    0]\n [  16   56   12   17   11 1184   13    5    4    4]\n [   3    4   21    8   13   23 1240    0    7   10]\n [   5   29    4    6    1    4    1 1314    3    6]\n [   3    1    3    5    8    2    3    9 1231    5]\n [   2    3    3    1    1    0    3    6   16 1489]]\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "0.9552547203420021"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "with tf.Session() as session:\n    \n    session.run(tf.global_variables_initializer())\n    \n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=savePath)\n    \n    train_data_len = len(x_train)\n    train_num_batch = int((train_data_len - 1) / batch_size) + 1\n\n    y_train_cls = np.argmax(y_train, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_train_pred_cls = np.zeros(shape=len(x_train), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(train_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, train_data_len)\n        feed_dict = {\n            lstm.inputX: x_train[start_id:end_id],\n            lstm.dropoutKeepProb: 1.0\n        }\n        y_train_pred_cls[start_id:end_id] = session.run(lstm.y_pred_cls, feed_dict=feed_dict)\n\n    accuracy_score = metrics.accuracy_score(y_train_cls, y_train_pred_cls)\n    print(accuracy_score)\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_train_cls, y_train_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_train_cls, y_train_pred_cls)\n    print(cm)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM3/lstm_model\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM3/lstm_model\n", "name": "stderr"}, {"output_type": "stream", "text": "0.9552547203420021\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.94      0.98      0.96      1364\n                   Filter       0.91      0.97      0.94      1452\n    Compute Derived Value       0.95      0.95      0.95      1535\n            Find Extremum       0.96      0.95      0.96      1660\n                     Sort       0.96      0.97      0.97      1206\n          Determine Range       0.94      0.90      0.92      1322\nCharacterize Distribution       0.98      0.93      0.96      1329\n           Find Anomalies       0.97      0.96      0.96      1373\n                  Cluster       0.97      0.97      0.97      1270\n                Correlate       0.98      0.98      0.98      1524\n\n                micro avg       0.96      0.96      0.96     14035\n                macro avg       0.96      0.96      0.96     14035\n             weighted avg       0.96      0.96      0.96     14035\n\nConfusion Matrix...\n[[1332    7   16    1    2    2    1    2    0    1]\n [   6 1402    7    8    1   16    1    7    0    4]\n [  37   13 1455    5    1   15    1    7    0    1]\n [  16   24   12 1585   12    4    1    3    0    3]\n [   4    1    0   12 1175    3    1    0   10    0]\n [  16   56   12   17   11 1184   13    5    4    4]\n [   3    4   21    8   13   23 1240    0    7   10]\n [   5   29    4    6    1    4    1 1314    3    6]\n [   3    1    3    5    8    2    3    9 1231    5]\n [   2    3    3    1    1    0    3    6   16 1489]]\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(predict_sentences, word_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    \n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(predict_sentences)):\n        data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n        \n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    feed_dict = {\n        lstm.inputX: x_test[start_id:end_id],\n        lstm.dropoutKeepProb: 1.0\n    }\n    predict_result = sess.run(lstm.y_pred_cls, feed_dict=feed_dict)\n    predict_result = [i+1 for i in predict_result]\n    return predict_result\n\nsession = tf.Session()\n\ndef predict11(predict_sentences, probability_threshold=0.26):  # 0.26189747\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for psi in predict_sentences:\n\n        data_id.append([word_to_id[x] for x in preprocess_sentence(psi).split() if x in word_to_id])\n\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, seq_length)\n    feed_dict = {\n        lstm.inputX: x_pad,\n        lstm.dropoutKeepProb: 1.0\n    }\n    predict_result = session.run(tf.nn.softmax(lstm.predictions), feed_dict=feed_dict)\n    # print(predict_result)\n    result = []\n    for i in predict_result:\n        if max(i) > probability_threshold:\n            result.append(i.argmax()+1)\n        else:\n            result.append(0)\n    return result", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import string", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def preprocess_sentence(sent):\n    new_sent = ''\n    for i in range(len(sent)):\n        if sent[i] in string.punctuation:\n            if i > 0 and i < len(sent) - 1:\n                if sent[i] in \",.\" and sent[i-1].isdigit() and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and (sent[i-1].isdigit() or sent[i+1].isdigit()):\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                elif sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            elif i == 0:\n                if sent[i] == \"$\" and sent[i+1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i+1] != ' ':\n                    new_sent += sent[i] + ' '\n                else:\n                    new_sent += sent[i]\n            else:\n                if sent[i] == \"%\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i] == \"$\" and sent[i-1].isdigit():\n                    new_sent += sent[i]\n                    continue\n                if sent[i-1] != ' ':\n                    new_sent += ' ' + sent[i]\n                else:\n                    new_sent += sent[i]\n        else:\n            new_sent += sent[i]\n    return new_sent.strip().lower()", "execution_count": 11, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "saver = tf.train.Saver()\nsaver.restore(sess=session, save_path=savePath)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM3/lstm_model\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM3/lstm_model\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  # 7\n                         \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n                         \"List center - backs by what their transfer _ fee was .\",  # 5\n                         \"can you tell me what is arkansas 's population on the date july 1st of 2002 ?\",  # 1\n                         \"show the way the number of likes were distributed .\",  # 7\n                         \"is it true that people living on average depends on higher gdp of a country\",  # 10\n                     \"What's the distribution of duration?\",\n                     \"WHat's the different of duration?\"\n                         ]\n\nprint(predict11(predict_sentences))", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "[7, 1, 9, 1, 7, 10, 7, 7]\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "{'the': 0,\n ',': 1,\n '.': 2,\n 'of': 3,\n 'to': 4,\n 'and': 5,\n 'in': 6,\n 'a': 7,\n '\"': 8,\n \"'s\": 9,\n 'for': 10,\n '-': 11,\n 'that': 12,\n 'on': 13,\n 'is': 14,\n 'was': 15,\n 'said': 16,\n 'with': 17,\n 'he': 18,\n 'as': 19,\n 'it': 20,\n 'by': 21,\n 'at': 22,\n '(': 23,\n ')': 24,\n 'from': 25,\n 'his': 26,\n \"''\": 27,\n '``': 28,\n 'an': 29,\n 'be': 30,\n 'has': 31,\n 'are': 32,\n 'have': 33,\n 'but': 34,\n 'were': 35,\n 'not': 36,\n 'this': 37,\n 'who': 38,\n 'they': 39,\n 'had': 40,\n 'i': 41,\n 'which': 42,\n 'will': 43,\n 'their': 44,\n ':': 45,\n 'or': 46,\n 'its': 47,\n 'one': 48,\n 'after': 49,\n 'new': 50,\n 'been': 51,\n 'also': 52,\n 'we': 53,\n 'would': 54,\n 'two': 55,\n 'more': 56,\n \"'\": 57,\n 'first': 58,\n 'about': 59,\n 'up': 60,\n 'when': 61,\n 'year': 62,\n 'there': 63,\n 'all': 64,\n '--': 65,\n 'out': 66,\n 'she': 67,\n 'other': 68,\n 'people': 69,\n \"n't\": 70,\n 'her': 71,\n 'percent': 72,\n 'than': 73,\n 'over': 74,\n 'into': 75,\n 'last': 76,\n 'some': 77,\n 'government': 78,\n 'time': 79,\n '$': 80,\n 'you': 81,\n 'years': 82,\n 'if': 83,\n 'no': 84,\n 'world': 85,\n 'can': 86,\n 'three': 87,\n 'do': 88,\n ';': 89,\n 'president': 90,\n 'only': 91,\n 'state': 92,\n 'million': 93,\n 'could': 94,\n 'us': 95,\n 'most': 96,\n '_': 97,\n 'against': 98,\n 'u.s.': 99,\n 'so': 100,\n 'them': 101,\n 'what': 102,\n 'him': 103,\n 'united': 104,\n 'during': 105,\n 'before': 106,\n 'may': 107,\n 'since': 108,\n 'many': 109,\n 'while': 110,\n 'where': 111,\n 'states': 112,\n 'because': 113,\n 'now': 114,\n 'city': 115,\n 'made': 116,\n 'like': 117,\n 'between': 118,\n 'did': 119,\n 'just': 120,\n 'national': 121,\n 'day': 122,\n 'country': 123,\n 'under': 124,\n 'such': 125,\n 'second': 126,\n 'then': 127,\n 'company': 128,\n 'group': 129,\n 'any': 130,\n 'through': 131,\n 'china': 132,\n 'four': 133,\n 'being': 134,\n 'down': 135,\n 'war': 136,\n 'back': 137,\n 'off': 138,\n 'south': 139,\n 'american': 140,\n 'minister': 141,\n 'police': 142,\n 'well': 143,\n 'including': 144,\n 'team': 145,\n 'international': 146,\n 'week': 147,\n 'officials': 148,\n 'still': 149,\n 'both': 150,\n 'even': 151,\n 'high': 152,\n 'part': 153,\n 'told': 154,\n 'those': 155,\n 'end': 156,\n 'former': 157,\n 'these': 158,\n 'make': 159,\n 'billion': 160,\n 'work': 161,\n 'our': 162,\n 'home': 163,\n 'school': 164,\n 'party': 165,\n 'house': 166,\n 'old': 167,\n 'later': 168,\n 'get': 169,\n 'another': 170,\n 'tuesday': 171,\n 'news': 172,\n 'long': 173,\n 'five': 174,\n 'called': 175,\n '1': 176,\n 'wednesday': 177,\n 'military': 178,\n 'way': 179,\n 'used': 180,\n 'much': 181,\n 'next': 182,\n 'monday': 183,\n 'thursday': 184,\n 'friday': 185,\n 'game': 186,\n 'here': 187,\n '?': 188,\n 'should': 189,\n 'take': 190,\n 'very': 191,\n 'my': 192,\n 'north': 193,\n 'security': 194,\n 'season': 195,\n 'york': 196,\n 'how': 197,\n 'public': 198,\n 'early': 199,\n 'according': 200,\n 'several': 201,\n 'court': 202,\n 'say': 203,\n 'around': 204,\n 'foreign': 205,\n '10': 206,\n 'until': 207,\n 'set': 208,\n 'political': 209,\n 'says': 210,\n 'market': 211,\n 'however': 212,\n 'family': 213,\n 'life': 214,\n 'same': 215,\n 'general': 216,\n '\u2013': 217,\n 'left': 218,\n 'good': 219,\n 'top': 220,\n 'university': 221,\n 'going': 222,\n 'number': 223,\n 'major': 224,\n 'known': 225,\n 'points': 226,\n 'won': 227,\n 'six': 228,\n 'month': 229,\n 'dollars': 230,\n 'bank': 231,\n '2': 232,\n 'iraq': 233,\n 'use': 234,\n 'members': 235,\n 'each': 236,\n 'area': 237,\n 'found': 238,\n 'official': 239,\n 'sunday': 240,\n 'place': 241,\n 'go': 242,\n 'based': 243,\n 'among': 244,\n 'third': 245,\n 'times': 246,\n 'took': 247,\n 'right': 248,\n 'days': 249,\n 'local': 250,\n 'economic': 251,\n 'countries': 252,\n 'see': 253,\n 'best': 254,\n 'report': 255,\n 'killed': 256,\n 'held': 257,\n 'business': 258,\n 'west': 259,\n 'does': 260,\n 'own': 261,\n '%': 262,\n 'came': 263,\n 'law': 264,\n 'months': 265,\n 'women': 266,\n \"'re\": 267,\n 'power': 268,\n 'think': 269,\n 'service': 270,\n 'children': 271,\n 'bush': 272,\n 'show': 273,\n '/': 274,\n 'help': 275,\n 'chief': 276,\n 'saturday': 277,\n 'system': 278,\n 'john': 279,\n 'support': 280,\n 'series': 281,\n 'play': 282,\n 'office': 283,\n 'following': 284,\n 'me': 285,\n 'meeting': 286,\n 'expected': 287,\n 'late': 288,\n 'washington': 289,\n 'games': 290,\n 'european': 291,\n 'league': 292,\n 'reported': 293,\n 'final': 294,\n 'added': 295,\n 'without': 296,\n 'british': 297,\n 'white': 298,\n 'history': 299,\n 'man': 300,\n 'men': 301,\n 'became': 302,\n 'want': 303,\n 'march': 304,\n 'case': 305,\n 'few': 306,\n 'run': 307,\n 'money': 308,\n 'began': 309,\n 'open': 310,\n 'name': 311,\n 'trade': 312,\n 'center': 313,\n '3': 314,\n 'israel': 315,\n 'oil': 316,\n 'too': 317,\n 'al': 318,\n 'film': 319,\n 'win': 320,\n 'led': 321,\n 'east': 322,\n 'central': 323,\n '20': 324,\n 'air': 325,\n 'come': 326,\n 'chinese': 327,\n 'town': 328,\n 'leader': 329,\n 'army': 330,\n 'line': 331,\n 'never': 332,\n 'little': 333,\n 'played': 334,\n 'prime': 335,\n 'death': 336,\n 'companies': 337,\n 'least': 338,\n 'put': 339,\n 'forces': 340,\n 'past': 341,\n 'de': 342,\n 'half': 343,\n 'june': 344,\n 'saying': 345,\n 'know': 346,\n 'federal': 347,\n 'french': 348,\n 'peace': 349,\n 'earlier': 350,\n 'capital': 351,\n 'force': 352,\n 'great': 353,\n 'union': 354,\n 'near': 355,\n 'released': 356,\n 'small': 357,\n 'department': 358,\n 'every': 359,\n 'health': 360,\n 'japan': 361,\n 'head': 362,\n 'ago': 363,\n 'night': 364,\n 'big': 365,\n 'cup': 366,\n 'election': 367,\n 'region': 368,\n 'director': 369,\n 'talks': 370,\n 'program': 371,\n 'far': 372,\n 'today': 373,\n 'statement': 374,\n 'july': 375,\n 'although': 376,\n 'district': 377,\n 'again': 378,\n 'born': 379,\n 'development': 380,\n 'leaders': 381,\n 'council': 382,\n 'close': 383,\n 'record': 384,\n 'along': 385,\n 'county': 386,\n 'france': 387,\n 'went': 388,\n 'point': 389,\n 'must': 390,\n 'spokesman': 391,\n 'your': 392,\n 'member': 393,\n 'plan': 394,\n 'financial': 395,\n 'april': 396,\n 'recent': 397,\n 'campaign': 398,\n 'become': 399,\n 'troops': 400,\n 'whether': 401,\n 'lost': 402,\n 'music': 403,\n '15': 404,\n 'got': 405,\n 'israeli': 406,\n '30': 407,\n 'need': 408,\n '4': 409,\n 'lead': 410,\n 'already': 411,\n 'russia': 412,\n 'though': 413,\n 'might': 414,\n 'free': 415,\n 'hit': 416,\n 'rights': 417,\n '11': 418,\n 'information': 419,\n 'away': 420,\n '12': 421,\n '5': 422,\n 'others': 423,\n 'control': 424,\n 'within': 425,\n 'large': 426,\n 'economy': 427,\n 'press': 428,\n 'agency': 429,\n 'water': 430,\n 'died': 431,\n 'career': 432,\n 'making': 433,\n '...': 434,\n 'deal': 435,\n 'attack': 436,\n 'side': 437,\n 'seven': 438,\n 'better': 439,\n 'less': 440,\n 'september': 441,\n 'once': 442,\n 'clinton': 443,\n 'main': 444,\n 'due': 445,\n 'committee': 446,\n 'building': 447,\n 'conference': 448,\n 'club': 449,\n 'january': 450,\n 'decision': 451,\n 'stock': 452,\n 'america': 453,\n 'given': 454,\n 'give': 455,\n 'often': 456,\n 'announced': 457,\n 'television': 458,\n 'industry': 459,\n 'order': 460,\n 'young': 461,\n \"'ve\": 462,\n 'palestinian': 463,\n 'age': 464,\n 'start': 465,\n 'administration': 466,\n 'russian': 467,\n 'prices': 468,\n 'round': 469,\n 'december': 470,\n 'nations': 471,\n \"'m\": 472,\n 'human': 473,\n 'india': 474,\n 'defense': 475,\n 'asked': 476,\n 'total': 477,\n 'october': 478,\n 'players': 479,\n 'bill': 480,\n 'important': 481,\n 'southern': 482,\n 'move': 483,\n 'fire': 484,\n 'population': 485,\n 'rose': 486,\n 'november': 487,\n 'include': 488,\n 'further': 489,\n 'nuclear': 490,\n 'street': 491,\n 'taken': 492,\n 'media': 493,\n 'different': 494,\n 'issue': 495,\n 'received': 496,\n 'secretary': 497,\n 'return': 498,\n 'college': 499,\n 'working': 500,\n 'community': 501,\n 'eight': 502,\n 'groups': 503,\n 'despite': 504,\n 'level': 505,\n 'largest': 506,\n 'whose': 507,\n 'attacks': 508,\n 'germany': 509,\n 'august': 510,\n 'change': 511,\n 'church': 512,\n 'nation': 513,\n 'german': 514,\n 'station': 515,\n 'london': 516,\n 'weeks': 517,\n 'having': 518,\n '18': 519,\n 'research': 520,\n 'black': 521,\n 'services': 522,\n 'story': 523,\n '6': 524,\n 'europe': 525,\n 'sales': 526,\n 'policy': 527,\n 'visit': 528,\n 'northern': 529,\n 'lot': 530,\n 'across': 531,\n 'per': 532,\n 'current': 533,\n 'board': 534,\n 'football': 535,\n 'ministry': 536,\n 'workers': 537,\n 'vote': 538,\n 'book': 539,\n 'fell': 540,\n 'seen': 541,\n 'role': 542,\n 'students': 543,\n 'shares': 544,\n 'iran': 545,\n 'process': 546,\n 'agreement': 547,\n 'quarter': 548,\n 'full': 549,\n 'match': 550,\n 'started': 551,\n 'growth': 552,\n 'yet': 553,\n 'moved': 554,\n 'possible': 555,\n 'western': 556,\n 'special': 557,\n '100': 558,\n 'plans': 559,\n 'interest': 560,\n 'behind': 561,\n 'strong': 562,\n 'england': 563,\n 'named': 564,\n 'food': 565,\n 'period': 566,\n 'real': 567,\n 'authorities': 568,\n 'car': 569,\n 'term': 570,\n 'rate': 571,\n 'race': 572,\n 'nearly': 573,\n 'korea': 574,\n 'enough': 575,\n 'site': 576,\n 'opposition': 577,\n 'keep': 578,\n '25': 579,\n 'call': 580,\n 'future': 581,\n 'taking': 582,\n 'island': 583,\n '2008': 584,\n '2006': 585,\n 'road': 586,\n 'outside': 587,\n 'really': 588,\n 'century': 589,\n 'democratic': 590,\n 'almost': 591,\n 'single': 592,\n 'share': 593,\n 'leading': 594,\n 'trying': 595,\n 'find': 596,\n 'album': 597,\n 'senior': 598,\n 'minutes': 599,\n 'together': 600,\n 'congress': 601,\n 'index': 602,\n 'australia': 603,\n 'results': 604,\n 'hard': 605,\n 'hours': 606,\n 'land': 607,\n 'action': 608,\n 'higher': 609,\n 'field': 610,\n 'cut': 611,\n 'coach': 612,\n 'elections': 613,\n 'san': 614,\n 'issues': 615,\n 'executive': 616,\n 'february': 617,\n 'production': 618,\n 'areas': 619,\n 'river': 620,\n 'face': 621,\n 'using': 622,\n 'japanese': 623,\n 'province': 624,\n 'park': 625,\n 'price': 626,\n 'commission': 627,\n 'california': 628,\n 'father': 629,\n 'son': 630,\n 'education': 631,\n '7': 632,\n 'village': 633,\n 'energy': 634,\n 'shot': 635,\n 'short': 636,\n 'africa': 637,\n 'key': 638,\n 'red': 639,\n 'association': 640,\n 'average': 641,\n 'pay': 642,\n 'exchange': 643,\n 'eu': 644,\n 'something': 645,\n 'gave': 646,\n 'likely': 647,\n 'player': 648,\n 'george': 649,\n '2007': 650,\n 'victory': 651,\n '8': 652,\n 'low': 653,\n 'things': 654,\n '2010': 655,\n 'pakistan': 656,\n '14': 657,\n 'post': 658,\n 'social': 659,\n 'continue': 660,\n 'ever': 661,\n 'look': 662,\n 'chairman': 663,\n 'job': 664,\n '2000': 665,\n 'soldiers': 666,\n 'able': 667,\n 'parliament': 668,\n 'front': 669,\n 'himself': 670,\n 'problems': 671,\n 'private': 672,\n 'lower': 673,\n 'list': 674,\n 'built': 675,\n '13': 676,\n 'efforts': 677,\n 'dollar': 678,\n 'miles': 679,\n 'included': 680,\n 'radio': 681,\n 'live': 682,\n 'form': 683,\n 'david': 684,\n 'african': 685,\n 'increase': 686,\n 'reports': 687,\n 'sent': 688,\n 'fourth': 689,\n 'always': 690,\n 'king': 691,\n '50': 692,\n 'tax': 693,\n 'taiwan': 694,\n 'britain': 695,\n '16': 696,\n 'playing': 697,\n 'title': 698,\n 'middle': 699,\n 'meet': 700,\n 'global': 701,\n 'wife': 702,\n '2009': 703,\n 'position': 704,\n 'located': 705,\n 'clear': 706,\n 'ahead': 707,\n '2004': 708,\n '2005': 709,\n 'iraqi': 710,\n 'english': 711,\n 'result': 712,\n 'release': 713,\n 'violence': 714,\n 'goal': 715,\n 'project': 716,\n 'closed': 717,\n 'border': 718,\n 'body': 719,\n 'soon': 720,\n 'crisis': 721,\n 'division': 722,\n '&amp;': 723,\n 'served': 724,\n 'tour': 725,\n 'hospital': 726,\n 'kong': 727,\n 'test': 728,\n 'hong': 729,\n 'u.n.': 730,\n 'inc.': 731,\n 'technology': 732,\n 'believe': 733,\n 'organization': 734,\n 'published': 735,\n 'weapons': 736,\n 'agreed': 737,\n 'why': 738,\n 'nine': 739,\n 'summer': 740,\n 'wanted': 741,\n 'republican': 742,\n 'act': 743,\n 'recently': 744,\n 'texas': 745,\n 'course': 746,\n 'problem': 747,\n 'senate': 748,\n 'medical': 749,\n 'un': 750,\n 'done': 751,\n 'reached': 752,\n 'star': 753,\n 'continued': 754,\n 'investors': 755,\n 'living': 756,\n 'care': 757,\n 'signed': 758,\n '17': 759,\n 'art': 760,\n 'provide': 761,\n 'worked': 762,\n 'presidential': 763,\n 'gold': 764,\n 'obama': 765,\n 'morning': 766,\n 'dead': 767,\n 'opened': 768,\n \"'ll\": 769,\n 'event': 770,\n 'previous': 771,\n 'cost': 772,\n 'instead': 773,\n 'canada': 774,\n 'band': 775,\n 'teams': 776,\n 'daily': 777,\n '2001': 778,\n 'available': 779,\n 'drug': 780,\n 'coming': 781,\n '2003': 782,\n 'investment': 783,\n '\u2019s': 784,\n 'michael': 785,\n 'civil': 786,\n 'woman': 787,\n 'training': 788,\n 'appeared': 789,\n '9': 790,\n 'involved': 791,\n 'indian': 792,\n 'similar': 793,\n 'situation': 794,\n '24': 795,\n 'los': 796,\n 'running': 797,\n 'fighting': 798,\n 'mark': 799,\n '40': 800,\n 'trial': 801,\n 'hold': 802,\n 'australian': 803,\n 'thought': 804,\n '!': 805,\n 'study': 806,\n 'fall': 807,\n 'mother': 808,\n 'met': 809,\n 'relations': 810,\n 'anti': 811,\n '2002': 812,\n 'song': 813,\n 'popular': 814,\n 'base': 815,\n 'tv': 816,\n 'ground': 817,\n 'markets': 818,\n 'ii': 819,\n 'newspaper': 820,\n 'staff': 821,\n 'saw': 822,\n 'hand': 823,\n 'hope': 824,\n 'operations': 825,\n 'pressure': 826,\n 'americans': 827,\n 'eastern': 828,\n 'st.': 829,\n 'legal': 830,\n 'asia': 831,\n 'budget': 832,\n 'returned': 833,\n 'considered': 834,\n 'love': 835,\n 'wrote': 836,\n 'stop': 837,\n 'fight': 838,\n 'currently': 839,\n 'charges': 840,\n 'try': 841,\n 'aid': 842,\n 'ended': 843,\n 'management': 844,\n 'brought': 845,\n 'cases': 846,\n 'decided': 847,\n 'failed': 848,\n 'network': 849,\n 'works': 850,\n 'gas': 851,\n 'turned': 852,\n 'fact': 853,\n 'vice': 854,\n 'ca': 855,\n 'mexico': 856,\n 'trading': 857,\n 'especially': 858,\n 'reporters': 859,\n 'afghanistan': 860,\n 'common': 861,\n 'looking': 862,\n 'space': 863,\n 'rates': 864,\n 'manager': 865,\n 'loss': 866,\n '2011': 867,\n 'justice': 868,\n 'thousands': 869,\n 'james': 870,\n 'rather': 871,\n 'fund': 872,\n 'thing': 873,\n 'republic': 874,\n 'opening': 875,\n 'accused': 876,\n 'winning': 877,\n 'scored': 878,\n 'championship': 879,\n 'example': 880,\n 'getting': 881,\n 'biggest': 882,\n 'performance': 883,\n 'sports': 884,\n '1998': 885,\n 'let': 886,\n 'allowed': 887,\n 'schools': 888,\n 'means': 889,\n 'turn': 890,\n 'leave': 891,\n 'no.': 892,\n 'robert': 893,\n 'personal': 894,\n 'stocks': 895,\n 'showed': 896,\n 'light': 897,\n 'arrested': 898,\n 'person': 899,\n 'either': 900,\n 'offer': 901,\n 'majority': 902,\n 'battle': 903,\n '19': 904,\n 'class': 905,\n 'evidence': 906,\n 'makes': 907,\n 'society': 908,\n 'products': 909,\n 'regional': 910,\n 'needed': 911,\n 'stage': 912,\n 'am': 913,\n 'doing': 914,\n 'families': 915,\n 'construction': 916,\n 'various': 917,\n '1996': 918,\n 'sold': 919,\n 'independent': 920,\n 'kind': 921,\n 'airport': 922,\n 'paul': 923,\n 'judge': 924,\n 'internet': 925,\n 'movement': 926,\n 'room': 927,\n 'followed': 928,\n 'original': 929,\n 'angeles': 930,\n 'italy': 931,\n '`': 932,\n 'data': 933,\n 'comes': 934,\n 'parties': 935,\n 'nothing': 936,\n 'sea': 937,\n 'bring': 938,\n '2012': 939,\n 'annual': 940,\n 'officer': 941,\n 'beijing': 942,\n 'present': 943,\n 'remain': 944,\n 'nato': 945,\n '1999': 946,\n '22': 947,\n 'remains': 948,\n 'allow': 949,\n 'florida': 950,\n 'computer': 951,\n '21': 952,\n 'contract': 953,\n 'coast': 954,\n 'created': 955,\n 'demand': 956,\n 'operation': 957,\n 'events': 958,\n 'islamic': 959,\n 'beat': 960,\n 'analysts': 961,\n 'interview': 962,\n 'helped': 963,\n 'child': 964,\n 'probably': 965,\n 'spent': 966,\n 'asian': 967,\n 'effort': 968,\n 'cooperation': 969,\n 'shows': 970,\n 'calls': 971,\n 'investigation': 972,\n 'lives': 973,\n 'video': 974,\n 'yen': 975,\n 'runs': 976,\n 'tried': 977,\n 'bad': 978,\n 'described': 979,\n '1994': 980,\n 'toward': 981,\n 'written': 982,\n 'throughout': 983,\n 'established': 984,\n 'mission': 985,\n 'associated': 986,\n 'buy': 987,\n 'growing': 988,\n 'green': 989,\n 'forward': 990,\n 'competition': 991,\n 'poor': 992,\n 'latest': 993,\n 'banks': 994,\n 'question': 995,\n '1997': 996,\n 'prison': 997,\n 'feel': 998,\n 'attention': 999,\n ...}"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id_array = list(word_to_id.keys())", "execution_count": 27, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "with open(\"s3://corpus-2/dataset/vocab.txt\", \"w\", encoding=\"utf-8\") as fp:\n    fp.write(\"\\n\".join(word_to_id_array))", "execution_count": 28, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "embedding.shape", "execution_count": 29, "outputs": [{"output_type": "execute_result", "execution_count": 29, "data": {"text/plain": "(400000, 100)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.zeros((2,5))", "execution_count": 30, "outputs": [{"output_type": "execute_result", "execution_count": 30, "data": {"text/plain": "array([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}