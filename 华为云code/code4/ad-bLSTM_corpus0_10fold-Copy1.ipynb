{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_5_new.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_info = {\n    \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\n\ndef dataset_split(info):\n    if info:\n        [num, pi] = info\n        train_data = [[] for i in range(num)]\n        with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                train_data[index].append([content,label])\n\n        for i in range(num):\n            np.random.shuffle(train_data[i])\n            train_data[i] = np.asarray(train_data[i])\n\n        np.random.shuffle(train_data)   \n        return train_data\n    \n    \n    train_data = []\n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data)\n\n\ndef mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(model, train_data, split_type):\n    \n    print(split_type)\n    \n    test_acc = []\n    fold_id = 0\n    \n    if split_type != \"random\":\n        tx = []\n        ty = []\n        for ti in train_data:\n            x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            tx.append(x_train)\n            ty.append(y_train)\n\n        tx = np.asarray(tx)\n        ty = np.asarray(ty)\n\n        print(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n        \n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_x, train_y = mergeData(tx[train_i],ty[train_i])\n            test_x, test_y = mergeData(tx[test_i],ty[test_i])\n            test_acc.append(model_train(model, train_x, train_y, test_x, test_y, categories))\n        \n    else:\n        tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n        print(len(tx),len(tx[0]),len(tx[1]))\n\n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            test_acc.append(model_train(model,tx[train_i], ty[train_i],tx[test_i], ty[test_i],categories))\n        \n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\nprint(len(embedding),embedding_dim,vocab_size)\n \nseq_length = 41  # seq_length = 37  TREC", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, x_val, y_val, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.dropoutKeepProb] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, model, x_val, y_val, model.loss, model.acc, 64)\n                if acc_val > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_val\n                    last_improved = total_batch\n                    # saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, loss_val, acc_val, duration, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                test_data_len = len(x_val)\n                test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n                y_test_cls = np.argmax(y_val, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.zeros(shape=len(x_val), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, test_data_len)\n                    feed_dict = {\n                        model.inputX: x_val[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_test_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            wordEmbedding = tf.Variable(initial_value=wordEmbedding)\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nlstm = AdversarailLSTM(embedding)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:128\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_info = {\n    # \"random\": False,\n    # \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    #\"table\": [37, 3]\n}\n\nkf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, split_type))", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "bundle\n920 15 14 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:   4.48, Train Acc: 17.19%, Val Loss:  4.512, Val Acc: 17.54%, Time: 7.11s *\nIter: 60, Train Loss:  3.564, Train Acc: 60.94%, Val Loss:  4.064, Val Acc: 31.68%, Time: 13.19s *\nIter: 90, Train Loss:  2.871, Train Acc: 65.62%, Val Loss:   3.42, Val Acc: 44.41%, Time: 19.22s *\nIter: 120, Train Loss:  2.333, Train Acc: 59.38%, Val Loss:   2.98, Val Acc: 50.71%, Time: 25.29s *\nIter: 150, Train Loss:  1.997, Train Acc: 70.31%, Val Loss:  2.652, Val Acc: 57.21%, Time: 31.28s *\nIter: 180, Train Loss:  1.551, Train Acc: 78.12%, Val Loss:  2.828, Val Acc: 54.74%, Time: 37.29s \nEpoch: 2\nIter: 210, Train Loss:  1.273, Train Acc: 81.25%, Val Loss:  2.668, Val Acc: 57.43%, Time: 3.33s *\nIter: 240, Train Loss:  1.426, Train Acc: 79.69%, Val Loss:  2.637, Val Acc: 61.74%, Time: 9.40s *\nIter: 270, Train Loss:  1.079, Train Acc: 84.38%, Val Loss:  2.466, Val Acc: 63.79%, Time: 15.39s *\nIter: 300, Train Loss: 0.4294, Train Acc: 96.88%, Val Loss:  2.488, Val Acc: 64.14%, Time: 21.36s *\nIter: 330, Train Loss: 0.7871, Train Acc: 90.62%, Val Loss:  2.662, Val Acc: 62.94%, Time: 27.31s \nIter: 360, Train Loss: 0.9247, Train Acc: 79.69%, Val Loss:  2.766, Val Acc: 59.83%, Time: 33.35s \nIter: 390, Train Loss: 0.7677, Train Acc: 90.62%, Val Loss:  2.559, Val Acc: 64.71%, Time: 39.34s *\nEpoch: 3\nIter: 420, Train Loss: 0.4982, Train Acc: 95.31%, Val Loss:  2.528, Val Acc: 66.05%, Time: 5.15s *\nIter: 450, Train Loss: 0.2877, Train Acc: 96.88%, Val Loss:  2.347, Val Acc: 67.33%, Time: 11.06s *\nIter: 480, Train Loss: 0.7333, Train Acc: 87.50%, Val Loss:  2.669, Val Acc: 64.29%, Time: 17.04s \nIter: 510, Train Loss: 0.3046, Train Acc: 96.88%, Val Loss:  2.613, Val Acc: 63.79%, Time: 23.04s \nIter: 540, Train Loss: 0.5071, Train Acc: 93.75%, Val Loss:  2.565, Val Acc: 64.92%, Time: 29.15s \nIter: 570, Train Loss: 0.3508, Train Acc: 95.31%, Val Loss:  2.569, Val Acc: 66.48%, Time: 35.16s \nEpoch: 4\nIter: 600, Train Loss: 0.4732, Train Acc: 90.62%, Val Loss:  2.967, Val Acc: 60.96%, Time: 2.40s \nIter: 630, Train Loss: 0.4689, Train Acc: 93.75%, Val Loss:  2.596, Val Acc: 64.64%, Time: 8.38s \nIter: 660, Train Loss: 0.3708, Train Acc: 95.31%, Val Loss:  2.503, Val Acc: 66.20%, Time: 14.38s \nIter: 690, Train Loss: 0.5214, Train Acc: 92.19%, Val Loss:  2.763, Val Acc: 65.91%, Time: 20.44s \nIter: 720, Train Loss: 0.5381, Train Acc: 90.62%, Val Loss:  2.647, Val Acc: 64.78%, Time: 26.44s \nIter: 750, Train Loss: 0.3732, Train Acc: 96.88%, Val Loss:  2.772, Val Acc: 64.99%, Time: 32.41s \nIter: 780, Train Loss: 0.3451, Train Acc: 93.75%, Val Loss:  2.727, Val Acc: 65.98%, Time: 38.38s \nEpoch: 5\nIter: 810, Train Loss: 0.06383, Train Acc: 100.00%, Val Loss:  2.865, Val Acc: 66.20%, Time: 4.25s \nIter: 840, Train Loss: 0.1799, Train Acc: 95.31%, Val Loss:  2.845, Val Acc: 67.26%, Time: 10.35s \nIter: 870, Train Loss: 0.2169, Train Acc: 96.88%, Val Loss:  3.141, Val Acc: 64.85%, Time: 16.42s \nIter: 900, Train Loss: 0.1143, Train Acc: 98.44%, Val Loss:  2.896, Val Acc: 64.92%, Time: 22.35s \nIter: 930, Train Loss: 0.2324, Train Acc: 96.88%, Val Loss:   2.91, Val Acc: 64.78%, Time: 28.36s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.60      0.52      0.56       141\n                   Filter       0.51      0.76      0.61       155\n    Compute Derived Value       0.75      0.60      0.66       174\n            Find Extremum       0.61      0.48      0.54       147\n                     Sort       0.59      0.77      0.67       102\n          Determine Range       0.67      0.57      0.62       128\nCharacterize Distribution       0.76      0.81      0.78       124\n           Find Anomalies       0.69      0.77      0.73       167\n                  Cluster       0.78      0.73      0.76       119\n                Correlate       0.80      0.69      0.74       157\n\n                micro avg       0.67      0.67      0.67      1414\n                macro avg       0.68      0.67      0.67      1414\n             weighted avg       0.68      0.67      0.67      1414\n\nConfusion Matrix...\n[[ 74  34  14  10   0   0   8   0   0   1]\n [  4 118   3   0   1  19   0   9   1   0]\n [  5  13 104  16   4   0  11  14   2   5]\n [ 19  18   0  70  23  15   0   1   0   1]\n [  0  13   0   0  79   0   0   1   9   0]\n [ 16  18   6   2   7  73   3   3   0   0]\n [  0   2   5  13   2   1 100   0   1   0]\n [  1  13   0   3   0   0   3 129   4  14]\n [  0   0   0   0  15   0   3   8  87   6]\n [  5   2   7   0   2   1   4  21   7 108]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.431, Train Acc: 26.56%, Val Loss:  4.494, Val Acc: 13.78%, Time: 7.10s *\nIter: 60, Train Loss:  3.822, Train Acc: 31.25%, Val Loss:    3.9, Val Acc: 34.32%, Time: 13.05s *\nIter: 90, Train Loss:   3.24, Train Acc: 45.31%, Val Loss:  3.437, Val Acc: 42.69%, Time: 19.10s *\nIter: 120, Train Loss:  2.388, Train Acc: 56.25%, Val Loss:   3.15, Val Acc: 43.18%, Time: 25.10s *\nIter: 150, Train Loss:  1.966, Train Acc: 75.00%, Val Loss:  2.822, Val Acc: 50.42%, Time: 31.20s *\nIter: 180, Train Loss:  1.781, Train Acc: 68.75%, Val Loss:  2.675, Val Acc: 54.22%, Time: 37.18s *\nEpoch: 2\nIter: 210, Train Loss:  1.362, Train Acc: 82.81%, Val Loss:  2.366, Val Acc: 59.99%, Time: 3.37s *\nIter: 240, Train Loss:  1.705, Train Acc: 78.12%, Val Loss:  2.295, Val Acc: 63.01%, Time: 9.39s *\nIter: 270, Train Loss: 0.8027, Train Acc: 89.06%, Val Loss:  2.268, Val Acc: 61.60%, Time: 15.38s \nIter: 300, Train Loss: 0.7827, Train Acc: 87.50%, Val Loss:  2.189, Val Acc: 63.78%, Time: 21.46s *\nIter: 330, Train Loss: 0.6787, Train Acc: 89.06%, Val Loss:  2.065, Val Acc: 67.65%, Time: 27.51s *\nIter: 360, Train Loss:  1.058, Train Acc: 81.25%, Val Loss:  2.141, Val Acc: 65.89%, Time: 33.48s \nIter: 390, Train Loss:  0.771, Train Acc: 90.62%, Val Loss:   2.17, Val Acc: 64.42%, Time: 39.52s \nEpoch: 3\nIter: 420, Train Loss: 0.3877, Train Acc: 93.75%, Val Loss:  2.233, Val Acc: 65.33%, Time: 5.10s \nIter: 450, Train Loss: 0.7258, Train Acc: 87.50%, Val Loss:   2.21, Val Acc: 66.46%, Time: 11.15s \nIter: 480, Train Loss: 0.3769, Train Acc: 96.88%, Val Loss:   2.22, Val Acc: 66.32%, Time: 17.11s \nIter: 510, Train Loss: 0.6334, Train Acc: 90.62%, Val Loss:  2.504, Val Acc: 63.71%, Time: 23.10s \nIter: 540, Train Loss: 0.4206, Train Acc: 95.31%, Val Loss:  2.292, Val Acc: 66.60%, Time: 29.05s \nIter: 570, Train Loss: 0.2179, Train Acc: 98.44%, Val Loss:   2.25, Val Acc: 67.02%, Time: 34.99s \nEpoch: 4\nIter: 600, Train Loss: 0.4099, Train Acc: 95.31%, Val Loss:  1.981, Val Acc: 68.50%, Time: 2.41s *\nIter: 630, Train Loss: 0.3497, Train Acc: 96.88%, Val Loss:  2.173, Val Acc: 68.50%, Time: 8.43s \nIter: 660, Train Loss:  0.265, Train Acc: 93.75%, Val Loss:  2.377, Val Acc: 66.46%, Time: 14.54s \nIter: 690, Train Loss: 0.3675, Train Acc: 95.31%, Val Loss:  2.212, Val Acc: 68.50%, Time: 20.59s \nIter: 720, Train Loss: 0.1771, Train Acc: 100.00%, Val Loss:  2.582, Val Acc: 65.19%, Time: 26.56s \nIter: 750, Train Loss: 0.3013, Train Acc: 96.88%, Val Loss:  2.751, Val Acc: 66.10%, Time: 32.64s \nIter: 780, Train Loss: 0.2954, Train Acc: 96.88%, Val Loss:  2.474, Val Acc: 67.02%, Time: 38.64s \nEpoch: 5\nIter: 810, Train Loss: 0.1775, Train Acc: 96.88%, Val Loss:  2.049, Val Acc: 68.85%, Time: 4.20s *\nIter: 840, Train Loss: 0.4446, Train Acc: 93.75%, Val Loss:  2.391, Val Acc: 66.32%, Time: 10.17s \nIter: 870, Train Loss:  0.251, Train Acc: 98.44%, Val Loss:  2.149, Val Acc: 67.23%, Time: 16.19s \nIter: 900, Train Loss: 0.09017, Train Acc: 100.00%, Val Loss:  2.484, Val Acc: 66.03%, Time: 22.22s \nIter: 930, Train Loss: 0.1062, Train Acc: 98.44%, Val Loss:  2.618, Val Acc: 67.30%, Time: 28.25s \nIter: 960, Train Loss: 0.1868, Train Acc: 96.88%, Val Loss:  2.438, Val Acc: 69.55%, Time: 34.27s *\nIter: 990, Train Loss: 0.01182, Train Acc: 100.00%, Val Loss:  2.485, Val Acc: 66.88%, Time: 40.28s \nEpoch: 6\nIter: 1020, Train Loss: 0.1041, Train Acc: 98.44%, Val Loss:  2.507, Val Acc: 68.28%, Time: 6.07s \nIter: 1050, Train Loss: 0.1216, Train Acc: 98.44%, Val Loss:  2.299, Val Acc: 68.92%, Time: 12.16s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1080, Train Loss: 0.1813, Train Acc: 96.88%, Val Loss:  2.454, Val Acc: 67.72%, Time: 18.24s \nIter: 1110, Train Loss: 0.1873, Train Acc: 96.88%, Val Loss:  2.502, Val Acc: 66.60%, Time: 24.26s \nIter: 1140, Train Loss: 0.09777, Train Acc: 100.00%, Val Loss:  2.417, Val Acc: 69.41%, Time: 30.21s \nIter: 1170, Train Loss: 0.2302, Train Acc: 98.44%, Val Loss:   2.33, Val Acc: 69.13%, Time: 36.21s \nEpoch: 7\nIter: 1200, Train Loss: 0.01451, Train Acc: 100.00%, Val Loss:  2.652, Val Acc: 67.44%, Time: 3.33s \nIter: 1230, Train Loss: 0.0223, Train Acc: 100.00%, Val Loss:  2.678, Val Acc: 68.00%, Time: 9.30s \nIter: 1260, Train Loss: 0.3029, Train Acc: 95.31%, Val Loss:  2.578, Val Acc: 69.27%, Time: 15.34s \nIter: 1290, Train Loss:  0.213, Train Acc: 96.88%, Val Loss:  2.747, Val Acc: 66.32%, Time: 21.32s \nIter: 1320, Train Loss: 0.1789, Train Acc: 98.44%, Val Loss:   2.93, Val Acc: 66.24%, Time: 27.27s \nIter: 1350, Train Loss: 0.03266, Train Acc: 100.00%, Val Loss:  2.794, Val Acc: 67.86%, Time: 33.36s \nIter: 1380, Train Loss: 0.04149, Train Acc: 98.44%, Val Loss:  2.704, Val Acc: 66.67%, Time: 39.29s \nEpoch: 8\nIter: 1410, Train Loss: 0.02742, Train Acc: 100.00%, Val Loss:  2.875, Val Acc: 67.09%, Time: 5.14s \nIter: 1440, Train Loss:  0.189, Train Acc: 98.44%, Val Loss:  2.971, Val Acc: 65.47%, Time: 11.13s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.68      0.76      0.72       125\n                   Filter       0.52      0.64      0.57       112\n    Compute Derived Value       0.64      0.53      0.58       268\n            Find Extremum       0.69      0.82      0.75       154\n                     Sort       0.83      0.84      0.83       136\n          Determine Range       0.44      0.58      0.50       109\nCharacterize Distribution       0.69      0.62      0.65       106\n           Find Anomalies       0.59      0.60      0.59        95\n                  Cluster       0.81      0.82      0.82       159\n                Correlate       0.84      0.55      0.66       158\n\n                micro avg       0.67      0.67      0.67      1422\n                macro avg       0.67      0.68      0.67      1422\n             weighted avg       0.68      0.67      0.67      1422\n\nConfusion Matrix...\n[[ 95   4  11   5   0   0   1   9   0   0]\n [  1  72  15   3   0  16   0   2   0   3]\n [ 27  15 142  26   3  39  15   1   0   0]\n [  3   4   2 126   0   7   0   6   0   6]\n [  0   0   0   5 114   1   1   1  11   3]\n [  0   7   1  16  15  63   3   4   0   0]\n [  2   0  24   0   4   6  66   1   1   2]\n [ 11  25   0   0   0   0   0  57   2   0]\n [  0   1   2   1   1   4   9   8 130   3]\n [  0  11  26   1   1   7   1   8  16  87]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.405, Train Acc: 18.75%, Val Loss:   4.43, Val Acc: 20.74%, Time: 6.99s *\nIter: 60, Train Loss:  3.736, Train Acc: 37.50%, Val Loss:  4.097, Val Acc: 28.55%, Time: 12.95s *\nIter: 90, Train Loss:   2.95, Train Acc: 54.69%, Val Loss:   3.33, Val Acc: 43.04%, Time: 18.91s *\nIter: 120, Train Loss:  2.748, Train Acc: 53.12%, Val Loss:  3.129, Val Acc: 48.93%, Time: 24.89s *\nIter: 150, Train Loss:  2.118, Train Acc: 70.31%, Val Loss:  2.967, Val Acc: 51.56%, Time: 30.82s *\nIter: 180, Train Loss:  1.729, Train Acc: 76.56%, Val Loss:  2.588, Val Acc: 58.81%, Time: 36.86s *\nEpoch: 2\nIter: 210, Train Loss:  1.511, Train Acc: 79.69%, Val Loss:  2.495, Val Acc: 59.87%, Time: 3.27s *\nIter: 240, Train Loss: 0.9587, Train Acc: 90.62%, Val Loss:  2.568, Val Acc: 59.87%, Time: 9.16s \nIter: 270, Train Loss:  1.096, Train Acc: 85.94%, Val Loss:  2.698, Val Acc: 60.16%, Time: 15.12s *\nIter: 300, Train Loss: 0.8219, Train Acc: 90.62%, Val Loss:  2.254, Val Acc: 64.77%, Time: 21.04s *\nIter: 330, Train Loss:  1.043, Train Acc: 87.50%, Val Loss:  2.622, Val Acc: 63.42%, Time: 26.97s \nIter: 360, Train Loss: 0.9283, Train Acc: 81.25%, Val Loss:  2.292, Val Acc: 65.55%, Time: 32.89s *\nIter: 390, Train Loss: 0.7824, Train Acc: 85.94%, Val Loss:  2.302, Val Acc: 65.27%, Time: 38.79s \nEpoch: 3\nIter: 420, Train Loss: 0.2964, Train Acc: 95.31%, Val Loss:  2.402, Val Acc: 62.57%, Time: 5.02s \nIter: 450, Train Loss: 0.3752, Train Acc: 93.75%, Val Loss:  2.305, Val Acc: 65.55%, Time: 10.94s \nIter: 480, Train Loss: 0.3412, Train Acc: 95.31%, Val Loss:  2.183, Val Acc: 67.40%, Time: 16.89s *\nIter: 510, Train Loss: 0.3616, Train Acc: 95.31%, Val Loss:  2.368, Val Acc: 66.12%, Time: 22.85s \nIter: 540, Train Loss: 0.2959, Train Acc: 96.88%, Val Loss:  2.172, Val Acc: 68.25%, Time: 28.80s *\nIter: 570, Train Loss: 0.4435, Train Acc: 95.31%, Val Loss:  2.499, Val Acc: 65.13%, Time: 34.68s \nEpoch: 4\nIter: 600, Train Loss: 0.3744, Train Acc: 95.31%, Val Loss:  2.182, Val Acc: 66.97%, Time: 2.37s \nIter: 630, Train Loss: 0.2778, Train Acc: 98.44%, Val Loss:  2.478, Val Acc: 66.62%, Time: 8.34s \nIter: 660, Train Loss: 0.3733, Train Acc: 93.75%, Val Loss:  2.488, Val Acc: 66.83%, Time: 14.27s \nIter: 690, Train Loss: 0.1747, Train Acc: 98.44%, Val Loss:  2.794, Val Acc: 63.64%, Time: 20.22s \nIter: 720, Train Loss: 0.1253, Train Acc: 98.44%, Val Loss:  2.491, Val Acc: 65.27%, Time: 26.18s \nIter: 750, Train Loss: 0.1273, Train Acc: 100.00%, Val Loss:  2.299, Val Acc: 68.68%, Time: 32.11s *\nIter: 780, Train Loss: 0.2319, Train Acc: 95.31%, Val Loss:  2.181, Val Acc: 70.81%, Time: 38.03s *\nEpoch: 5\nIter: 810, Train Loss: 0.1687, Train Acc: 98.44%, Val Loss:  2.601, Val Acc: 66.76%, Time: 4.15s \nIter: 840, Train Loss: 0.05937, Train Acc: 100.00%, Val Loss:  2.581, Val Acc: 67.76%, Time: 10.07s \nIter: 870, Train Loss: 0.04149, Train Acc: 100.00%, Val Loss:  2.525, Val Acc: 69.03%, Time: 16.06s \nIter: 900, Train Loss: 0.03861, Train Acc: 100.00%, Val Loss:  2.416, Val Acc: 68.47%, Time: 21.92s \nIter: 930, Train Loss: 0.4163, Train Acc: 95.31%, Val Loss:    2.6, Val Acc: 69.32%, Time: 27.83s \nIter: 960, Train Loss: 0.1441, Train Acc: 98.44%, Val Loss:   2.84, Val Acc: 64.20%, Time: 33.81s \nIter: 990, Train Loss: 0.0511, Train Acc: 100.00%, Val Loss:  2.902, Val Acc: 63.92%, Time: 39.67s \nEpoch: 6\nIter: 1020, Train Loss: 0.04109, Train Acc: 98.44%, Val Loss:  2.687, Val Acc: 66.34%, Time: 5.99s \nIter: 1050, Train Loss:  0.113, Train Acc: 98.44%, Val Loss:  2.648, Val Acc: 67.26%, Time: 11.91s \nIter: 1080, Train Loss: 0.09007, Train Acc: 100.00%, Val Loss:  2.497, Val Acc: 69.25%, Time: 17.88s \nIter: 1110, Train Loss: 0.05661, Train Acc: 100.00%, Val Loss:  2.816, Val Acc: 67.12%, Time: 23.87s \nIter: 1140, Train Loss: 0.07516, Train Acc: 100.00%, Val Loss:  2.765, Val Acc: 67.90%, Time: 29.80s \nIter: 1170, Train Loss: 0.06882, Train Acc: 98.44%, Val Loss:  2.754, Val Acc: 67.19%, Time: 35.74s \nEpoch: 7\nIter: 1200, Train Loss: 0.2329, Train Acc: 98.44%, Val Loss:  2.778, Val Acc: 67.26%, Time: 3.26s \nIter: 1230, Train Loss: 0.04328, Train Acc: 100.00%, Val Loss:  2.688, Val Acc: 70.53%, Time: 9.24s \nIter: 1260, Train Loss: 0.1292, Train Acc: 98.44%, Val Loss:  3.057, Val Acc: 68.32%, Time: 15.16s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.49      0.47        72\n                   Filter       0.58      0.54      0.56       167\n    Compute Derived Value       0.72      0.48      0.58       212\n            Find Extremum       0.79      0.74      0.77       270\n                     Sort       0.67      0.93      0.78        56\n          Determine Range       0.75      0.70      0.72       200\nCharacterize Distribution       0.54      0.79      0.64       100\n           Find Anomalies       0.53      0.65      0.58        94\n                  Cluster       0.78      0.81      0.79       105\n                Correlate       0.76      0.83      0.79       132\n\n                micro avg       0.68      0.68      0.68      1408\n                macro avg       0.66      0.70      0.67      1408\n             weighted avg       0.69      0.68      0.68      1408\n\nConfusion Matrix...\n[[ 35   5   4   7   2  10   4   0   0   5]\n [  1  91  13  12  10   1   4  29   5   1]\n [ 32  18 102   9   6  18  11   7   3   6]\n [  5  16   8 201   2   6  21   9   2   0]\n [  0   0   1   0  52   0   1   0   2   0]\n [  2  10  10  22   3 140   3   2   3   5]\n [  0   0   2   0   0   4  79   0   0  15]\n [  1  16   1   2   1   5   4  61   1   2]\n [  0   0   0   1   2   3  13   0  85   1]\n [  0   2   0   0   0   0   5   7   8 110]]\nFold:  4\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.462, Train Acc: 12.50%, Val Loss:  4.665, Val Acc:  5.70%, Time: 7.01s *\nIter: 60, Train Loss:  4.027, Train Acc: 35.94%, Val Loss:  4.182, Val Acc: 29.49%, Time: 12.94s *\nIter: 90, Train Loss:  3.242, Train Acc: 50.00%, Val Loss:  3.287, Val Acc: 48.08%, Time: 18.87s *\nIter: 120, Train Loss:  2.604, Train Acc: 54.69%, Val Loss:  2.885, Val Acc: 55.13%, Time: 24.89s *\nIter: 150, Train Loss:  2.135, Train Acc: 67.19%, Val Loss:  2.595, Val Acc: 60.11%, Time: 30.92s *\nIter: 180, Train Loss:  1.505, Train Acc: 76.56%, Val Loss:  2.243, Val Acc: 65.81%, Time: 36.84s *\nEpoch: 2\nIter: 210, Train Loss:   1.55, Train Acc: 73.44%, Val Loss:  2.025, Val Acc: 69.02%, Time: 3.27s *\nIter: 240, Train Loss:  0.968, Train Acc: 87.50%, Val Loss:  1.941, Val Acc: 69.66%, Time: 9.22s *\nIter: 270, Train Loss:    1.0, Train Acc: 89.06%, Val Loss:  1.853, Val Acc: 72.79%, Time: 15.14s *\nIter: 300, Train Loss: 0.7302, Train Acc: 92.19%, Val Loss:  2.053, Val Acc: 68.95%, Time: 21.09s \nIter: 330, Train Loss: 0.8143, Train Acc: 92.19%, Val Loss:  1.995, Val Acc: 69.66%, Time: 27.09s \nIter: 360, Train Loss: 0.4874, Train Acc: 92.19%, Val Loss:  1.873, Val Acc: 70.66%, Time: 33.08s \nIter: 390, Train Loss: 0.7058, Train Acc: 93.75%, Val Loss:   1.87, Val Acc: 71.58%, Time: 39.03s \nEpoch: 3\nIter: 420, Train Loss: 0.5202, Train Acc: 93.75%, Val Loss:  2.285, Val Acc: 68.02%, Time: 5.10s \nIter: 450, Train Loss: 0.5156, Train Acc: 93.75%, Val Loss:  2.022, Val Acc: 69.66%, Time: 11.10s \nIter: 480, Train Loss: 0.4794, Train Acc: 93.75%, Val Loss:  2.145, Val Acc: 68.52%, Time: 17.05s \nIter: 510, Train Loss: 0.3853, Train Acc: 92.19%, Val Loss:  1.939, Val Acc: 72.01%, Time: 23.05s \nIter: 540, Train Loss: 0.3383, Train Acc: 95.31%, Val Loss:  2.004, Val Acc: 70.09%, Time: 29.00s \nIter: 570, Train Loss: 0.3328, Train Acc: 93.75%, Val Loss:  2.011, Val Acc: 73.36%, Time: 34.97s *\nEpoch: 4\nIter: 600, Train Loss: 0.2006, Train Acc: 96.88%, Val Loss:  1.991, Val Acc: 72.22%, Time: 2.37s \nIter: 630, Train Loss: 0.3691, Train Acc: 93.75%, Val Loss:  2.278, Val Acc: 68.09%, Time: 8.28s \nIter: 660, Train Loss: 0.1899, Train Acc: 96.88%, Val Loss:  2.015, Val Acc: 71.51%, Time: 14.26s \nIter: 690, Train Loss: 0.2772, Train Acc: 96.88%, Val Loss:  2.168, Val Acc: 70.37%, Time: 20.20s \nIter: 720, Train Loss: 0.2369, Train Acc: 96.88%, Val Loss:  2.051, Val Acc: 71.79%, Time: 26.27s \nIter: 750, Train Loss: 0.08988, Train Acc: 100.00%, Val Loss:  2.181, Val Acc: 70.23%, Time: 32.20s \nIter: 780, Train Loss: 0.1714, Train Acc: 96.88%, Val Loss:  2.191, Val Acc: 71.58%, Time: 38.14s \nEpoch: 5\nIter: 810, Train Loss: 0.0726, Train Acc: 98.44%, Val Loss:  2.057, Val Acc: 73.29%, Time: 4.18s \nIter: 840, Train Loss: 0.2717, Train Acc: 96.88%, Val Loss:   2.33, Val Acc: 69.80%, Time: 10.12s \nIter: 870, Train Loss:  0.121, Train Acc: 96.88%, Val Loss:  2.098, Val Acc: 72.36%, Time: 16.09s \nIter: 900, Train Loss: 0.1392, Train Acc: 98.44%, Val Loss:  2.174, Val Acc: 71.08%, Time: 22.05s \nIter: 930, Train Loss: 0.1914, Train Acc: 98.44%, Val Loss:  2.132, Val Acc: 72.36%, Time: 27.95s \nIter: 960, Train Loss: 0.2527, Train Acc: 95.31%, Val Loss:  2.286, Val Acc: 69.23%, Time: 33.89s \nIter: 990, Train Loss:  0.411, Train Acc: 91.30%, Val Loss:   2.15, Val Acc: 72.36%, Time: 39.87s \nEpoch: 6\nIter: 1020, Train Loss: 0.06523, Train Acc: 100.00%, Val Loss:  2.175, Val Acc: 71.72%, Time: 5.98s \nIter: 1050, Train Loss: 0.06692, Train Acc: 100.00%, Val Loss:  2.738, Val Acc: 67.45%, Time: 11.99s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.76      0.66      0.71       211\n                   Filter       0.52      0.73      0.61        90\n    Compute Derived Value       0.72      0.80      0.76       158\n            Find Extremum       0.49      0.66      0.56       105\n                     Sort       0.82      0.85      0.83       162\n          Determine Range       0.46      0.54      0.50        46\nCharacterize Distribution       0.81      0.73      0.77       135\n           Find Anomalies       0.88      0.57      0.69       224\n                  Cluster       0.71      0.88      0.78       171\n                Correlate       0.81      0.61      0.69       102\n\n                micro avg       0.71      0.71      0.71      1404\n                macro avg       0.70      0.70      0.69      1404\n             weighted avg       0.74      0.71      0.71      1404\n\nConfusion Matrix...\n[[140   2  17  34   4  10   3   0   1   0]\n [  4  66   5   0   4   3   0   7   1   0]\n [  5   6 126   4   1   8   4   0   2   2]\n [  8  12   1  69   9   1   0   1   4   0]\n [  0   1   0   4 137   2   0   0  18   0]\n [  0   3   2  13   0  25   1   1   1   0]\n [  6   9   0   2   1   4  98   1  10   4]\n [ 13  27  12   7   0   1  13 128  15   8]\n [  7   2   0   3   6   0   1   1 150   1]\n [  2   0  11   4   6   0   1   6  10  62]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.481, Train Acc: 18.75%, Val Loss:  4.511, Val Acc: 17.07%, Time: 7.01s *\nIter: 60, Train Loss:  3.732, Train Acc: 32.81%, Val Loss:  4.055, Val Acc: 24.96%, Time: 13.11s *\nIter: 90, Train Loss:  3.187, Train Acc: 53.12%, Val Loss:  3.681, Val Acc: 36.81%, Time: 19.13s *\nIter: 120, Train Loss:  2.681, Train Acc: 59.38%, Val Loss:  3.159, Val Acc: 45.84%, Time: 25.18s *\nIter: 150, Train Loss:  1.801, Train Acc: 82.81%, Val Loss:  2.768, Val Acc: 53.74%, Time: 31.22s *\nIter: 180, Train Loss:  1.805, Train Acc: 64.06%, Val Loss:  2.702, Val Acc: 58.60%, Time: 37.28s *\nEpoch: 2\nIter: 210, Train Loss:  1.068, Train Acc: 81.25%, Val Loss:  2.473, Val Acc: 62.48%, Time: 3.34s *\nIter: 240, Train Loss: 0.7442, Train Acc: 89.06%, Val Loss:  2.434, Val Acc: 61.78%, Time: 9.42s \nIter: 270, Train Loss: 0.5833, Train Acc: 90.62%, Val Loss:  2.586, Val Acc: 62.48%, Time: 15.47s *\nIter: 300, Train Loss: 0.7061, Train Acc: 89.06%, Val Loss:  2.422, Val Acc: 64.32%, Time: 21.57s *\nIter: 330, Train Loss: 0.4737, Train Acc: 93.75%, Val Loss:  2.515, Val Acc: 63.89%, Time: 27.62s \nIter: 360, Train Loss: 0.9829, Train Acc: 85.94%, Val Loss:  2.436, Val Acc: 66.57%, Time: 33.62s *\nIter: 390, Train Loss:  0.663, Train Acc: 87.50%, Val Loss:  2.549, Val Acc: 66.01%, Time: 39.60s \nEpoch: 3\nIter: 420, Train Loss:  0.757, Train Acc: 87.50%, Val Loss:  2.546, Val Acc: 68.05%, Time: 5.09s *\nIter: 450, Train Loss: 0.4227, Train Acc: 93.75%, Val Loss:  2.613, Val Acc: 67.21%, Time: 11.10s \nIter: 480, Train Loss: 0.4625, Train Acc: 93.75%, Val Loss:  2.685, Val Acc: 68.34%, Time: 17.13s *\nIter: 510, Train Loss: 0.5319, Train Acc: 92.19%, Val Loss:  2.743, Val Acc: 64.81%, Time: 23.13s \nIter: 540, Train Loss: 0.3241, Train Acc: 93.75%, Val Loss:  2.679, Val Acc: 66.29%, Time: 29.17s \nIter: 570, Train Loss: 0.4135, Train Acc: 95.31%, Val Loss:  2.797, Val Acc: 66.64%, Time: 35.19s \nEpoch: 4\nIter: 600, Train Loss: 0.1724, Train Acc: 98.44%, Val Loss:  2.655, Val Acc: 66.50%, Time: 2.43s \nIter: 630, Train Loss: 0.2557, Train Acc: 96.88%, Val Loss:   2.83, Val Acc: 65.94%, Time: 8.41s \nIter: 660, Train Loss: 0.1927, Train Acc: 98.44%, Val Loss:  2.668, Val Acc: 67.63%, Time: 14.43s \nIter: 690, Train Loss: 0.1259, Train Acc: 98.44%, Val Loss:  2.819, Val Acc: 67.14%, Time: 20.41s \nIter: 720, Train Loss: 0.09225, Train Acc: 100.00%, Val Loss:  2.964, Val Acc: 67.63%, Time: 26.41s \nIter: 750, Train Loss:  0.178, Train Acc: 98.44%, Val Loss:  3.032, Val Acc: 66.50%, Time: 32.40s \nIter: 780, Train Loss: 0.2783, Train Acc: 95.31%, Val Loss:  2.884, Val Acc: 66.85%, Time: 38.41s \nEpoch: 5\nIter: 810, Train Loss: 0.2555, Train Acc: 96.88%, Val Loss:  3.033, Val Acc: 67.91%, Time: 4.20s \nIter: 840, Train Loss: 0.1153, Train Acc: 98.44%, Val Loss:  3.005, Val Acc: 67.21%, Time: 10.30s \nIter: 870, Train Loss: 0.07159, Train Acc: 98.44%, Val Loss:  3.132, Val Acc: 67.70%, Time: 16.34s \nIter: 900, Train Loss: 0.1101, Train Acc: 98.44%, Val Loss:  3.082, Val Acc: 67.84%, Time: 22.36s \nIter: 930, Train Loss: 0.03276, Train Acc: 100.00%, Val Loss:  2.955, Val Acc: 68.34%, Time: 28.35s \nIter: 960, Train Loss: 0.2132, Train Acc: 96.88%, Val Loss:  2.849, Val Acc: 68.41%, Time: 34.43s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 990, Train Loss: 0.04438, Train Acc: 100.00%, Val Loss:  2.899, Val Acc: 67.07%, Time: 40.42s \nEpoch: 6\nIter: 1020, Train Loss: 0.1123, Train Acc: 100.00%, Val Loss:   3.08, Val Acc: 66.85%, Time: 6.01s \nIter: 1050, Train Loss: 0.05018, Train Acc: 98.44%, Val Loss:  3.226, Val Acc: 67.28%, Time: 12.00s \nIter: 1080, Train Loss: 0.1035, Train Acc: 100.00%, Val Loss:  3.172, Val Acc: 66.50%, Time: 18.01s \nIter: 1110, Train Loss: 0.0466, Train Acc: 100.00%, Val Loss:  3.109, Val Acc: 67.56%, Time: 24.10s \nIter: 1140, Train Loss: 0.05384, Train Acc: 100.00%, Val Loss:  2.995, Val Acc: 68.19%, Time: 30.16s \nIter: 1170, Train Loss: 0.01673, Train Acc: 100.00%, Val Loss:  3.063, Val Acc: 69.32%, Time: 36.21s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2696, Train Acc: 95.31%, Val Loss:  3.205, Val Acc: 68.55%, Time: 3.32s \nIter: 1230, Train Loss: 0.03057, Train Acc: 100.00%, Val Loss:  3.441, Val Acc: 67.77%, Time: 9.32s \nIter: 1260, Train Loss: 0.05744, Train Acc: 100.00%, Val Loss:  3.382, Val Acc: 66.50%, Time: 15.36s \nIter: 1290, Train Loss: 0.1462, Train Acc: 98.44%, Val Loss:  3.279, Val Acc: 67.70%, Time: 21.36s \nIter: 1320, Train Loss: 0.00874, Train Acc: 100.00%, Val Loss:  3.297, Val Acc: 67.00%, Time: 27.41s \nIter: 1350, Train Loss: 0.01034, Train Acc: 100.00%, Val Loss:  3.307, Val Acc: 66.78%, Time: 33.37s \nIter: 1380, Train Loss: 0.1305, Train Acc: 98.44%, Val Loss:  3.231, Val Acc: 67.28%, Time: 39.42s \nEpoch: 8\nIter: 1410, Train Loss: 0.03255, Train Acc: 100.00%, Val Loss:  3.369, Val Acc: 67.49%, Time: 5.18s \nIter: 1440, Train Loss: 0.03118, Train Acc: 100.00%, Val Loss:  3.654, Val Acc: 66.78%, Time: 11.24s \nIter: 1470, Train Loss: 0.1182, Train Acc: 98.44%, Val Loss:  3.535, Val Acc: 67.21%, Time: 17.27s \nIter: 1500, Train Loss: 0.02467, Train Acc: 100.00%, Val Loss:  3.576, Val Acc: 66.57%, Time: 23.27s \nIter: 1530, Train Loss: 0.07594, Train Acc: 98.44%, Val Loss:   3.38, Val Acc: 66.64%, Time: 29.22s \nIter: 1560, Train Loss: 0.03229, Train Acc: 100.00%, Val Loss:  3.524, Val Acc: 68.05%, Time: 35.20s \nEpoch: 9\nIter: 1590, Train Loss: 0.1091, Train Acc: 98.44%, Val Loss:  3.772, Val Acc: 66.01%, Time: 2.43s \nIter: 1620, Train Loss: 0.07521, Train Acc: 100.00%, Val Loss:  3.296, Val Acc: 68.27%, Time: 8.46s \nIter: 1650, Train Loss: 0.004785, Train Acc: 100.00%, Val Loss:  3.532, Val Acc: 68.34%, Time: 14.49s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.73      0.51      0.60       173\n                   Filter       0.56      0.48      0.51       168\n    Compute Derived Value       0.46      0.62      0.53       148\n            Find Extremum       0.56      0.62      0.59        93\n                     Sort       0.82      0.62      0.71       108\n          Determine Range       0.75      0.89      0.81       150\nCharacterize Distribution       0.88      0.88      0.88       210\n           Find Anomalies       0.64      0.41      0.50       120\n                  Cluster       0.70      0.76      0.73       123\n                Correlate       0.64      0.86      0.73       125\n\n                micro avg       0.67      0.67      0.67      1418\n                macro avg       0.67      0.66      0.66      1418\n             weighted avg       0.68      0.67      0.67      1418\n\nConfusion Matrix...\n[[ 88  24  42   2   2   4   7   3   1   0]\n [ 14  80  13  22   4   6   0  16  12   1]\n [ 15  30  92   1   0   7   0   3   0   0]\n [  0   0  19  58   0   7   5   0   0   4]\n [  0   1   9  16  67  11   0   1   3   0]\n [  1   1   6   0   5 134   1   0   0   2]\n [  1   5   6   2   0   6 184   1   4   1]\n [  1   3  10   1   2   1  10  49   6  37]\n [  0   0   3   1   2   2   1   4  93  17]\n [  0   0   1   0   0   1   2   0  13 108]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.449, Train Acc: 25.00%, Val Loss:  4.451, Val Acc: 24.20%, Time: 7.05s *\nIter: 60, Train Loss:  3.801, Train Acc: 39.06%, Val Loss:  3.755, Val Acc: 38.51%, Time: 12.94s *\nIter: 90, Train Loss:  3.379, Train Acc: 43.75%, Val Loss:  3.219, Val Acc: 46.48%, Time: 18.91s *\nIter: 120, Train Loss:   2.53, Train Acc: 62.50%, Val Loss:  2.602, Val Acc: 60.00%, Time: 24.80s *\nIter: 150, Train Loss:  1.781, Train Acc: 76.56%, Val Loss:  2.266, Val Acc: 63.77%, Time: 30.78s *\nIter: 180, Train Loss:  1.222, Train Acc: 79.69%, Val Loss:  2.221, Val Acc: 64.48%, Time: 36.75s *\nEpoch: 2\nIter: 210, Train Loss:  1.534, Train Acc: 75.00%, Val Loss:  2.324, Val Acc: 65.48%, Time: 3.24s *\nIter: 240, Train Loss:   1.36, Train Acc: 81.25%, Val Loss:  1.995, Val Acc: 68.54%, Time: 9.25s *\nIter: 270, Train Loss:   1.17, Train Acc: 82.81%, Val Loss:   2.02, Val Acc: 68.26%, Time: 15.13s \nIter: 300, Train Loss: 0.8268, Train Acc: 89.06%, Val Loss:  1.927, Val Acc: 68.90%, Time: 21.09s *\nIter: 330, Train Loss: 0.7847, Train Acc: 89.06%, Val Loss:   2.09, Val Acc: 68.54%, Time: 27.18s \nIter: 360, Train Loss: 0.8431, Train Acc: 92.19%, Val Loss:   2.09, Val Acc: 70.04%, Time: 33.10s *\nIter: 390, Train Loss: 0.6948, Train Acc: 87.50%, Val Loss:  2.159, Val Acc: 68.90%, Time: 39.06s \nEpoch: 3\nIter: 420, Train Loss: 0.4353, Train Acc: 93.75%, Val Loss:  2.093, Val Acc: 71.39%, Time: 5.09s *\nIter: 450, Train Loss: 0.4747, Train Acc: 95.31%, Val Loss:  2.114, Val Acc: 68.26%, Time: 11.04s \nIter: 480, Train Loss: 0.2347, Train Acc: 96.88%, Val Loss:  2.252, Val Acc: 69.68%, Time: 17.00s \nIter: 510, Train Loss: 0.5355, Train Acc: 89.06%, Val Loss:  2.333, Val Acc: 66.55%, Time: 22.92s \nIter: 540, Train Loss: 0.4071, Train Acc: 93.75%, Val Loss:  2.086, Val Acc: 71.25%, Time: 28.86s \nIter: 570, Train Loss: 0.3382, Train Acc: 96.88%, Val Loss:   2.32, Val Acc: 68.40%, Time: 34.83s \nEpoch: 4\nIter: 600, Train Loss: 0.3935, Train Acc: 96.88%, Val Loss:  2.139, Val Acc: 70.75%, Time: 2.34s \nIter: 630, Train Loss: 0.09461, Train Acc: 100.00%, Val Loss:  2.289, Val Acc: 71.81%, Time: 8.30s *\nIter: 660, Train Loss: 0.1375, Train Acc: 98.44%, Val Loss:  2.257, Val Acc: 69.75%, Time: 14.28s \nIter: 690, Train Loss: 0.1352, Train Acc: 96.88%, Val Loss:  2.375, Val Acc: 69.89%, Time: 20.11s \nIter: 720, Train Loss: 0.2979, Train Acc: 93.75%, Val Loss:  2.276, Val Acc: 70.89%, Time: 26.03s \nIter: 750, Train Loss: 0.1878, Train Acc: 96.88%, Val Loss:   2.41, Val Acc: 70.11%, Time: 31.99s \nIter: 780, Train Loss: 0.2485, Train Acc: 96.88%, Val Loss:  2.303, Val Acc: 70.60%, Time: 37.87s \nEpoch: 5\nIter: 810, Train Loss: 0.1066, Train Acc: 98.44%, Val Loss:   2.47, Val Acc: 70.11%, Time: 4.13s \nIter: 840, Train Loss: 0.08011, Train Acc: 100.00%, Val Loss:  2.457, Val Acc: 69.47%, Time: 10.01s \nIter: 870, Train Loss:  0.582, Train Acc: 89.06%, Val Loss:  2.578, Val Acc: 68.83%, Time: 15.93s \nIter: 900, Train Loss: 0.1384, Train Acc: 98.44%, Val Loss:  2.493, Val Acc: 70.68%, Time: 21.83s \nIter: 930, Train Loss: 0.05388, Train Acc: 100.00%, Val Loss:  2.346, Val Acc: 71.96%, Time: 27.79s *\nIter: 960, Train Loss: 0.1409, Train Acc: 98.44%, Val Loss:   2.45, Val Acc: 70.89%, Time: 33.77s \nIter: 990, Train Loss: 0.01776, Train Acc: 100.00%, Val Loss:  2.512, Val Acc: 71.17%, Time: 39.74s \nEpoch: 6\nIter: 1020, Train Loss: 0.2293, Train Acc: 96.88%, Val Loss:  2.732, Val Acc: 69.11%, Time: 6.01s \nIter: 1050, Train Loss: 0.3098, Train Acc: 93.75%, Val Loss:  2.431, Val Acc: 71.25%, Time: 11.98s \nIter: 1080, Train Loss: 0.06598, Train Acc: 100.00%, Val Loss:  2.244, Val Acc: 72.03%, Time: 17.94s *\nIter: 1110, Train Loss: 0.05642, Train Acc: 100.00%, Val Loss:  2.517, Val Acc: 70.04%, Time: 23.88s \nIter: 1140, Train Loss: 0.2485, Train Acc: 96.88%, Val Loss:  3.024, Val Acc: 66.69%, Time: 29.83s \nIter: 1170, Train Loss: 0.1703, Train Acc: 98.44%, Val Loss:  2.496, Val Acc: 70.75%, Time: 35.84s \nEpoch: 7\nIter: 1200, Train Loss: 0.09744, Train Acc: 98.44%, Val Loss:  2.336, Val Acc: 70.68%, Time: 3.23s \nIter: 1230, Train Loss: 0.03067, Train Acc: 100.00%, Val Loss:  2.559, Val Acc: 71.46%, Time: 9.16s \nIter: 1260, Train Loss: 0.02718, Train Acc: 100.00%, Val Loss:  2.662, Val Acc: 71.32%, Time: 15.22s \nIter: 1290, Train Loss: 0.08806, Train Acc: 100.00%, Val Loss:  2.575, Val Acc: 70.18%, Time: 21.13s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1320, Train Loss: 0.0818, Train Acc: 98.44%, Val Loss:  2.738, Val Acc: 69.25%, Time: 27.03s \nIter: 1350, Train Loss:   0.24, Train Acc: 96.88%, Val Loss:  2.668, Val Acc: 69.61%, Time: 33.03s \nIter: 1380, Train Loss: 0.01129, Train Acc: 100.00%, Val Loss:  2.863, Val Acc: 69.25%, Time: 38.93s \nEpoch: 8\nIter: 1410, Train Loss: 0.008572, Train Acc: 100.00%, Val Loss:   3.01, Val Acc: 70.53%, Time: 5.08s \nIter: 1440, Train Loss: 0.02655, Train Acc: 100.00%, Val Loss:  3.004, Val Acc: 69.96%, Time: 11.04s \nIter: 1470, Train Loss: 0.01636, Train Acc: 100.00%, Val Loss:  3.044, Val Acc: 69.68%, Time: 16.90s \nIter: 1500, Train Loss: 0.1023, Train Acc: 98.44%, Val Loss:  2.931, Val Acc: 70.25%, Time: 22.83s \nIter: 1530, Train Loss: 0.03058, Train Acc: 100.00%, Val Loss:  2.933, Val Acc: 69.40%, Time: 28.75s \nIter: 1560, Train Loss: 0.03356, Train Acc: 100.00%, Val Loss:  2.804, Val Acc: 70.75%, Time: 34.74s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.88      0.68        65\n                   Filter       0.48      0.67      0.56       150\n    Compute Derived Value       0.67      0.63      0.65       167\n            Find Extremum       0.66      0.68      0.67       117\n                     Sort       0.96      0.84      0.90       145\n          Determine Range       0.79      0.51      0.62       150\nCharacterize Distribution       0.85      0.78      0.82       172\n           Find Anomalies       0.85      0.56      0.68       143\n                  Cluster       0.72      0.75      0.73       165\n                Correlate       0.65      0.84      0.74       131\n\n                micro avg       0.70      0.70      0.70      1405\n                macro avg       0.72      0.71      0.70      1405\n             weighted avg       0.73      0.70      0.71      1405\n\nConfusion Matrix...\n[[ 57   3   2   1   1   0   0   0   0   1]\n [ 13 101   9  10   0   3   1   2   8   3]\n [ 15  21 105  17   0   1   6   0   0   2]\n [  2  15  15  79   0   2   0   0   0   4]\n [  0   4   0   8 122   0   0   0  11   0]\n [ 13  28   8   4   3  76   3   1   7   7]\n [  2  11   6   0   0   4 135   5   7   2]\n [  0  25   4   0   1   2   0  80   3  28]\n [  0   1   5   0   0   8  14   2 124  11]\n [  1   1   2   0   0   0   0   4  13 110]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.495, Train Acc: 17.19%, Val Loss:  4.544, Val Acc:  9.96%, Time: 7.00s *\nIter: 60, Train Loss:  3.787, Train Acc: 39.06%, Val Loss:  4.051, Val Acc: 29.30%, Time: 12.94s *\nIter: 90, Train Loss:  3.076, Train Acc: 53.12%, Val Loss:  3.245, Val Acc: 47.56%, Time: 18.89s *\nIter: 120, Train Loss:  2.433, Train Acc: 64.06%, Val Loss:  2.793, Val Acc: 52.58%, Time: 24.78s *\nIter: 150, Train Loss:  1.917, Train Acc: 70.31%, Val Loss:  2.722, Val Acc: 53.58%, Time: 30.78s *\nIter: 180, Train Loss:  1.744, Train Acc: 76.56%, Val Loss:  2.128, Val Acc: 64.26%, Time: 36.79s *\nEpoch: 2\nIter: 210, Train Loss:  1.757, Train Acc: 70.31%, Val Loss:  2.369, Val Acc: 61.17%, Time: 3.24s \nIter: 240, Train Loss:  1.105, Train Acc: 79.69%, Val Loss:  2.069, Val Acc: 66.98%, Time: 9.21s *\nIter: 270, Train Loss:  1.071, Train Acc: 79.69%, Val Loss:  2.039, Val Acc: 68.84%, Time: 15.22s *\nIter: 300, Train Loss: 0.6086, Train Acc: 92.19%, Val Loss:  2.139, Val Acc: 67.98%, Time: 21.16s \nIter: 330, Train Loss: 0.7291, Train Acc: 89.06%, Val Loss:  1.922, Val Acc: 68.98%, Time: 27.04s *\nIter: 360, Train Loss: 0.6399, Train Acc: 90.62%, Val Loss:  1.952, Val Acc: 70.99%, Time: 32.95s *\nIter: 390, Train Loss: 0.3996, Train Acc: 93.75%, Val Loss:  2.218, Val Acc: 67.12%, Time: 38.85s \nEpoch: 3\nIter: 420, Train Loss: 0.7391, Train Acc: 85.94%, Val Loss:  2.241, Val Acc: 66.62%, Time: 5.10s \nIter: 450, Train Loss:  0.673, Train Acc: 90.62%, Val Loss:  2.066, Val Acc: 68.91%, Time: 11.07s \nIter: 480, Train Loss: 0.2837, Train Acc: 96.88%, Val Loss:   2.08, Val Acc: 69.05%, Time: 16.97s \nIter: 510, Train Loss:  0.622, Train Acc: 89.06%, Val Loss:  2.273, Val Acc: 68.98%, Time: 22.90s \nIter: 540, Train Loss: 0.5107, Train Acc: 92.19%, Val Loss:  1.989, Val Acc: 69.70%, Time: 28.82s \nIter: 570, Train Loss: 0.3006, Train Acc: 98.44%, Val Loss:  1.945, Val Acc: 71.63%, Time: 34.84s *\nEpoch: 4\nIter: 600, Train Loss: 0.1451, Train Acc: 96.88%, Val Loss:  1.911, Val Acc: 73.07%, Time: 2.38s *\nIter: 630, Train Loss: 0.3171, Train Acc: 95.31%, Val Loss:  1.985, Val Acc: 71.35%, Time: 8.29s \nIter: 660, Train Loss: 0.4629, Train Acc: 95.31%, Val Loss:  2.292, Val Acc: 68.70%, Time: 14.24s \nIter: 690, Train Loss: 0.3429, Train Acc: 95.31%, Val Loss:   2.03, Val Acc: 70.99%, Time: 20.18s \nIter: 720, Train Loss: 0.2984, Train Acc: 95.31%, Val Loss:  2.003, Val Acc: 71.42%, Time: 26.07s \nIter: 750, Train Loss: 0.2665, Train Acc: 95.31%, Val Loss:  1.975, Val Acc: 70.49%, Time: 31.99s \nIter: 780, Train Loss: 0.07062, Train Acc: 100.00%, Val Loss:  2.053, Val Acc: 71.35%, Time: 37.90s \nEpoch: 5\nIter: 810, Train Loss: 0.09615, Train Acc: 98.44%, Val Loss:  2.205, Val Acc: 72.06%, Time: 4.14s \nIter: 840, Train Loss: 0.1972, Train Acc: 98.44%, Val Loss:   2.03, Val Acc: 73.57%, Time: 10.04s *\nIter: 870, Train Loss: 0.1309, Train Acc: 98.44%, Val Loss:  2.272, Val Acc: 71.06%, Time: 15.98s \nIter: 900, Train Loss: 0.06885, Train Acc: 100.00%, Val Loss:  2.228, Val Acc: 72.06%, Time: 21.87s \nIter: 930, Train Loss: 0.2016, Train Acc: 96.88%, Val Loss:  2.222, Val Acc: 72.35%, Time: 27.77s \nIter: 960, Train Loss: 0.2011, Train Acc: 98.44%, Val Loss:  2.132, Val Acc: 72.35%, Time: 33.76s \nIter: 990, Train Loss:  0.238, Train Acc: 93.55%, Val Loss:  2.093, Val Acc: 72.42%, Time: 39.69s \nEpoch: 6\nIter: 1020, Train Loss: 0.3195, Train Acc: 95.31%, Val Loss:  2.186, Val Acc: 71.99%, Time: 5.93s \nIter: 1050, Train Loss: 0.09732, Train Acc: 98.44%, Val Loss:  2.196, Val Acc: 71.42%, Time: 11.84s \nIter: 1080, Train Loss: 0.1271, Train Acc: 98.44%, Val Loss:  2.399, Val Acc: 71.99%, Time: 17.73s \nIter: 1110, Train Loss: 0.09225, Train Acc: 100.00%, Val Loss:  2.549, Val Acc: 70.42%, Time: 23.63s \nIter: 1140, Train Loss: 0.2853, Train Acc: 95.31%, Val Loss:   2.56, Val Acc: 71.42%, Time: 29.55s \nIter: 1170, Train Loss: 0.1491, Train Acc: 98.44%, Val Loss:  2.413, Val Acc: 71.85%, Time: 35.58s \nEpoch: 7\nIter: 1200, Train Loss: 0.07478, Train Acc: 100.00%, Val Loss:  2.573, Val Acc: 70.85%, Time: 3.26s \nIter: 1230, Train Loss: 0.02204, Train Acc: 100.00%, Val Loss:  2.496, Val Acc: 71.85%, Time: 9.24s \nIter: 1260, Train Loss: 0.01114, Train Acc: 100.00%, Val Loss:  2.201, Val Acc: 73.35%, Time: 15.21s \nIter: 1290, Train Loss: 0.09663, Train Acc: 98.44%, Val Loss:  2.347, Val Acc: 73.28%, Time: 21.20s \nIter: 1320, Train Loss: 0.2052, Train Acc: 98.44%, Val Loss:  2.306, Val Acc: 70.99%, Time: 27.15s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.92      0.65      0.77       107\n                   Filter       0.69      0.54      0.61       219\n    Compute Derived Value       0.37      0.75      0.49        83\n            Find Extremum       0.71      0.78      0.75       152\n                     Sort       0.98      0.77      0.86       146\n          Determine Range       0.69      0.70      0.69       152\nCharacterize Distribution       0.65      0.66      0.66       124\n           Find Anomalies       0.78      0.71      0.74       190\n                  Cluster       0.62      0.87      0.72        77\n                Correlate       0.77      0.72      0.74       146\n\n                micro avg       0.70      0.70      0.70      1396\n                macro avg       0.72      0.72      0.70      1396\n             weighted avg       0.73      0.70      0.71      1396\n\nConfusion Matrix...\n[[ 70  14  10   9   0   0   1   3   0   0]\n [  1 118  45  11   0  17   1  11   9   6]\n [  0   0  62   0   0   0  19   0   0   2]\n [  0   3   4 119   0  25   0   1   0   0]\n [  0   2   0  19 112   4   6   1   2   0]\n [  0   1  34   0   1 107   3   1   5   0]\n [  3  10   1   0   0   0  82   3   8  17]\n [  1  19  10   6   0   2   1 135  11   5]\n [  0   4   0   1   1   0   3   0  67   1]\n [  1   0   2   2   0   1  10  19   6 105]]\nFold:  8\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.529, Train Acc: 10.94%, Val Loss:  4.501, Val Acc: 17.12%, Time: 7.01s *\nIter: 60, Train Loss:  3.966, Train Acc: 26.56%, Val Loss:  4.005, Val Acc: 34.97%, Time: 12.99s *\nIter: 90, Train Loss:  2.969, Train Acc: 46.88%, Val Loss:  3.734, Val Acc: 42.62%, Time: 18.90s *\nIter: 120, Train Loss:  2.478, Train Acc: 64.06%, Val Loss:  3.056, Val Acc: 56.21%, Time: 24.92s *\nIter: 150, Train Loss:  1.916, Train Acc: 67.19%, Val Loss:   2.76, Val Acc: 59.15%, Time: 30.77s *\nIter: 180, Train Loss:  1.825, Train Acc: 71.88%, Val Loss:   2.48, Val Acc: 63.34%, Time: 36.71s *\nEpoch: 2\nIter: 210, Train Loss:  1.931, Train Acc: 68.75%, Val Loss:  2.574, Val Acc: 59.74%, Time: 3.12s \nIter: 240, Train Loss:  1.338, Train Acc: 81.25%, Val Loss:  2.541, Val Acc: 60.54%, Time: 9.01s \nIter: 270, Train Loss:   1.36, Train Acc: 76.56%, Val Loss:  2.822, Val Acc: 58.27%, Time: 14.89s \nIter: 300, Train Loss: 0.9426, Train Acc: 87.50%, Val Loss:   2.64, Val Acc: 61.94%, Time: 20.78s \nIter: 330, Train Loss:   1.15, Train Acc: 82.81%, Val Loss:  2.786, Val Acc: 60.62%, Time: 26.77s \nIter: 360, Train Loss:  1.123, Train Acc: 82.81%, Val Loss:  2.453, Val Acc: 66.57%, Time: 32.71s *\nIter: 390, Train Loss: 0.5208, Train Acc: 93.75%, Val Loss:  2.657, Val Acc: 65.03%, Time: 38.60s \nEpoch: 3\nIter: 420, Train Loss: 0.7691, Train Acc: 84.38%, Val Loss:  2.924, Val Acc: 62.60%, Time: 4.90s \nIter: 450, Train Loss: 0.8811, Train Acc: 92.19%, Val Loss:  2.561, Val Acc: 66.27%, Time: 10.84s \nIter: 480, Train Loss: 0.4336, Train Acc: 96.88%, Val Loss:  2.732, Val Acc: 64.29%, Time: 16.76s \nIter: 510, Train Loss: 0.5048, Train Acc: 93.75%, Val Loss:  2.646, Val Acc: 65.83%, Time: 22.68s \nIter: 540, Train Loss: 0.3515, Train Acc: 93.75%, Val Loss:  3.183, Val Acc: 60.10%, Time: 28.60s \nIter: 570, Train Loss: 0.6715, Train Acc: 93.75%, Val Loss:  2.721, Val Acc: 64.36%, Time: 34.53s \nEpoch: 4\nIter: 600, Train Loss: 0.2631, Train Acc: 96.88%, Val Loss:  2.809, Val Acc: 65.76%, Time: 1.93s \nIter: 630, Train Loss: 0.7031, Train Acc: 90.62%, Val Loss:  2.683, Val Acc: 66.50%, Time: 7.81s \nIter: 660, Train Loss: 0.4091, Train Acc: 93.75%, Val Loss:  2.759, Val Acc: 66.35%, Time: 13.79s \nIter: 690, Train Loss: 0.4991, Train Acc: 93.75%, Val Loss:  3.151, Val Acc: 64.07%, Time: 19.77s \nIter: 720, Train Loss:  0.346, Train Acc: 98.44%, Val Loss:  2.747, Val Acc: 66.64%, Time: 25.78s *\nIter: 750, Train Loss: 0.7639, Train Acc: 93.75%, Val Loss:  2.779, Val Acc: 66.05%, Time: 31.67s \nIter: 780, Train Loss: 0.2155, Train Acc: 98.44%, Val Loss:  2.848, Val Acc: 67.23%, Time: 37.62s *\nEpoch: 5\nIter: 810, Train Loss: 0.1641, Train Acc: 96.88%, Val Loss:  2.902, Val Acc: 64.66%, Time: 3.55s \nIter: 840, Train Loss: 0.09959, Train Acc: 98.44%, Val Loss:  2.843, Val Acc: 67.67%, Time: 9.53s *\nIter: 870, Train Loss: 0.4968, Train Acc: 92.19%, Val Loss:  2.858, Val Acc: 68.70%, Time: 15.45s *\nIter: 900, Train Loss: 0.1155, Train Acc: 100.00%, Val Loss:  3.195, Val Acc: 64.29%, Time: 21.43s \nIter: 930, Train Loss: 0.5006, Train Acc: 92.19%, Val Loss:  2.929, Val Acc: 67.30%, Time: 27.34s \nIter: 960, Train Loss: 0.2446, Train Acc: 98.44%, Val Loss:  3.004, Val Acc: 65.32%, Time: 33.28s \nIter: 990, Train Loss:  0.187, Train Acc: 95.31%, Val Loss:  2.855, Val Acc: 68.33%, Time: 39.21s \nEpoch: 6\nIter: 1020, Train Loss: 0.2869, Train Acc: 95.31%, Val Loss:  3.026, Val Acc: 64.66%, Time: 5.31s \nIter: 1050, Train Loss: 0.04491, Train Acc: 100.00%, Val Loss:  2.984, Val Acc: 67.67%, Time: 11.29s \nIter: 1080, Train Loss: 0.02943, Train Acc: 100.00%, Val Loss:  3.159, Val Acc: 67.01%, Time: 17.32s \nIter: 1110, Train Loss: 0.2081, Train Acc: 95.31%, Val Loss:  3.009, Val Acc: 67.52%, Time: 23.25s \nIter: 1140, Train Loss: 0.0939, Train Acc: 98.44%, Val Loss:  3.148, Val Acc: 66.86%, Time: 29.28s \nIter: 1170, Train Loss:  0.207, Train Acc: 95.31%, Val Loss:  3.087, Val Acc: 66.20%, Time: 35.23s \nEpoch: 7\nIter: 1200, Train Loss: 0.06107, Train Acc: 98.44%, Val Loss:   3.21, Val Acc: 66.79%, Time: 2.37s \nIter: 1230, Train Loss: 0.1493, Train Acc: 96.88%, Val Loss:   3.18, Val Acc: 67.01%, Time: 8.27s \nIter: 1260, Train Loss: 0.2525, Train Acc: 95.31%, Val Loss:  3.405, Val Acc: 64.51%, Time: 14.16s \nIter: 1290, Train Loss: 0.2192, Train Acc: 96.88%, Val Loss:  3.218, Val Acc: 64.00%, Time: 20.09s \nIter: 1320, Train Loss: 0.06273, Train Acc: 100.00%, Val Loss:  3.241, Val Acc: 66.05%, Time: 26.03s \nIter: 1350, Train Loss: 0.01793, Train Acc: 100.00%, Val Loss:   3.43, Val Acc: 65.69%, Time: 31.99s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.81      0.60      0.69       266\n                   Filter       0.71      0.42      0.53       157\n    Compute Derived Value       0.11      0.25      0.16        72\n            Find Extremum       0.75      0.72      0.73       172\n                     Sort       0.87      0.72      0.79        74\n          Determine Range       0.49      0.76      0.60        79\nCharacterize Distribution       0.81      0.73      0.76       190\n           Find Anomalies       0.58      0.87      0.70        93\n                  Cluster       0.84      0.82      0.83        91\n                Correlate       0.78      0.77      0.78       167\n\n                micro avg       0.66      0.66      0.66      1361\n                macro avg       0.67      0.67      0.66      1361\n             weighted avg       0.72      0.66      0.68      1361\n\nConfusion Matrix...\n[[159  15  47  13   0   3   5  18   2   4]\n [  6  66  34   2   3  21  17   5   0   3]\n [ 19   3  18   0   0  22   1   9   0   0]\n [  1   4  26 123   4   1   4   6   3   0]\n [  0   2   0   8  53   3   0   6   2   0]\n [ 12   1   0   2   0  60   1   0   0   3]\n [  0   1   8   1   1  11 138   8   6  16]\n [  0   0   8   0   0   0   1  81   0   3]\n [  0   1   0   1   0   1   1   5  75   7]\n [  0   0  17  15   0   0   3   2   1 129]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.597, Train Acc: 18.75%, Val Loss:  4.492, Val Acc: 23.57%, Time: 7.06s *\nIter: 60, Train Loss:  3.927, Train Acc: 34.38%, Val Loss:  4.063, Val Acc: 32.18%, Time: 13.16s *\nIter: 90, Train Loss:  3.154, Train Acc: 56.25%, Val Loss:  3.547, Val Acc: 42.91%, Time: 19.12s *\nIter: 120, Train Loss:  2.794, Train Acc: 54.69%, Val Loss:  3.112, Val Acc: 53.49%, Time: 25.10s *\nIter: 150, Train Loss:  1.908, Train Acc: 71.88%, Val Loss:  2.769, Val Acc: 59.56%, Time: 31.07s *\nIter: 180, Train Loss:  1.697, Train Acc: 67.19%, Val Loss:  2.656, Val Acc: 60.97%, Time: 37.08s *\nEpoch: 2\nIter: 210, Train Loss:  1.027, Train Acc: 92.19%, Val Loss:  2.589, Val Acc: 63.73%, Time: 3.34s *\nIter: 240, Train Loss:  1.076, Train Acc: 82.81%, Val Loss:  2.594, Val Acc: 62.81%, Time: 9.42s \nIter: 270, Train Loss:  1.139, Train Acc: 81.25%, Val Loss:  2.409, Val Acc: 67.47%, Time: 15.44s *\nIter: 300, Train Loss:  1.104, Train Acc: 84.38%, Val Loss:  2.405, Val Acc: 67.47%, Time: 21.44s \nIter: 330, Train Loss: 0.7312, Train Acc: 93.75%, Val Loss:  2.233, Val Acc: 69.30%, Time: 27.42s *\nIter: 360, Train Loss: 0.7019, Train Acc: 90.62%, Val Loss:  2.206, Val Acc: 69.02%, Time: 33.47s \nIter: 390, Train Loss: 0.6197, Train Acc: 92.19%, Val Loss:  2.251, Val Acc: 70.29%, Time: 39.40s *\nEpoch: 3\nIter: 420, Train Loss: 0.6069, Train Acc: 89.06%, Val Loss:  2.585, Val Acc: 65.35%, Time: 5.07s \nIter: 450, Train Loss: 0.5353, Train Acc: 93.75%, Val Loss:  2.426, Val Acc: 67.68%, Time: 11.09s \nIter: 480, Train Loss: 0.7051, Train Acc: 87.50%, Val Loss:  2.559, Val Acc: 67.25%, Time: 17.09s \nIter: 510, Train Loss: 0.3471, Train Acc: 98.44%, Val Loss:  2.126, Val Acc: 70.92%, Time: 23.09s *\nIter: 540, Train Loss: 0.5839, Train Acc: 93.75%, Val Loss:  2.499, Val Acc: 68.81%, Time: 29.17s \nIter: 570, Train Loss: 0.6385, Train Acc: 93.75%, Val Loss:  2.415, Val Acc: 69.44%, Time: 35.10s \nEpoch: 4\nIter: 600, Train Loss: 0.4311, Train Acc: 96.88%, Val Loss:  2.451, Val Acc: 68.38%, Time: 2.43s \nIter: 630, Train Loss: 0.1648, Train Acc: 98.44%, Val Loss:  2.416, Val Acc: 71.14%, Time: 8.42s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 660, Train Loss: 0.5952, Train Acc: 90.62%, Val Loss:  2.535, Val Acc: 69.94%, Time: 14.41s \nIter: 690, Train Loss: 0.2174, Train Acc: 98.44%, Val Loss:  2.487, Val Acc: 67.96%, Time: 20.39s \nIter: 720, Train Loss:  0.197, Train Acc: 98.44%, Val Loss:  2.611, Val Acc: 69.16%, Time: 26.40s \nIter: 750, Train Loss: 0.1927, Train Acc: 98.44%, Val Loss:  2.577, Val Acc: 69.65%, Time: 32.36s \nIter: 780, Train Loss: 0.3417, Train Acc: 96.88%, Val Loss:  2.387, Val Acc: 71.91%, Time: 38.30s *\nEpoch: 5\nIter: 810, Train Loss:  0.238, Train Acc: 96.88%, Val Loss:  2.278, Val Acc: 73.47%, Time: 4.22s *\nIter: 840, Train Loss: 0.1826, Train Acc: 98.44%, Val Loss:  2.315, Val Acc: 71.98%, Time: 10.23s \nIter: 870, Train Loss:  0.133, Train Acc: 98.44%, Val Loss:  2.235, Val Acc: 73.54%, Time: 16.24s *\nIter: 900, Train Loss: 0.1227, Train Acc: 96.88%, Val Loss:  2.589, Val Acc: 70.08%, Time: 22.24s \nIter: 930, Train Loss: 0.09693, Train Acc: 100.00%, Val Loss:  2.698, Val Acc: 66.55%, Time: 28.30s \nIter: 960, Train Loss: 0.2941, Train Acc: 95.31%, Val Loss:  2.768, Val Acc: 69.65%, Time: 34.36s \nIter: 990, Train Loss: 0.06584, Train Acc: 100.00%, Val Loss:   2.66, Val Acc: 69.87%, Time: 40.37s \nEpoch: 6\nIter: 1020, Train Loss: 0.1327, Train Acc: 98.44%, Val Loss:    2.9, Val Acc: 69.02%, Time: 6.01s \nIter: 1050, Train Loss: 0.1148, Train Acc: 98.44%, Val Loss:  2.742, Val Acc: 68.10%, Time: 11.99s \nIter: 1080, Train Loss: 0.1547, Train Acc: 96.88%, Val Loss:  2.615, Val Acc: 70.50%, Time: 18.03s \nIter: 1110, Train Loss: 0.2492, Train Acc: 95.31%, Val Loss:  2.802, Val Acc: 67.61%, Time: 24.01s \nIter: 1140, Train Loss: 0.1333, Train Acc: 98.44%, Val Loss:  2.854, Val Acc: 64.86%, Time: 30.07s \nIter: 1170, Train Loss: 0.03954, Train Acc: 100.00%, Val Loss:   2.73, Val Acc: 68.60%, Time: 36.00s \nEpoch: 7\nIter: 1200, Train Loss: 0.01135, Train Acc: 100.00%, Val Loss:  2.987, Val Acc: 66.90%, Time: 3.32s \nIter: 1230, Train Loss: 0.01214, Train Acc: 100.00%, Val Loss:  3.112, Val Acc: 67.25%, Time: 9.32s \nIter: 1260, Train Loss: 0.03919, Train Acc: 100.00%, Val Loss:   2.59, Val Acc: 69.23%, Time: 15.32s \nIter: 1290, Train Loss: 0.05483, Train Acc: 100.00%, Val Loss:  3.458, Val Acc: 62.74%, Time: 21.30s \nIter: 1320, Train Loss: 0.07025, Train Acc: 100.00%, Val Loss:  3.224, Val Acc: 65.42%, Time: 27.29s \nIter: 1350, Train Loss: 0.05911, Train Acc: 100.00%, Val Loss:  2.914, Val Acc: 70.08%, Time: 33.32s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.68      0.68      0.68        76\n                   Filter       0.33      0.83      0.47        94\n    Compute Derived Value       0.78      0.66      0.71       126\n            Find Extremum       0.94      0.65      0.77       270\n                     Sort       0.86      0.74      0.79       168\n          Determine Range       0.63      0.50      0.56       181\nCharacterize Distribution       0.51      0.69      0.58        90\n           Find Anomalies       0.63      0.70      0.66       122\n                  Cluster       0.75      0.79      0.77       169\n                Correlate       0.95      0.69      0.80       121\n\n                micro avg       0.68      0.68      0.68      1417\n                macro avg       0.71      0.69      0.68      1417\n             weighted avg       0.75      0.68      0.70      1417\n\nConfusion Matrix...\n[[ 52  10   6   3   0   0   5   0   0   0]\n [  1  78   0   0   0   1   0  14   0   0]\n [  0   8  83   0   0  10  18   4   3   0]\n [  2  29   5 175   8  32   0   9  10   0]\n [  9  12   5   4 124   3   0   0  11   0]\n [  5  53   8   2   1  91  18   2   1   0]\n [  8   9   0   0   2   0  62   3   6   0]\n [  0  18   0   2   1   3   3  86   9   0]\n [  0  14   0   0   8   1   0   8 134   4]\n [  0   4   0   0   0   3  16  11   4  83]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.513, Train Acc: 15.62%, Val Loss:  4.623, Val Acc: 12.23%, Time: 7.01s *\nIter: 60, Train Loss:   3.94, Train Acc: 35.94%, Val Loss:  4.012, Val Acc: 30.43%, Time: 13.03s *\nIter: 90, Train Loss:  2.993, Train Acc: 46.88%, Val Loss:  3.274, Val Acc: 49.50%, Time: 18.97s *\nIter: 120, Train Loss:  2.171, Train Acc: 68.75%, Val Loss:  2.645, Val Acc: 60.00%, Time: 24.86s *\nIter: 150, Train Loss:  1.743, Train Acc: 79.69%, Val Loss:  2.457, Val Acc: 65.04%, Time: 30.84s *\nIter: 180, Train Loss:   1.61, Train Acc: 71.88%, Val Loss:  2.345, Val Acc: 64.10%, Time: 36.82s \nEpoch: 2\nIter: 210, Train Loss: 0.9621, Train Acc: 87.50%, Val Loss:  2.075, Val Acc: 69.14%, Time: 3.29s *\nIter: 240, Train Loss:  1.055, Train Acc: 85.94%, Val Loss:  2.224, Val Acc: 66.98%, Time: 9.21s \nIter: 270, Train Loss: 0.9925, Train Acc: 85.94%, Val Loss:  2.168, Val Acc: 69.50%, Time: 15.21s *\nIter: 300, Train Loss:  1.166, Train Acc: 81.25%, Val Loss:  2.048, Val Acc: 69.93%, Time: 21.20s *\nIter: 330, Train Loss:  1.472, Train Acc: 76.56%, Val Loss:  2.011, Val Acc: 71.51%, Time: 27.12s *\nIter: 360, Train Loss: 0.6742, Train Acc: 89.06%, Val Loss:  2.029, Val Acc: 71.73%, Time: 33.13s *\nIter: 390, Train Loss: 0.6178, Train Acc: 90.62%, Val Loss:  2.182, Val Acc: 70.00%, Time: 39.06s \nEpoch: 3\nIter: 420, Train Loss: 0.4562, Train Acc: 93.75%, Val Loss:  2.058, Val Acc: 70.72%, Time: 5.06s \nIter: 450, Train Loss: 0.3709, Train Acc: 95.31%, Val Loss:  2.397, Val Acc: 70.72%, Time: 11.04s \nIter: 480, Train Loss: 0.2789, Train Acc: 95.31%, Val Loss:  2.275, Val Acc: 70.00%, Time: 17.02s \nIter: 510, Train Loss: 0.6687, Train Acc: 87.50%, Val Loss:  2.083, Val Acc: 72.52%, Time: 22.96s *\nIter: 540, Train Loss: 0.6279, Train Acc: 93.75%, Val Loss:  2.265, Val Acc: 71.08%, Time: 28.92s \nIter: 570, Train Loss: 0.3418, Train Acc: 96.88%, Val Loss:  2.429, Val Acc: 70.14%, Time: 34.89s \nEpoch: 4\nIter: 600, Train Loss: 0.5072, Train Acc: 95.31%, Val Loss:  2.329, Val Acc: 70.94%, Time: 2.37s \nIter: 630, Train Loss: 0.2825, Train Acc: 98.44%, Val Loss:  2.619, Val Acc: 67.70%, Time: 8.39s \nIter: 660, Train Loss: 0.2951, Train Acc: 96.88%, Val Loss:  2.566, Val Acc: 68.56%, Time: 14.43s \nIter: 690, Train Loss: 0.2105, Train Acc: 98.44%, Val Loss:  2.543, Val Acc: 69.64%, Time: 20.33s \nIter: 720, Train Loss: 0.2396, Train Acc: 95.31%, Val Loss:   2.46, Val Acc: 69.57%, Time: 26.24s \nIter: 750, Train Loss: 0.3338, Train Acc: 95.31%, Val Loss:  2.455, Val Acc: 69.57%, Time: 32.15s \nIter: 780, Train Loss: 0.2059, Train Acc: 96.88%, Val Loss:  2.389, Val Acc: 70.72%, Time: 38.11s \nEpoch: 5\nIter: 810, Train Loss: 0.1026, Train Acc: 100.00%, Val Loss:  2.258, Val Acc: 70.72%, Time: 4.18s \nIter: 840, Train Loss: 0.1984, Train Acc: 96.88%, Val Loss:  2.491, Val Acc: 69.57%, Time: 10.10s \nIter: 870, Train Loss: 0.2447, Train Acc: 95.31%, Val Loss:  2.441, Val Acc: 69.14%, Time: 15.99s \nIter: 900, Train Loss: 0.2511, Train Acc: 95.31%, Val Loss:   2.44, Val Acc: 70.07%, Time: 21.94s \nIter: 930, Train Loss: 0.1923, Train Acc: 96.88%, Val Loss:  2.597, Val Acc: 68.20%, Time: 27.89s \nIter: 960, Train Loss: 0.2307, Train Acc: 96.88%, Val Loss:  2.504, Val Acc: 68.78%, Time: 33.96s \nIter: 990, Train Loss: 0.09105, Train Acc: 100.00%, Val Loss:  2.722, Val Acc: 68.35%, Time: 39.92s \nEpoch: 6\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.61      0.98      0.75       128\n                   Filter       0.62      0.52      0.57       140\n    Compute Derived Value       0.53      0.78      0.63       127\n            Find Extremum       0.67      0.64      0.65       180\n                     Sort       0.77      0.94      0.85       109\n          Determine Range       0.84      0.57      0.68       127\nCharacterize Distribution       0.54      0.81      0.65        78\n           Find Anomalies       0.77      0.55      0.64       125\n                  Cluster       0.62      0.77      0.69        91\n                Correlate       0.90      0.52      0.66       285\n\n                micro avg       0.67      0.67      0.67      1390\n                macro avg       0.68      0.71      0.68      1390\n             weighted avg       0.71      0.67      0.67      1390\n\nConfusion Matrix...\n[[125   0   1   0   0   0   2   0   0   0]\n [ 30  73  14  10   2   3   1   0   7   0]\n [ 17   0  99   0   1   0   2   2   0   6]\n [ 17   6  17 115  15   8   0   1   1   0]\n [  0   0   4   0 103   1   0   0   1   0]\n [  4   0  15  16   4  72  10   5   0   1]\n [  1   1   0  13   0   0  63   0   0   0]\n [  0  31   3  15   3   1   0  69   1   2]\n [  0   0   4   0   4   0   5   0  70   8]\n [ 12   7  31   3   2   1  34  13  33 149]]\n[0.6661951909476662, 0.6694796061884669, 0.6789772727272727, 0.7129629629629629, 0.6720733427362482, 0.703914590747331, 0.6998567335243553, 0.662747979426892, 0.6831333803810868, 0.6748201438848921]\n0.6824161203527174, 0.016412765061964263, 0.017300573432347224, 0.0002693788569792347\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_info = {\n    # \"random\": False,\n    # \"expert\": [20, 4],\n    #\"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\nkf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, split_type))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "table\n37 91 315 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.346, Train Acc: 21.88%, Val Loss:  4.517, Val Acc: 16.52%, Time: 8.67s *\nIter: 60, Train Loss:  3.729, Train Acc: 42.19%, Val Loss:  4.021, Val Acc: 32.59%, Time: 14.56s *\nIter: 90, Train Loss:  2.826, Train Acc: 53.12%, Val Loss:  3.537, Val Acc: 43.65%, Time: 20.61s *\nIter: 120, Train Loss:   2.32, Train Acc: 59.38%, Val Loss:  2.981, Val Acc: 48.51%, Time: 26.59s *\nIter: 150, Train Loss:  2.123, Train Acc: 62.50%, Val Loss:  2.859, Val Acc: 54.33%, Time: 32.62s *\nIter: 180, Train Loss:  1.685, Train Acc: 73.44%, Val Loss:  2.602, Val Acc: 56.28%, Time: 38.57s *\nEpoch: 2\nIter: 210, Train Loss:  1.126, Train Acc: 89.06%, Val Loss:  2.623, Val Acc: 57.25%, Time: 3.06s *\nIter: 240, Train Loss:  1.011, Train Acc: 81.25%, Val Loss:  2.684, Val Acc: 55.23%, Time: 8.98s \nIter: 270, Train Loss: 0.9333, Train Acc: 85.94%, Val Loss:  2.522, Val Acc: 59.57%, Time: 14.96s *\nIter: 300, Train Loss: 0.8401, Train Acc: 89.06%, Val Loss:  2.452, Val Acc: 60.09%, Time: 20.95s *\nIter: 330, Train Loss:  1.116, Train Acc: 82.81%, Val Loss:  2.242, Val Acc: 63.15%, Time: 26.94s *\nIter: 360, Train Loss: 0.9518, Train Acc: 89.06%, Val Loss:   2.49, Val Acc: 60.76%, Time: 32.88s \nIter: 390, Train Loss: 0.5859, Train Acc: 90.62%, Val Loss:  2.581, Val Acc: 59.72%, Time: 38.88s \nEpoch: 3\nIter: 420, Train Loss:  0.541, Train Acc: 93.75%, Val Loss:  2.487, Val Acc: 60.46%, Time: 4.80s \nIter: 450, Train Loss: 0.3892, Train Acc: 93.75%, Val Loss:    2.8, Val Acc: 60.39%, Time: 10.80s \nIter: 480, Train Loss: 0.6147, Train Acc: 92.19%, Val Loss:  2.522, Val Acc: 63.23%, Time: 16.78s *\nIter: 510, Train Loss: 0.2662, Train Acc: 96.88%, Val Loss:  2.315, Val Acc: 64.42%, Time: 22.72s *\nIter: 540, Train Loss: 0.2922, Train Acc: 95.31%, Val Loss:  2.497, Val Acc: 62.63%, Time: 28.64s \nIter: 570, Train Loss: 0.4702, Train Acc: 93.75%, Val Loss:  2.516, Val Acc: 62.78%, Time: 34.63s \nEpoch: 4\nIter: 600, Train Loss:  0.286, Train Acc: 93.75%, Val Loss:  2.466, Val Acc: 60.99%, Time: 1.87s \nIter: 630, Train Loss: 0.3774, Train Acc: 96.88%, Val Loss:  2.726, Val Acc: 60.31%, Time: 7.85s \nIter: 660, Train Loss: 0.04668, Train Acc: 100.00%, Val Loss:  2.652, Val Acc: 63.68%, Time: 13.86s \nIter: 690, Train Loss: 0.2645, Train Acc: 98.44%, Val Loss:  3.082, Val Acc: 56.05%, Time: 19.85s \nIter: 210, Train Loss: 0.9628, Train Acc: 89.06%, Val Loss:  2.431, Val Acc: 62.19%, Time: 4.60s *\nIter: 240, Train Loss:  1.274, Train Acc: 79.69%, Val Loss:  2.488, Val Acc: 62.93%, Time: 11.00s *\nIter: 270, Train Loss:  0.919, Train Acc: 89.06%, Val Loss:  2.637, Val Acc: 62.65%, Time: 17.46s \nIter: 300, Train Loss: 0.9574, Train Acc: 85.94%, Val Loss:  2.774, Val Acc: 60.55%, Time: 23.88s \nIter: 330, Train Loss:  1.278, Train Acc: 82.81%, Val Loss:  2.632, Val Acc: 62.14%, Time: 30.30s \nIter: 360, Train Loss: 0.5577, Train Acc: 92.19%, Val Loss:  2.891, Val Acc: 61.63%, Time: 36.75s \nEpoch: 3\nIter: 390, Train Loss: 0.7413, Train Acc: 92.19%, Val Loss:  2.806, Val Acc: 62.76%, Time: 2.82s \nIter: 420, Train Loss: 0.4098, Train Acc: 93.75%, Val Loss:  3.063, Val Acc: 60.03%, Time: 9.23s \nIter: 450, Train Loss: 0.9078, Train Acc: 85.94%, Val Loss:  2.881, Val Acc: 62.88%, Time: 15.76s \nIter: 480, Train Loss: 0.7035, Train Acc: 92.19%, Val Loss:  2.446, Val Acc: 66.23%, Time: 22.16s *\nIter: 510, Train Loss: 0.4946, Train Acc: 93.75%, Val Loss:  2.857, Val Acc: 63.45%, Time: 28.59s \nIter: 540, Train Loss: 0.2722, Train Acc: 93.75%, Val Loss:  2.801, Val Acc: 65.15%, Time: 35.08s \nIter: 570, Train Loss: 0.2407, Train Acc: 96.88%, Val Loss:  2.622, Val Acc: 63.79%, Time: 41.58s \nEpoch: 4\nIter: 600, Train Loss: 0.1186, Train Acc: 98.44%, Val Loss:   2.59, Val Acc: 67.20%, Time: 5.46s *\nIter: 630, Train Loss: 0.09376, Train Acc: 100.00%, Val Loss:  2.696, Val Acc: 64.58%, Time: 11.92s \nIter: 660, Train Loss: 0.1191, Train Acc: 98.44%, Val Loss:  2.515, Val Acc: 67.37%, Time: 18.34s *\nIter: 690, Train Loss: 0.1742, Train Acc: 98.44%, Val Loss:  3.248, Val Acc: 62.31%, Time: 24.73s \nIter: 720, Train Loss: 0.3133, Train Acc: 96.88%, Val Loss:  2.804, Val Acc: 65.43%, Time: 31.20s \nIter: 750, Train Loss: 0.4408, Train Acc: 92.19%, Val Loss:  2.876, Val Acc: 65.55%, Time: 37.63s \nEpoch: 5\nIter: 780, Train Loss: 0.02379, Train Acc: 100.00%, Val Loss:  2.731, Val Acc: 67.42%, Time: 3.68s *\nIter: 810, Train Loss: 0.09542, Train Acc: 98.44%, Val Loss:  3.394, Val Acc: 63.67%, Time: 10.11s \nIter: 840, Train Loss: 0.08564, Train Acc: 100.00%, Val Loss:  2.666, Val Acc: 65.43%, Time: 16.54s \nIter: 870, Train Loss:  0.223, Train Acc: 98.44%, Val Loss:  3.204, Val Acc: 64.35%, Time: 23.07s \nIter: 900, Train Loss: 0.4446, Train Acc: 95.31%, Val Loss:  2.713, Val Acc: 66.17%, Time: 29.48s \nIter: 930, Train Loss: 0.05769, Train Acc: 100.00%, Val Loss:  3.005, Val Acc: 63.90%, Time: 35.91s \nIter: 960, Train Loss: 0.3804, Train Acc: 96.15%, Val Loss:  3.106, Val Acc: 63.56%, Time: 42.39s \nEpoch: 6\nIter: 990, Train Loss: 0.07327, Train Acc: 100.00%, Val Loss:  3.217, Val Acc: 64.30%, Time: 6.49s \nIter: 1020, Train Loss: 0.04854, Train Acc: 100.00%, Val Loss:  3.359, Val Acc: 63.50%, Time: 12.93s \nIter: 1050, Train Loss: 0.2345, Train Acc: 96.88%, Val Loss:  3.033, Val Acc: 64.18%, Time: 19.33s \nIter: 1080, Train Loss: 0.2994, Train Acc: 93.75%, Val Loss:  3.043, Val Acc: 65.66%, Time: 25.76s \nIter: 1110, Train Loss: 0.1368, Train Acc: 98.44%, Val Loss:  2.939, Val Acc: 64.87%, Time: 32.27s \nIter: 1140, Train Loss: 0.07376, Train Acc: 98.44%, Val Loss:  2.871, Val Acc: 65.55%, Time: 38.66s \nEpoch: 7\nIter: 1170, Train Loss: 0.1588, Train Acc: 96.88%, Val Loss:  2.831, Val Acc: 65.38%, Time: 4.60s \nIter: 1200, Train Loss: 0.0135, Train Acc: 100.00%, Val Loss:  3.256, Val Acc: 63.73%, Time: 11.08s \nIter: 1230, Train Loss: 0.02041, Train Acc: 100.00%, Val Loss:  3.179, Val Acc: 65.26%, Time: 17.47s \nIter: 1260, Train Loss: 0.1129, Train Acc: 98.44%, Val Loss:  2.986, Val Acc: 67.14%, Time: 23.94s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.41      0.69      0.52       150\n                   Filter       0.58      0.55      0.57       193\n    Compute Derived Value       0.69      0.63      0.65       198\n            Find Extremum       0.62      0.60      0.61       201\n                     Sort       0.76      0.95      0.85       171\n          Determine Range       0.66      0.65      0.66       149\nCharacterize Distribution       0.88      0.62      0.73       218\n           Find Anomalies       0.68      0.78      0.72       161\n                  Cluster       0.92      0.52      0.66       147\n                Correlate       0.69      0.67      0.68       171\n\n                micro avg       0.66      0.66      0.66      1759\n                macro avg       0.69      0.67      0.66      1759\n             weighted avg       0.69      0.66      0.67      1759\n\nConfusion Matrix...\n[[104  25   0  12   3   4   0   1   1   0]\n [ 57 107   1   7   0   9   0  12   0   0]\n [ 29   3 124  20   4   1   3   8   0   6]\n [ 13  12  12 121  13   4   8  17   0   1]\n [  3   3   0   0 162   0   0   1   1   1]\n [ 19   3  11   2  13  97   2   2   0   0]\n [ 18   3  23  15   5   5 136   2   5   6]\n [  2  18   1   0   1   8   4 125   0   2]\n [  4   5   0   2   7   9   1   8  76  35]\n [  4   5   9  17   4  10   0   8   0 114]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.507, Train Acc: 20.31%, Val Loss:  4.514, Val Acc: 15.94%, Time: 7.21s *\nIter: 60, Train Loss:  3.784, Train Acc: 39.06%, Val Loss:  4.229, Val Acc: 24.44%, Time: 13.19s *\nIter: 90, Train Loss:   2.99, Train Acc: 60.94%, Val Loss:  3.735, Val Acc: 33.71%, Time: 19.31s *\nIter: 120, Train Loss:  2.367, Train Acc: 67.19%, Val Loss:  3.264, Val Acc: 43.82%, Time: 25.39s *\nIter: 150, Train Loss:  1.692, Train Acc: 76.56%, Val Loss:  3.159, Val Acc: 46.14%, Time: 31.49s *\nIter: 180, Train Loss:  1.186, Train Acc: 84.38%, Val Loss:  2.858, Val Acc: 51.40%, Time: 37.56s *\nEpoch: 2\nIter: 210, Train Loss:  1.425, Train Acc: 82.81%, Val Loss:  2.663, Val Acc: 56.04%, Time: 3.35s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 240, Train Loss:   1.12, Train Acc: 81.25%, Val Loss:  2.716, Val Acc: 55.41%, Time: 9.41s \nIter: 270, Train Loss:  1.204, Train Acc: 85.94%, Val Loss:  2.859, Val Acc: 55.13%, Time: 15.53s \nIter: 300, Train Loss: 0.8986, Train Acc: 87.50%, Val Loss:  2.597, Val Acc: 56.53%, Time: 21.71s *\nIter: 330, Train Loss:   1.07, Train Acc: 81.25%, Val Loss:  2.424, Val Acc: 59.83%, Time: 27.82s *\nIter: 360, Train Loss: 0.9322, Train Acc: 84.38%, Val Loss:  2.662, Val Acc: 57.09%, Time: 33.87s \nIter: 390, Train Loss:   0.66, Train Acc: 93.75%, Val Loss:  2.664, Val Acc: 61.31%, Time: 40.02s *\nEpoch: 3\nIter: 420, Train Loss: 0.5858, Train Acc: 89.06%, Val Loss:  2.737, Val Acc: 57.72%, Time: 5.17s \nIter: 450, Train Loss:  0.517, Train Acc: 90.62%, Val Loss:  2.806, Val Acc: 56.18%, Time: 11.25s \nIter: 480, Train Loss: 0.6053, Train Acc: 93.75%, Val Loss:  2.637, Val Acc: 59.06%, Time: 17.39s \nIter: 510, Train Loss:  0.327, Train Acc: 93.75%, Val Loss:  2.818, Val Acc: 58.29%, Time: 23.57s \nIter: 540, Train Loss: 0.2402, Train Acc: 96.88%, Val Loss:  2.995, Val Acc: 58.64%, Time: 29.69s \nIter: 570, Train Loss: 0.3907, Train Acc: 93.75%, Val Loss:  3.079, Val Acc: 57.79%, Time: 35.90s \nEpoch: 4\nIter: 600, Train Loss: 0.1413, Train Acc: 100.00%, Val Loss:  3.035, Val Acc: 57.65%, Time: 2.48s \nIter: 630, Train Loss: 0.1878, Train Acc: 98.44%, Val Loss:  3.034, Val Acc: 58.36%, Time: 8.64s \nIter: 660, Train Loss: 0.7141, Train Acc: 93.75%, Val Loss:  3.224, Val Acc: 57.58%, Time: 14.74s \nIter: 690, Train Loss: 0.5269, Train Acc: 92.19%, Val Loss:  3.051, Val Acc: 58.85%, Time: 20.88s \nIter: 720, Train Loss: 0.3272, Train Acc: 95.31%, Val Loss:  3.278, Val Acc: 56.95%, Time: 26.99s \nIter: 750, Train Loss: 0.1514, Train Acc: 96.88%, Val Loss:  3.153, Val Acc: 57.72%, Time: 33.09s \nIter: 780, Train Loss: 0.08509, Train Acc: 100.00%, Val Loss:  3.389, Val Acc: 57.65%, Time: 39.18s \nEpoch: 5\nIter: 810, Train Loss: 0.3155, Train Acc: 95.31%, Val Loss:  3.221, Val Acc: 55.55%, Time: 4.28s \nIter: 840, Train Loss: 0.1113, Train Acc: 100.00%, Val Loss:  3.246, Val Acc: 56.74%, Time: 10.40s \nIter: 870, Train Loss: 0.2591, Train Acc: 93.75%, Val Loss:  3.124, Val Acc: 59.13%, Time: 16.54s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.30      0.35      0.32       126\n                   Filter       0.42      0.56      0.48       140\n    Compute Derived Value       0.44      0.43      0.44       166\n            Find Extremum       0.78      0.55      0.64       183\n                     Sort       0.89      0.72      0.80       133\n          Determine Range       0.64      0.65      0.65       124\nCharacterize Distribution       0.64      0.80      0.72       147\n           Find Anomalies       0.68      0.56      0.61       138\n                  Cluster       0.79      0.70      0.74       145\n                Correlate       0.51      0.58      0.54       122\n\n                micro avg       0.59      0.59      0.59      1424\n                macro avg       0.61      0.59      0.59      1424\n             weighted avg       0.61      0.59      0.59      1424\n\nConfusion Matrix...\n[[ 44  24  35   5   0   0   5   6   0   7]\n [ 19  78  14   1   0   7   2   6   4   9]\n [ 36  26  72   2   3   5  14   0   1   7]\n [ 26  15  12 100   0  19   8   0   0   3]\n [  2   7   5   7  96   7   4   0   5   0]\n [ 10   9   2   8   6  81   3   3   1   1]\n [  4   1   6   2   0   5 118   1   1   9]\n [  0  19  15   2   0   0   1  77   0  24]\n [  3   2   1   1   3   0  15  10 101   9]\n [  5   5   1   0   0   2  13  10  15  71]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.578, Train Acc: 10.94%, Val Loss:  4.606, Val Acc:  8.80%, Time: 8.58s *\nIter: 60, Train Loss:  4.092, Train Acc: 37.50%, Val Loss:  4.082, Val Acc: 32.10%, Time: 16.05s *\nIter: 90, Train Loss:  3.182, Train Acc: 46.88%, Val Loss:  3.327, Val Acc: 43.55%, Time: 23.49s *\nIter: 120, Train Loss:  2.579, Train Acc: 65.62%, Val Loss:  2.957, Val Acc: 51.67%, Time: 30.94s *\nIter: 150, Train Loss:  1.794, Train Acc: 73.44%, Val Loss:  2.635, Val Acc: 57.86%, Time: 38.39s *\nEpoch: 2\nIter: 180, Train Loss:  1.367, Train Acc: 79.69%, Val Loss:  2.631, Val Acc: 58.15%, Time: 3.37s *\nIter: 210, Train Loss: 0.7919, Train Acc: 92.19%, Val Loss:  2.469, Val Acc: 61.78%, Time: 10.93s *\nIter: 240, Train Loss:  1.242, Train Acc: 79.69%, Val Loss:  2.418, Val Acc: 63.59%, Time: 18.39s *\nIter: 270, Train Loss:  1.029, Train Acc: 87.50%, Val Loss:  2.379, Val Acc: 64.35%, Time: 25.81s *\nIter: 300, Train Loss: 0.8609, Train Acc: 87.50%, Val Loss:  2.334, Val Acc: 64.53%, Time: 33.27s *\nIter: 330, Train Loss: 0.9424, Train Acc: 79.69%, Val Loss:  2.322, Val Acc: 65.91%, Time: 40.76s *\nEpoch: 3\nIter: 360, Train Loss: 0.6281, Train Acc: 92.19%, Val Loss:  2.296, Val Acc: 65.76%, Time: 3.84s \nIter: 390, Train Loss: 0.4118, Train Acc: 95.31%, Val Loss:  2.458, Val Acc: 63.70%, Time: 11.31s \nIter: 420, Train Loss: 0.5756, Train Acc: 92.19%, Val Loss:  2.553, Val Acc: 64.93%, Time: 18.79s \nIter: 450, Train Loss:  0.408, Train Acc: 95.31%, Val Loss:  2.336, Val Acc: 66.88%, Time: 26.29s *\nIter: 480, Train Loss: 0.3944, Train Acc: 93.75%, Val Loss:   2.41, Val Acc: 66.41%, Time: 33.76s \nIter: 510, Train Loss: 0.4091, Train Acc: 92.19%, Val Loss:  2.495, Val Acc: 65.76%, Time: 41.16s \nEpoch: 4\nIter: 540, Train Loss: 0.5478, Train Acc: 93.75%, Val Loss:  2.542, Val Acc: 65.83%, Time: 4.32s \nIter: 570, Train Loss: 0.6504, Train Acc: 95.31%, Val Loss:  2.577, Val Acc: 66.85%, Time: 11.85s \nIter: 600, Train Loss: 0.1173, Train Acc: 100.00%, Val Loss:  2.537, Val Acc: 66.38%, Time: 19.39s \nIter: 630, Train Loss: 0.4668, Train Acc: 92.19%, Val Loss:  2.436, Val Acc: 67.25%, Time: 26.85s *\nIter: 660, Train Loss: 0.2732, Train Acc: 96.88%, Val Loss:  2.513, Val Acc: 66.49%, Time: 34.34s \nIter: 690, Train Loss: 0.1413, Train Acc: 95.31%, Val Loss:  2.665, Val Acc: 66.49%, Time: 41.94s \nEpoch: 5\nIter: 720, Train Loss: 0.1822, Train Acc: 98.44%, Val Loss:  2.476, Val Acc: 67.68%, Time: 4.73s *\nIter: 750, Train Loss: 0.09263, Train Acc: 100.00%, Val Loss:  2.855, Val Acc: 65.91%, Time: 12.22s \nIter: 780, Train Loss: 0.1494, Train Acc: 98.44%, Val Loss:  2.744, Val Acc: 67.14%, Time: 19.67s \nIter: 810, Train Loss: 0.04047, Train Acc: 100.00%, Val Loss:  2.708, Val Acc: 66.92%, Time: 27.13s \nIter: 840, Train Loss: 0.5076, Train Acc: 93.75%, Val Loss:  2.897, Val Acc: 65.25%, Time: 34.58s \nIter: 870, Train Loss: 0.1677, Train Acc: 96.88%, Val Loss:  2.885, Val Acc: 65.72%, Time: 42.07s \nEpoch: 6\nIter: 900, Train Loss: 0.09126, Train Acc: 98.44%, Val Loss:  2.758, Val Acc: 68.12%, Time: 5.20s *\nIter: 930, Train Loss: 0.07369, Train Acc: 98.44%, Val Loss:  2.784, Val Acc: 67.28%, Time: 12.81s \nIter: 960, Train Loss: 0.2959, Train Acc: 96.88%, Val Loss:  2.899, Val Acc: 65.94%, Time: 20.26s \nIter: 990, Train Loss: 0.02269, Train Acc: 100.00%, Val Loss:  2.978, Val Acc: 66.12%, Time: 27.70s \nIter: 1020, Train Loss: 0.2896, Train Acc: 93.75%, Val Loss:  2.764, Val Acc: 66.67%, Time: 35.18s \nIter: 1050, Train Loss: 0.4371, Train Acc: 93.75%, Val Loss:  3.043, Val Acc: 65.65%, Time: 42.71s \nEpoch: 7\nIter: 1080, Train Loss: 0.07365, Train Acc: 98.44%, Val Loss:  3.103, Val Acc: 65.94%, Time: 5.61s \nIter: 1110, Train Loss: 0.09593, Train Acc: 98.44%, Val Loss:  3.109, Val Acc: 66.59%, Time: 13.11s \nIter: 1140, Train Loss: 0.03418, Train Acc: 100.00%, Val Loss:  3.118, Val Acc: 67.39%, Time: 20.50s \nIter: 1170, Train Loss: 0.03751, Train Acc: 100.00%, Val Loss:  2.926, Val Acc: 67.28%, Time: 28.04s \nIter: 1200, Train Loss: 0.09396, Train Acc: 98.44%, Val Loss:  3.031, Val Acc: 66.74%, Time: 35.46s \nIter: 1230, Train Loss: 0.04875, Train Acc: 100.00%, Val Loss:  3.043, Val Acc: 66.63%, Time: 42.87s \nEpoch: 8\nIter: 1260, Train Loss: 0.0551, Train Acc: 100.00%, Val Loss:  3.304, Val Acc: 64.64%, Time: 6.08s \nIter: 1290, Train Loss: 0.06616, Train Acc: 98.44%, Val Loss:  3.098, Val Acc: 67.32%, Time: 13.51s \nIter: 1320, Train Loss: 0.06467, Train Acc: 98.44%, Val Loss:  2.992, Val Acc: 67.10%, Time: 20.98s \nIter: 1350, Train Loss: 0.0327, Train Acc: 100.00%, Val Loss:  3.137, Val Acc: 66.96%, Time: 28.40s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1380, Train Loss: 0.04614, Train Acc: 100.00%, Val Loss:  3.479, Val Acc: 63.95%, Time: 35.89s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.79      0.27      0.41       279\n                   Filter       0.47      0.47      0.47       243\n    Compute Derived Value       0.67      0.51      0.58       301\n            Find Extremum       0.56      0.81      0.66       261\n                     Sort       0.75      0.84      0.79       265\n          Determine Range       0.58      0.67      0.62       319\nCharacterize Distribution       0.78      0.65      0.71       267\n           Find Anomalies       0.70      0.57      0.63       270\n                  Cluster       0.65      0.82      0.72       271\n                Correlate       0.65      0.81      0.72       284\n\n                micro avg       0.64      0.64      0.64      2760\n                macro avg       0.66      0.64      0.63      2760\n             weighted avg       0.66      0.64      0.63      2760\n\nConfusion Matrix...\n[[ 76  15  29  63  21  39  12   0   2  22]\n [  2 113   8  21   8  33   0  28  21   9]\n [  6  39 155  17   6  61   6   3   5   3]\n [  9  20   3 212   6   1   0   2   7   1]\n [  0   8   0  14 223   4   2   1  13   0]\n [  0  30   6  14  20 215  14   6   8   6]\n [  0   0  17  10   0   4 174   5  29  28]\n [  0  13   5  20   8   2   2 154  25  41]\n [  0   0   2   2   3   7  11  11 223  12]\n [  3   3   5   7   4   7   3  11  12 229]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.439, Train Acc: 15.62%, Val Loss:  4.498, Val Acc: 13.98%, Time: 7.52s *\nIter: 60, Train Loss:  3.853, Train Acc: 37.50%, Val Loss:  3.902, Val Acc: 38.48%, Time: 13.93s *\nIter: 90, Train Loss:  3.411, Train Acc: 45.31%, Val Loss:  3.188, Val Acc: 49.29%, Time: 20.26s *\nIter: 120, Train Loss:  2.493, Train Acc: 56.25%, Val Loss:  2.787, Val Acc: 54.23%, Time: 26.62s *\nIter: 150, Train Loss:  1.835, Train Acc: 67.19%, Val Loss:  2.687, Val Acc: 58.58%, Time: 33.05s *\nIter: 180, Train Loss:  1.674, Train Acc: 73.44%, Val Loss:  2.324, Val Acc: 63.51%, Time: 39.38s *\nEpoch: 2\nIter: 210, Train Loss:   1.11, Train Acc: 82.81%, Val Loss:  2.294, Val Acc: 64.57%, Time: 4.37s *\nIter: 240, Train Loss:  1.064, Train Acc: 87.50%, Val Loss:  2.255, Val Acc: 65.45%, Time: 10.66s *\nIter: 270, Train Loss: 0.8554, Train Acc: 87.50%, Val Loss:  2.356, Val Acc: 63.98%, Time: 17.06s \nIter: 300, Train Loss: 0.8084, Train Acc: 89.06%, Val Loss:  2.269, Val Acc: 65.80%, Time: 23.44s *\nIter: 330, Train Loss: 0.8856, Train Acc: 85.94%, Val Loss:  2.174, Val Acc: 66.39%, Time: 29.76s *\nIter: 360, Train Loss:  1.153, Train Acc: 85.94%, Val Loss:  2.193, Val Acc: 67.57%, Time: 36.06s *\nEpoch: 3\nIter: 390, Train Loss: 0.8649, Train Acc: 84.38%, Val Loss:  2.382, Val Acc: 66.39%, Time: 2.43s \nIter: 420, Train Loss: 0.3872, Train Acc: 95.31%, Val Loss:  2.215, Val Acc: 68.16%, Time: 8.75s *\nIter: 450, Train Loss: 0.4047, Train Acc: 95.31%, Val Loss:  2.696, Val Acc: 62.57%, Time: 15.11s \nIter: 480, Train Loss: 0.4773, Train Acc: 93.75%, Val Loss:  2.207, Val Acc: 68.21%, Time: 21.43s *\nIter: 510, Train Loss: 0.4739, Train Acc: 90.62%, Val Loss:   2.27, Val Acc: 68.57%, Time: 27.76s *\nIter: 540, Train Loss:  0.463, Train Acc: 90.62%, Val Loss:   2.32, Val Acc: 67.74%, Time: 34.10s \nIter: 570, Train Loss: 0.2683, Train Acc: 96.88%, Val Loss:  2.311, Val Acc: 68.04%, Time: 40.55s \nEpoch: 4\nIter: 600, Train Loss:  0.217, Train Acc: 98.44%, Val Loss:  2.321, Val Acc: 69.45%, Time: 4.99s *\nIter: 630, Train Loss: 0.1259, Train Acc: 98.44%, Val Loss:  2.437, Val Acc: 68.80%, Time: 11.27s \nIter: 660, Train Loss: 0.1848, Train Acc: 96.88%, Val Loss:  2.431, Val Acc: 68.33%, Time: 17.62s \nIter: 690, Train Loss: 0.1656, Train Acc: 98.44%, Val Loss:  2.563, Val Acc: 67.45%, Time: 23.95s \nIter: 720, Train Loss: 0.1209, Train Acc: 98.44%, Val Loss:  2.529, Val Acc: 66.75%, Time: 30.27s \nIter: 750, Train Loss: 0.2919, Train Acc: 96.88%, Val Loss:   2.43, Val Acc: 68.63%, Time: 36.63s \nEpoch: 5\nIter: 780, Train Loss: 0.1166, Train Acc: 98.44%, Val Loss:  2.321, Val Acc: 69.86%, Time: 3.05s *\nIter: 810, Train Loss: 0.04637, Train Acc: 100.00%, Val Loss:    2.5, Val Acc: 69.80%, Time: 9.44s \nIter: 840, Train Loss: 0.05232, Train Acc: 100.00%, Val Loss:  2.593, Val Acc: 68.92%, Time: 15.77s \nIter: 870, Train Loss: 0.1865, Train Acc: 98.44%, Val Loss:   2.69, Val Acc: 67.39%, Time: 22.23s \nIter: 900, Train Loss: 0.3446, Train Acc: 96.88%, Val Loss:  2.563, Val Acc: 69.15%, Time: 28.54s \nIter: 930, Train Loss: 0.1474, Train Acc: 98.44%, Val Loss:  2.835, Val Acc: 66.69%, Time: 34.89s \nIter: 960, Train Loss:  0.116, Train Acc: 98.44%, Val Loss:  2.585, Val Acc: 69.27%, Time: 41.24s \nEpoch: 6\nIter: 990, Train Loss: 0.1932, Train Acc: 96.88%, Val Loss:  2.661, Val Acc: 69.39%, Time: 5.56s \nIter: 1020, Train Loss: 0.1183, Train Acc: 98.44%, Val Loss:  2.754, Val Acc: 67.57%, Time: 11.94s \nIter: 1050, Train Loss: 0.1264, Train Acc: 98.44%, Val Loss:  2.679, Val Acc: 68.04%, Time: 18.30s \nIter: 1080, Train Loss: 0.05785, Train Acc: 100.00%, Val Loss:  2.707, Val Acc: 67.69%, Time: 24.65s \nIter: 1110, Train Loss:  0.113, Train Acc: 98.44%, Val Loss:  2.599, Val Acc: 70.04%, Time: 31.06s *\nIter: 1140, Train Loss: 0.05468, Train Acc: 100.00%, Val Loss:  2.629, Val Acc: 70.92%, Time: 37.51s *\nEpoch: 7\nIter: 1170, Train Loss: 0.02305, Train Acc: 100.00%, Val Loss:  2.679, Val Acc: 69.39%, Time: 3.61s \nIter: 1200, Train Loss: 0.1454, Train Acc: 96.88%, Val Loss:  2.672, Val Acc: 70.09%, Time: 10.01s \nIter: 1230, Train Loss: 0.06644, Train Acc: 98.44%, Val Loss:  2.779, Val Acc: 70.39%, Time: 16.38s \nIter: 1260, Train Loss: 0.1169, Train Acc: 98.44%, Val Loss:  2.645, Val Acc: 70.39%, Time: 22.77s \nIter: 1290, Train Loss: 0.04594, Train Acc: 100.00%, Val Loss:  2.685, Val Acc: 69.68%, Time: 29.12s \nIter: 1320, Train Loss: 0.1258, Train Acc: 98.44%, Val Loss:  2.991, Val Acc: 67.69%, Time: 35.45s \nIter: 1350, Train Loss: 0.07473, Train Acc: 100.00%, Val Loss:  2.835, Val Acc: 67.04%, Time: 41.84s \nEpoch: 8\nIter: 1380, Train Loss:  0.015, Train Acc: 100.00%, Val Loss:   2.93, Val Acc: 69.04%, Time: 6.23s \nIter: 1410, Train Loss:  0.132, Train Acc: 98.44%, Val Loss:  2.989, Val Acc: 68.57%, Time: 12.57s \nIter: 1440, Train Loss: 0.02398, Train Acc: 100.00%, Val Loss:  2.921, Val Acc: 68.33%, Time: 19.04s \nIter: 1470, Train Loss: 0.0305, Train Acc: 100.00%, Val Loss:  3.062, Val Acc: 66.39%, Time: 25.35s \nIter: 1500, Train Loss: 0.01558, Train Acc: 100.00%, Val Loss:  2.927, Val Acc: 67.45%, Time: 31.68s \nIter: 1530, Train Loss: 0.08236, Train Acc: 98.44%, Val Loss:  2.902, Val Acc: 68.92%, Time: 37.99s \nEpoch: 9\nIter: 1560, Train Loss: 0.02423, Train Acc: 100.00%, Val Loss:  2.869, Val Acc: 69.80%, Time: 4.27s \nIter: 1590, Train Loss:  0.017, Train Acc: 100.00%, Val Loss:  3.014, Val Acc: 69.10%, Time: 10.64s \nIter: 1620, Train Loss: 0.07846, Train Acc: 100.00%, Val Loss:  3.358, Val Acc: 65.45%, Time: 16.97s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.71      0.70      0.71       122\n                   Filter       0.66      0.72      0.69       163\n    Compute Derived Value       0.52      0.44      0.47       194\n            Find Extremum       0.68      0.71      0.69       218\n                     Sort       0.82      0.79      0.81       150\n          Determine Range       0.57      0.85      0.68       170\nCharacterize Distribution       0.78      0.66      0.71       144\n           Find Anomalies       0.78      0.70      0.74       171\n                  Cluster       0.86      0.78      0.82       158\n                Correlate       0.74      0.69      0.71       212\n\n                micro avg       0.70      0.70      0.70      1702\n                macro avg       0.71      0.70      0.70      1702\n             weighted avg       0.71      0.70      0.70      1702\n\nConfusion Matrix...\n[[ 86   3  17   3   0   7   6   0   0   0]\n [  0 117  13   0   1   9   0  11   0  12]\n [  5  11  85  46   0  34   2  10   0   1]\n [  1  15   8 154  12  25   0   2   1   0]\n [ 16   1   0   8 119   4   0   0   1   1]\n [  7   2   4   5   0 144   3   2   1   2]\n [  0   2   1   0   1  19  95   8   2  16]\n [  1  21  11   3   0   0   1 120   3  11]\n [  5   4   3   2  11   1   0   1 123   8]\n [  0   2  23   5   1   8  15   0  12 146]]\nFold:  6\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.431, Train Acc: 17.19%, Val Loss:  4.489, Val Acc: 21.56%, Time: 6.89s *\nIter: 60, Train Loss:  4.063, Train Acc: 29.69%, Val Loss:  3.997, Val Acc: 34.87%, Time: 12.76s *\nIter: 90, Train Loss:  2.928, Train Acc: 50.00%, Val Loss:  3.232, Val Acc: 48.71%, Time: 18.52s *\nIter: 120, Train Loss:  2.007, Train Acc: 71.88%, Val Loss:  2.832, Val Acc: 52.62%, Time: 24.29s *\nIter: 150, Train Loss:  1.936, Train Acc: 68.75%, Val Loss:  2.598, Val Acc: 56.26%, Time: 30.08s *\nIter: 180, Train Loss:   1.61, Train Acc: 78.12%, Val Loss:  2.427, Val Acc: 62.29%, Time: 35.92s *\nEpoch: 2\nIter: 210, Train Loss:  1.328, Train Acc: 82.81%, Val Loss:  2.321, Val Acc: 63.71%, Time: 2.45s *\nIter: 240, Train Loss: 0.7415, Train Acc: 87.50%, Val Loss:  2.534, Val Acc: 61.22%, Time: 8.29s \nIter: 270, Train Loss:  1.189, Train Acc: 79.69%, Val Loss:  2.559, Val Acc: 62.47%, Time: 14.02s \nIter: 300, Train Loss: 0.8755, Train Acc: 90.62%, Val Loss:   2.33, Val Acc: 67.61%, Time: 19.74s *\nIter: 330, Train Loss:  1.166, Train Acc: 87.50%, Val Loss:  2.178, Val Acc: 67.61%, Time: 25.51s *\nIter: 360, Train Loss: 0.6187, Train Acc: 90.62%, Val Loss:  2.273, Val Acc: 66.55%, Time: 31.31s \nIter: 390, Train Loss: 0.7631, Train Acc: 92.19%, Val Loss:  2.353, Val Acc: 66.90%, Time: 37.23s \nEpoch: 3\nIter: 420, Train Loss: 0.5369, Train Acc: 92.19%, Val Loss:  2.269, Val Acc: 69.74%, Time: 3.63s *\nIter: 450, Train Loss: 0.5507, Train Acc: 92.19%, Val Loss:  2.196, Val Acc: 68.86%, Time: 9.40s \nIter: 480, Train Loss: 0.1416, Train Acc: 100.00%, Val Loss:  2.412, Val Acc: 68.06%, Time: 15.19s \nIter: 510, Train Loss: 0.2677, Train Acc: 96.88%, Val Loss:  2.413, Val Acc: 70.01%, Time: 20.98s *\nIter: 540, Train Loss: 0.4945, Train Acc: 93.75%, Val Loss:  2.444, Val Acc: 69.12%, Time: 26.74s \nIter: 570, Train Loss: 0.6468, Train Acc: 89.06%, Val Loss:  2.262, Val Acc: 69.83%, Time: 32.54s \nIter: 600, Train Loss: 0.4543, Train Acc: 93.75%, Val Loss:   2.45, Val Acc: 70.28%, Time: 38.27s *\nEpoch: 4\nIter: 630, Train Loss:  0.332, Train Acc: 95.31%, Val Loss:  2.442, Val Acc: 70.01%, Time: 4.84s \nIter: 660, Train Loss: 0.2863, Train Acc: 98.44%, Val Loss:  2.372, Val Acc: 71.16%, Time: 10.62s *\nIter: 690, Train Loss: 0.4673, Train Acc: 93.75%, Val Loss:  2.455, Val Acc: 70.28%, Time: 16.56s \nIter: 720, Train Loss: 0.2133, Train Acc: 98.44%, Val Loss:  2.696, Val Acc: 67.52%, Time: 22.32s \nIter: 750, Train Loss: 0.4664, Train Acc: 93.75%, Val Loss:  2.568, Val Acc: 69.57%, Time: 28.07s \nIter: 780, Train Loss: 0.2473, Train Acc: 98.44%, Val Loss:  2.769, Val Acc: 68.06%, Time: 33.91s \nEpoch: 5\nIter: 810, Train Loss: 0.2888, Train Acc: 93.75%, Val Loss:  2.184, Val Acc: 72.85%, Time: 1.56s *\nIter: 840, Train Loss: 0.1321, Train Acc: 100.00%, Val Loss:   2.61, Val Acc: 70.72%, Time: 7.38s \nIter: 870, Train Loss: 0.07538, Train Acc: 100.00%, Val Loss:  2.663, Val Acc: 70.63%, Time: 13.20s \nIter: 900, Train Loss: 0.2684, Train Acc: 98.44%, Val Loss:  2.632, Val Acc: 69.65%, Time: 18.97s \nIter: 930, Train Loss: 0.1001, Train Acc: 98.44%, Val Loss:  2.732, Val Acc: 70.19%, Time: 24.73s \nIter: 960, Train Loss: 0.09424, Train Acc: 100.00%, Val Loss:   2.55, Val Acc: 72.76%, Time: 30.43s \nIter: 990, Train Loss: 0.04206, Train Acc: 100.00%, Val Loss:  2.762, Val Acc: 70.01%, Time: 36.27s \nEpoch: 6\nIter: 1020, Train Loss: 0.1966, Train Acc: 95.31%, Val Loss:  2.715, Val Acc: 70.90%, Time: 2.73s \nIter: 1050, Train Loss: 0.1669, Train Acc: 96.88%, Val Loss:  2.742, Val Acc: 69.92%, Time: 8.50s \nIter: 1080, Train Loss: 0.1016, Train Acc: 98.44%, Val Loss:  2.749, Val Acc: 70.19%, Time: 14.25s \nIter: 1110, Train Loss: 0.05674, Train Acc: 100.00%, Val Loss:  2.678, Val Acc: 71.61%, Time: 19.92s \nIter: 1140, Train Loss: 0.03337, Train Acc: 100.00%, Val Loss:  2.765, Val Acc: 69.03%, Time: 25.68s \nIter: 1170, Train Loss: 0.1887, Train Acc: 98.44%, Val Loss:  2.845, Val Acc: 69.48%, Time: 31.47s \nIter: 1200, Train Loss: 0.1768, Train Acc: 96.88%, Val Loss:  2.959, Val Acc: 68.06%, Time: 37.23s \nEpoch: 7\nIter: 1230, Train Loss: 0.1233, Train Acc: 98.44%, Val Loss:  2.638, Val Acc: 70.01%, Time: 3.97s \nIter: 1260, Train Loss: 0.08494, Train Acc: 98.44%, Val Loss:  2.871, Val Acc: 70.45%, Time: 9.65s \nIter: 1290, Train Loss: 0.1807, Train Acc: 98.44%, Val Loss:  3.053, Val Acc: 69.74%, Time: 15.42s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.44      0.15      0.23       104\n                   Filter       0.83      0.73      0.77       125\n    Compute Derived Value       0.36      0.42      0.39       102\n            Find Extremum       0.79      0.69      0.73       131\n                     Sort       1.00      0.78      0.88        86\n          Determine Range       0.65      0.75      0.69        83\nCharacterize Distribution       0.62      0.80      0.70       115\n           Find Anomalies       0.79      0.94      0.86        88\n                  Cluster       0.89      0.97      0.93       123\n                Correlate       0.69      0.81      0.74       170\n\n                micro avg       0.71      0.71      0.71      1127\n                macro avg       0.71      0.70      0.69      1127\n             weighted avg       0.71      0.71      0.70      1127\n\nConfusion Matrix...\n[[ 16   2  30   3   0  11  20   2   3  17]\n [ 12  91  14   2   0   2   1   3   0   0]\n [  6   3  43   2   0  14  14   1   0  19]\n [  0   2  18  90   0   0   2   6   7   6]\n [  0   1   0  15  67   1   0   1   1   0]\n [  0   0   0   1   0  62   5   1   1  13]\n [  2   6   2   0   0   6  92   2   2   3]\n [  0   0   0   0   0   0   0  83   1   4]\n [  0   0   0   0   0   0   2   2 119   0]\n [  0   5  11   1   0   0  12   4   0 137]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.307, Train Acc: 26.56%, Val Loss:  4.466, Val Acc: 17.78%, Time: 6.70s *\nIter: 60, Train Loss:   3.13, Train Acc: 51.56%, Val Loss:   4.17, Val Acc: 32.12%, Time: 12.28s *\nIter: 90, Train Loss:   2.73, Train Acc: 54.69%, Val Loss:  3.363, Val Acc: 49.17%, Time: 17.84s *\nIter: 120, Train Loss:  2.051, Train Acc: 70.31%, Val Loss:  2.799, Val Acc: 58.63%, Time: 23.44s *\nIter: 150, Train Loss:  1.694, Train Acc: 75.00%, Val Loss:  2.528, Val Acc: 63.51%, Time: 29.07s *\nIter: 180, Train Loss:  1.449, Train Acc: 81.25%, Val Loss:  2.297, Val Acc: 65.70%, Time: 34.60s *\nEpoch: 2\nIter: 210, Train Loss:  1.871, Train Acc: 75.00%, Val Loss:  2.062, Val Acc: 67.88%, Time: 1.85s *\nIter: 240, Train Loss:  1.159, Train Acc: 85.94%, Val Loss:  2.263, Val Acc: 68.40%, Time: 7.44s *\nIter: 270, Train Loss: 0.9035, Train Acc: 85.94%, Val Loss:  2.161, Val Acc: 69.65%, Time: 13.05s *\nIter: 300, Train Loss:   1.18, Train Acc: 82.81%, Val Loss:  2.167, Val Acc: 68.92%, Time: 18.71s \nIter: 330, Train Loss: 0.8264, Train Acc: 85.94%, Val Loss:  2.185, Val Acc: 70.89%, Time: 24.29s *\nIter: 360, Train Loss: 0.9604, Train Acc: 82.81%, Val Loss:  2.032, Val Acc: 70.48%, Time: 29.85s \nIter: 390, Train Loss: 0.7391, Train Acc: 84.38%, Val Loss:  2.136, Val Acc: 71.83%, Time: 35.47s *\nEpoch: 3\nIter: 420, Train Loss: 0.7025, Train Acc: 92.19%, Val Loss:   1.93, Val Acc: 74.95%, Time: 2.56s *\nIter: 450, Train Loss: 0.2947, Train Acc: 96.88%, Val Loss:  2.167, Val Acc: 73.70%, Time: 8.09s \nIter: 480, Train Loss: 0.3817, Train Acc: 92.19%, Val Loss:  1.974, Val Acc: 74.74%, Time: 13.62s \nIter: 510, Train Loss: 0.6931, Train Acc: 93.75%, Val Loss:   2.14, Val Acc: 71.83%, Time: 19.22s \nIter: 540, Train Loss: 0.5712, Train Acc: 93.75%, Val Loss:  2.326, Val Acc: 69.23%, Time: 24.77s \nIter: 570, Train Loss: 0.4022, Train Acc: 95.31%, Val Loss:  2.439, Val Acc: 69.96%, Time: 30.42s \nIter: 600, Train Loss: 0.3442, Train Acc: 96.88%, Val Loss:  2.363, Val Acc: 71.21%, Time: 36.02s \nEpoch: 4\nIter: 630, Train Loss:  0.452, Train Acc: 93.75%, Val Loss:  2.209, Val Acc: 69.85%, Time: 3.42s \nIter: 660, Train Loss: 0.5494, Train Acc: 90.62%, Val Loss:  2.137, Val Acc: 72.87%, Time: 9.02s \nIter: 690, Train Loss: 0.6175, Train Acc: 89.06%, Val Loss:   2.24, Val Acc: 69.75%, Time: 14.63s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 720, Train Loss: 0.3506, Train Acc: 96.88%, Val Loss:  2.277, Val Acc: 71.31%, Time: 20.22s \nIter: 750, Train Loss: 0.3449, Train Acc: 93.75%, Val Loss:  2.405, Val Acc: 70.79%, Time: 25.83s \nIter: 780, Train Loss:   0.37, Train Acc: 93.75%, Val Loss:  2.278, Val Acc: 72.66%, Time: 31.36s \nIter: 810, Train Loss: 0.1222, Train Acc: 100.00%, Val Loss:  2.333, Val Acc: 72.87%, Time: 36.92s \nEpoch: 5\nIter: 840, Train Loss: 0.1625, Train Acc: 96.88%, Val Loss:  2.283, Val Acc: 70.58%, Time: 4.09s \nIter: 870, Train Loss: 0.1092, Train Acc: 100.00%, Val Loss:  2.684, Val Acc: 70.27%, Time: 9.67s \nIter: 900, Train Loss:  0.203, Train Acc: 96.88%, Val Loss:  2.407, Val Acc: 73.91%, Time: 15.26s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.73      0.81      0.77        85\n                   Filter       0.66      0.73      0.69       107\n    Compute Derived Value       0.93      0.29      0.44        94\n            Find Extremum       0.79      0.68      0.73        96\n                     Sort       0.89      0.92      0.90        96\n          Determine Range       0.55      0.64      0.59        96\nCharacterize Distribution       0.94      0.94      0.94        98\n           Find Anomalies       0.69      0.62      0.65       103\n                  Cluster       0.69      0.97      0.81        90\n                Correlate       0.70      0.80      0.75        97\n\n                micro avg       0.74      0.74      0.74       962\n                macro avg       0.76      0.74      0.73       962\n             weighted avg       0.76      0.74      0.73       962\n\nConfusion Matrix...\n[[69 13  1  0  0  0  0  0  1  1]\n [ 1 78  0  2  1 11  0  8  5  1]\n [ 4  1 27 11  1 14  1  3  9 23]\n [ 0  0  0 65  2  4  0 15  3  7]\n [ 0  0  0  1 88  3  0  1  3  0]\n [12 11  1  3  1 61  2  2  3  0]\n [ 1  0  0  0  1  2 92  0  0  2]\n [ 6 15  0  0  2 15  0 64  1  0]\n [ 0  0  0  0  2  0  1  0 87  0]\n [ 2  0  0  0  1  0  2  0 14 78]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.457, Train Acc: 18.75%, Val Loss:  4.477, Val Acc: 16.41%, Time: 7.31s *\nIter: 60, Train Loss:  3.776, Train Acc: 37.50%, Val Loss:  3.905, Val Acc: 32.15%, Time: 13.34s *\nIter: 90, Train Loss:  3.354, Train Acc: 50.00%, Val Loss:  3.428, Val Acc: 45.75%, Time: 19.44s *\nIter: 120, Train Loss:  2.744, Train Acc: 59.38%, Val Loss:  2.746, Val Acc: 55.06%, Time: 25.55s *\nIter: 150, Train Loss:  1.811, Train Acc: 76.56%, Val Loss:  2.557, Val Acc: 59.01%, Time: 31.59s *\nIter: 180, Train Loss:   1.78, Train Acc: 73.44%, Val Loss:  2.337, Val Acc: 61.96%, Time: 37.71s *\nEpoch: 2\nIter: 210, Train Loss:  1.481, Train Acc: 79.69%, Val Loss:  2.428, Val Acc: 57.47%, Time: 3.71s \nIter: 240, Train Loss:  1.153, Train Acc: 85.94%, Val Loss:  2.329, Val Acc: 64.23%, Time: 9.80s *\nIter: 270, Train Loss: 0.6882, Train Acc: 92.19%, Val Loss:  2.272, Val Acc: 65.37%, Time: 15.88s *\nIter: 300, Train Loss:  1.155, Train Acc: 85.94%, Val Loss:  2.091, Val Acc: 67.52%, Time: 22.09s *\nIter: 330, Train Loss: 0.8598, Train Acc: 85.94%, Val Loss:  2.231, Val Acc: 62.22%, Time: 28.16s \nIter: 360, Train Loss: 0.7291, Train Acc: 92.19%, Val Loss:  2.085, Val Acc: 66.24%, Time: 34.23s \nIter: 390, Train Loss:  1.017, Train Acc: 79.69%, Val Loss:  2.368, Val Acc: 59.41%, Time: 40.36s \nEpoch: 3\nIter: 420, Train Loss: 0.5783, Train Acc: 95.31%, Val Loss:  2.426, Val Acc: 65.51%, Time: 5.81s \nIter: 450, Train Loss: 0.3892, Train Acc: 96.88%, Val Loss:  2.403, Val Acc: 62.36%, Time: 11.90s \nIter: 480, Train Loss: 0.3149, Train Acc: 95.31%, Val Loss:  2.031, Val Acc: 68.45%, Time: 18.00s *\nIter: 510, Train Loss:  0.323, Train Acc: 95.31%, Val Loss:  2.267, Val Acc: 66.38%, Time: 24.13s \nIter: 540, Train Loss: 0.3516, Train Acc: 95.31%, Val Loss:  2.254, Val Acc: 63.76%, Time: 30.22s \nIter: 570, Train Loss: 0.5194, Train Acc: 93.75%, Val Loss:  2.258, Val Acc: 66.38%, Time: 36.34s \nEpoch: 4\nIter: 600, Train Loss: 0.3664, Train Acc: 95.31%, Val Loss:  2.423, Val Acc: 62.22%, Time: 3.48s \nIter: 630, Train Loss: 0.3396, Train Acc: 96.88%, Val Loss:  2.413, Val Acc: 63.43%, Time: 9.61s \nIter: 660, Train Loss:  0.177, Train Acc: 96.88%, Val Loss:  2.694, Val Acc: 63.70%, Time: 15.72s \nIter: 690, Train Loss: 0.06411, Train Acc: 100.00%, Val Loss:  2.772, Val Acc: 59.61%, Time: 21.85s \nIter: 720, Train Loss: 0.1417, Train Acc: 98.44%, Val Loss:   2.35, Val Acc: 66.11%, Time: 27.99s \nIter: 750, Train Loss: 0.1184, Train Acc: 98.44%, Val Loss:    2.3, Val Acc: 66.18%, Time: 34.10s \nIter: 780, Train Loss: 0.1892, Train Acc: 98.44%, Val Loss:  2.652, Val Acc: 60.21%, Time: 40.17s \nEpoch: 5\nIter: 810, Train Loss: 0.1492, Train Acc: 98.44%, Val Loss:  2.629, Val Acc: 62.89%, Time: 5.56s \nIter: 840, Train Loss: 0.1736, Train Acc: 96.88%, Val Loss:  2.671, Val Acc: 65.04%, Time: 11.65s \nIter: 870, Train Loss: 0.5691, Train Acc: 93.75%, Val Loss:  2.793, Val Acc: 60.01%, Time: 17.73s \nIter: 900, Train Loss: 0.2581, Train Acc: 93.75%, Val Loss:  2.509, Val Acc: 64.97%, Time: 23.87s \nIter: 930, Train Loss: 0.05596, Train Acc: 100.00%, Val Loss:   2.79, Val Acc: 59.88%, Time: 29.94s \nIter: 960, Train Loss: 0.1004, Train Acc: 100.00%, Val Loss:  2.809, Val Acc: 58.74%, Time: 36.03s \nEpoch: 6\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.71      0.21      0.32       214\n                   Filter       0.52      0.59      0.55       213\n    Compute Derived Value       0.62      0.57      0.59       141\n            Find Extremum       0.53      0.60      0.56       153\n                     Sort       0.82      0.65      0.72        85\n          Determine Range       0.55      0.79      0.64        98\nCharacterize Distribution       0.79      0.90      0.84       125\n           Find Anomalies       0.56      0.59      0.57       169\n                  Cluster       0.35      0.84      0.50       100\n                Correlate       0.91      0.55      0.68       195\n\n                micro avg       0.59      0.59      0.59      1493\n                macro avg       0.64      0.63      0.60      1493\n             weighted avg       0.64      0.59      0.58      1493\n\nConfusion Matrix...\n[[ 45  10  20  30   0  10  16  29  45   9]\n [  6 126   4  24   4  18   0  15  16   0]\n [ 10  22  80  18   0   1   0   2   8   0]\n [  0  15  11  92   4  15   0   4  12   0]\n [  1   0   3   3  55   0   3   0  20   0]\n [  1  10   2   2   1  77   3   0   2   0]\n [  0   0   6   0   0   4 113   0   2   0]\n [  0  43   1   2   0   8   1  99  13   2]\n [  0   0   0   1   3   5   4   3  84   0]\n [  0  17   3   1   0   3   3  26  35 107]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.556, Train Acc: 12.50%, Val Loss:  4.581, Val Acc:  9.09%, Time: 6.78s *\nIter: 60, Train Loss:  4.005, Train Acc: 31.25%, Val Loss:  4.174, Val Acc: 29.01%, Time: 12.46s *\nIter: 90, Train Loss:  3.027, Train Acc: 51.56%, Val Loss:  3.476, Val Acc: 44.97%, Time: 18.14s *\nIter: 120, Train Loss:  2.771, Train Acc: 54.69%, Val Loss:  3.152, Val Acc: 45.74%, Time: 23.79s *\nIter: 150, Train Loss:  2.331, Train Acc: 62.50%, Val Loss:  2.842, Val Acc: 52.51%, Time: 29.44s *\nIter: 180, Train Loss:  1.634, Train Acc: 78.12%, Val Loss:  2.812, Val Acc: 54.74%, Time: 35.06s *\nEpoch: 2\nIter: 210, Train Loss:  1.202, Train Acc: 81.25%, Val Loss:  2.542, Val Acc: 59.77%, Time: 2.16s *\nIter: 240, Train Loss:  1.169, Train Acc: 82.81%, Val Loss:  2.838, Val Acc: 55.22%, Time: 7.83s \nIter: 270, Train Loss:  1.294, Train Acc: 76.56%, Val Loss:  2.692, Val Acc: 58.99%, Time: 13.47s \nIter: 300, Train Loss:  1.268, Train Acc: 84.38%, Val Loss:  2.512, Val Acc: 64.99%, Time: 19.11s *\nIter: 330, Train Loss:  1.005, Train Acc: 84.38%, Val Loss:  2.565, Val Acc: 63.54%, Time: 24.75s \nIter: 360, Train Loss:  1.186, Train Acc: 81.25%, Val Loss:  2.536, Val Acc: 63.25%, Time: 30.39s \nIter: 390, Train Loss:  0.705, Train Acc: 85.94%, Val Loss:  2.548, Val Acc: 62.48%, Time: 36.01s \nEpoch: 3\nIter: 420, Train Loss: 0.2179, Train Acc: 96.88%, Val Loss:  2.508, Val Acc: 64.80%, Time: 2.94s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 450, Train Loss: 0.3499, Train Acc: 96.88%, Val Loss:  2.641, Val Acc: 64.02%, Time: 8.70s \nIter: 480, Train Loss:  1.148, Train Acc: 87.50%, Val Loss:  2.692, Val Acc: 61.22%, Time: 14.41s \nIter: 510, Train Loss: 0.4885, Train Acc: 95.31%, Val Loss:    2.5, Val Acc: 63.54%, Time: 20.00s \nIter: 540, Train Loss:  0.332, Train Acc: 95.31%, Val Loss:  2.787, Val Acc: 60.64%, Time: 25.67s \nIter: 570, Train Loss: 0.2563, Train Acc: 98.44%, Val Loss:  2.564, Val Acc: 63.54%, Time: 31.29s \nIter: 600, Train Loss: 0.3503, Train Acc: 95.31%, Val Loss:  2.637, Val Acc: 63.93%, Time: 36.90s \nEpoch: 4\nIter: 630, Train Loss: 0.5856, Train Acc: 93.75%, Val Loss:  2.704, Val Acc: 66.92%, Time: 3.87s *\nIter: 660, Train Loss: 0.2968, Train Acc: 95.31%, Val Loss:  2.727, Val Acc: 63.93%, Time: 9.51s \nIter: 690, Train Loss: 0.3432, Train Acc: 95.31%, Val Loss:  2.996, Val Acc: 62.28%, Time: 15.13s \nIter: 720, Train Loss: 0.2841, Train Acc: 98.44%, Val Loss:  2.654, Val Acc: 63.44%, Time: 20.70s \nIter: 750, Train Loss: 0.1761, Train Acc: 96.88%, Val Loss:  2.557, Val Acc: 66.15%, Time: 26.32s \nIter: 780, Train Loss: 0.1142, Train Acc: 98.44%, Val Loss:   2.72, Val Acc: 66.25%, Time: 32.01s \nIter: 810, Train Loss: 0.1214, Train Acc: 98.44%, Val Loss:  2.917, Val Acc: 63.25%, Time: 37.68s \nEpoch: 5\nIter: 840, Train Loss: 0.2412, Train Acc: 95.31%, Val Loss:  2.571, Val Acc: 62.57%, Time: 4.73s \nIter: 870, Train Loss: 0.2096, Train Acc: 98.44%, Val Loss:  2.914, Val Acc: 65.28%, Time: 10.43s \nIter: 900, Train Loss: 0.1668, Train Acc: 98.44%, Val Loss:  2.734, Val Acc: 66.83%, Time: 16.05s \nIter: 930, Train Loss: 0.1286, Train Acc: 98.44%, Val Loss:  2.867, Val Acc: 61.90%, Time: 21.66s \nIter: 960, Train Loss: 0.1265, Train Acc: 98.44%, Val Loss:  2.801, Val Acc: 66.54%, Time: 27.31s \nIter: 990, Train Loss: 0.4482, Train Acc: 95.31%, Val Loss:  2.875, Val Acc: 63.64%, Time: 32.85s \nIter: 1020, Train Loss: 0.02057, Train Acc: 100.00%, Val Loss:  2.915, Val Acc: 63.06%, Time: 38.53s \nEpoch: 6\nIter: 1050, Train Loss: 0.05345, Train Acc: 100.00%, Val Loss:   3.03, Val Acc: 65.57%, Time: 5.63s \nIter: 1080, Train Loss: 0.05545, Train Acc: 98.44%, Val Loss:  2.887, Val Acc: 65.76%, Time: 11.21s \nIter: 1110, Train Loss: 0.07259, Train Acc: 100.00%, Val Loss:  2.981, Val Acc: 64.60%, Time: 16.82s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.63      0.55      0.59       112\n                   Filter       0.48      0.49      0.48       110\n    Compute Derived Value       0.57      0.57      0.57       156\n            Find Extremum       0.76      0.86      0.81       134\n                     Sort       0.47      0.69      0.56        58\n          Determine Range       0.80      0.57      0.66        99\nCharacterize Distribution       0.58      0.69      0.63        71\n           Find Anomalies       0.80      0.76      0.78       107\n                  Cluster       0.98      0.77      0.86        66\n                Correlate       0.54      0.55      0.55       121\n\n                micro avg       0.64      0.64      0.64      1034\n                macro avg       0.66      0.65      0.65      1034\n             weighted avg       0.66      0.64      0.64      1034\n\nConfusion Matrix...\n[[ 62   7  14   0  12   0   9   0   0   8]\n [  7  54  30   0   4   5   0   2   0   8]\n [ 13  18  89  20   3   3   5   2   0   3]\n [  0   6   3 115   5   2   1   2   0   0]\n [  0   4   8   2  40   1   0   1   1   1]\n [  0   7   4  11  11  56   8   0   0   2]\n [  0   0   2   0   2   1  49   0   0  17]\n [  0   4   0   2   4   0   1  81   0  15]\n [  0   1   3   0   3   0   5   0  51   3]\n [ 16  12   3   1   1   2   6  13   0  67]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.455, Train Acc: 14.06%, Val Loss:  4.508, Val Acc: 11.93%, Time: 6.11s *\nIter: 60, Train Loss:  3.819, Train Acc: 45.31%, Val Loss:  3.904, Val Acc: 40.83%, Time: 11.04s *\nIter: 90, Train Loss:  3.144, Train Acc: 45.31%, Val Loss:  3.254, Val Acc: 47.02%, Time: 16.01s *\nIter: 120, Train Loss:  2.171, Train Acc: 64.06%, Val Loss:   2.94, Val Acc: 53.21%, Time: 20.96s *\nIter: 150, Train Loss:   1.51, Train Acc: 79.69%, Val Loss:  2.801, Val Acc: 51.83%, Time: 25.94s \nIter: 180, Train Loss:  1.683, Train Acc: 71.88%, Val Loss:  2.752, Val Acc: 55.73%, Time: 30.84s *\nIter: 210, Train Loss:  1.242, Train Acc: 82.81%, Val Loss:  2.816, Val Acc: 55.96%, Time: 35.80s *\nEpoch: 2\nIter: 240, Train Loss: 0.8714, Train Acc: 89.06%, Val Loss:  2.596, Val Acc: 58.94%, Time: 4.55s *\nIter: 270, Train Loss: 0.6882, Train Acc: 90.62%, Val Loss:  2.698, Val Acc: 60.32%, Time: 9.48s *\nIter: 300, Train Loss: 0.8114, Train Acc: 89.06%, Val Loss:   2.84, Val Acc: 56.88%, Time: 14.50s \nIter: 330, Train Loss:  1.086, Train Acc: 76.56%, Val Loss:  2.738, Val Acc: 60.09%, Time: 19.47s \nIter: 360, Train Loss: 0.8862, Train Acc: 89.06%, Val Loss:  2.864, Val Acc: 60.78%, Time: 24.48s *\nIter: 390, Train Loss: 0.9183, Train Acc: 85.94%, Val Loss:  2.926, Val Acc: 55.73%, Time: 29.57s \nIter: 420, Train Loss:  0.792, Train Acc: 92.19%, Val Loss:  2.546, Val Acc: 62.61%, Time: 34.53s *\nEpoch: 3\nIter: 450, Train Loss: 0.3901, Train Acc: 95.31%, Val Loss:  2.724, Val Acc: 63.30%, Time: 4.06s *\nIter: 480, Train Loss: 0.5116, Train Acc: 90.62%, Val Loss:  2.847, Val Acc: 62.16%, Time: 9.02s \nIter: 510, Train Loss: 0.6245, Train Acc: 90.62%, Val Loss:  2.913, Val Acc: 62.16%, Time: 13.98s \nIter: 540, Train Loss: 0.3993, Train Acc: 95.31%, Val Loss:  2.662, Val Acc: 63.07%, Time: 18.98s \nIter: 570, Train Loss: 0.3037, Train Acc: 98.44%, Val Loss:   2.93, Val Acc: 64.45%, Time: 23.95s *\nIter: 600, Train Loss: 0.5452, Train Acc: 93.75%, Val Loss:  2.609, Val Acc: 64.22%, Time: 28.98s \nIter: 630, Train Loss: 0.2774, Train Acc: 98.44%, Val Loss:  2.661, Val Acc: 66.51%, Time: 33.96s *\nEpoch: 4\nIter: 660, Train Loss: 0.6435, Train Acc: 93.75%, Val Loss:   3.23, Val Acc: 59.63%, Time: 3.62s \nIter: 690, Train Loss: 0.3087, Train Acc: 93.75%, Val Loss:  3.238, Val Acc: 60.32%, Time: 8.64s \nIter: 720, Train Loss: 0.3246, Train Acc: 95.31%, Val Loss:   3.15, Val Acc: 62.61%, Time: 13.65s \nIter: 750, Train Loss: 0.3592, Train Acc: 93.75%, Val Loss:  2.896, Val Acc: 60.78%, Time: 18.74s \nIter: 780, Train Loss: 0.3295, Train Acc: 96.88%, Val Loss:   2.92, Val Acc: 63.07%, Time: 23.71s \nIter: 810, Train Loss: 0.1434, Train Acc: 96.88%, Val Loss:   3.06, Val Acc: 61.24%, Time: 28.68s \nIter: 840, Train Loss: 0.1143, Train Acc: 98.44%, Val Loss:  3.116, Val Acc: 61.24%, Time: 33.69s \nEpoch: 5\nIter: 870, Train Loss: 0.2105, Train Acc: 98.44%, Val Loss:  3.075, Val Acc: 63.53%, Time: 3.21s \nIter: 900, Train Loss: 0.2833, Train Acc: 96.88%, Val Loss:  3.066, Val Acc: 61.47%, Time: 8.14s \nIter: 930, Train Loss:  0.127, Train Acc: 96.88%, Val Loss:  3.079, Val Acc: 65.83%, Time: 13.09s \nIter: 960, Train Loss: 0.0635, Train Acc: 100.00%, Val Loss:  3.224, Val Acc: 64.91%, Time: 18.04s \nIter: 990, Train Loss: 0.3365, Train Acc: 95.31%, Val Loss:  3.125, Val Acc: 63.07%, Time: 23.00s \nIter: 1020, Train Loss: 0.04986, Train Acc: 100.00%, Val Loss:  3.236, Val Acc: 60.09%, Time: 27.96s \nIter: 1050, Train Loss: 0.2065, Train Acc: 95.31%, Val Loss:  3.025, Val Acc: 64.91%, Time: 32.89s \nEpoch: 6\nIter: 1080, Train Loss: 0.04296, Train Acc: 100.00%, Val Loss:  3.566, Val Acc: 60.78%, Time: 2.75s \nIter: 1110, Train Loss: 0.08656, Train Acc: 98.44%, Val Loss:    3.5, Val Acc: 58.26%, Time: 7.82s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.75      0.11      0.19        27\n                   Filter       0.49      0.61      0.54        28\n    Compute Derived Value       0.41      0.62      0.49        47\n            Find Extremum       0.70      0.82      0.75       109\n                     Sort       0.87      1.00      0.93        26\n          Determine Range       0.58      0.22      0.32        63\nCharacterize Distribution       0.65      0.97      0.78        33\n           Find Anomalies       0.71      0.64      0.67        45\n                  Cluster       0.76      0.69      0.72        32\n                Correlate       0.42      0.42      0.42        26\n\n                micro avg       0.62      0.62      0.62       436\n                macro avg       0.63      0.61      0.58       436\n             weighted avg       0.64      0.62      0.60       436\n\nConfusion Matrix...\n[[ 3  3  3  0  0  0  2 11  4  1]\n [ 0 17  0  0  2  0  0  0  1  8]\n [ 1  0 29  7  0  0  4  0  0  6]\n [ 0  1  8 89  1 10  0  0  0  0]\n [ 0  0  0  0 26  0  0  0  0  0]\n [ 0 14 16 16  1 14  1  1  0  0]\n [ 0  0  0  0  0  0 32  0  1  0]\n [ 0  0  0 15  0  0  0 29  1  0]\n [ 0  0  0  0  0  0 10  0 22  0]\n [ 0  0 15  0  0  0  0  0  0 11]]\n[0.6203288490284006, 0.6628766344513929, 0.5884831460674157, 0.6427536231884058, 0.6985898942420682, 0.709849157054126, 0.737006237006237, 0.5880776959142666, 0.6421663442940039, 0.6238532110091743]\n0.651398479225549, 0.04791225098871585, 0.05050394698333215, 0.0022955837948057025\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "raw", "source": "split_info = {\n    # \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\nkf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, split_type))"}, {"metadata": {}, "cell_type": "raw", "source": "kf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, split_type))"}, {"metadata": {}, "cell_type": "raw", "source": "def model_train2(model, x_train, y_train, x_val, y_val, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            #session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.dropoutKeepProb] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                print(x_batch.shape, y_batch.shape, x_val.shape, y_val.shape)\n                loss_val, acc_val = evaluate(session, model, x_val, y_val, model.loss, model.acc, 64)\n                if acc_val > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_val\n                    last_improved = total_batch\n                    # saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, loss_val, acc_val, duration, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                test_data_len = len(x_val)\n                test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n                y_test_cls = np.argmax(y_val, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.zeros(shape=len(x_val), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, test_data_len)\n                    feed_dict = {\n                        model.inputX: x_val[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_test_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score"}, {"metadata": {}, "cell_type": "raw", "source": "train_data = dataset_split(split_info[\"expert\"])\ntest_acc_split.append(train_split_data(lstm, train_data, \"expert\"))"}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}