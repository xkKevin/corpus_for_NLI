{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import shuffle\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_hf.txt\"\ntestDataPath = \"s3://corpus-2/dataset/corpus_5_new.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_info = {\n    \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\n\ndef dataset_split(info):\n    if info:\n        [num, pi] = info\n        train_data = [[] for i in range(num)]\n        test_data = [[] for i in range(num)]\n        \n        with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                train_data[index].append([content,label])\n                \n        with open(testDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                test_data[index].append([content,label])\n\n        for i in range(num):\n            # np.random.shuffle(train_data[i])\n            train_data[i], test_data[i] = shuffle(train_data[i], test_data[i])\n            train_data[i] = np.asarray(train_data[i])\n            test_data[i] = np.asarray(test_data[i])\n\n        train_data, test_data = shuffle(train_data, test_data)\n        return train_data, test_data\n    \n    \n    train_data = []\n    test_data = []\n    \n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n            \n    with open(testDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            test_data.append([content,label])\n            \n    train_data, test_data = shuffle(train_data, test_data)\n    return np.asarray(train_data), np.asarray(test_data)\n\n\ndef mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(model, train_data, test_data, split_type):\n    \n    print(split_type)\n    \n    test_acc = []\n    fold_id = 0\n    \n    if split_type != \"random\":\n        tx = []\n        ty = []\n        for ti in train_data:\n            x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            tx.append(x_train)\n            ty.append(y_train)\n\n        tx = np.asarray(tx)\n        ty = np.asarray(ty)\n\n        te_x = []\n        te_y = []\n        for ti in test_data:\n            x_test, y_test = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            te_x.append(x_test)\n            te_y.append(y_test)\n\n        te_x = np.asarray(te_x)\n        te_y = np.asarray(te_y)\n        \n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_x, train_y = mergeData(tx[train_i],ty[train_i])\n            test_x, test_y = mergeData(te_x[test_i],te_y[test_i])\n            test_acc.append(model_train(model, train_x, train_y, test_x, test_y, categories))\n        \n    else:\n        tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n        te_x, te_y = process_file(test_data[:,0], test_data[:,1], word_to_id, num_classes, seq_length)\n        # print(len(tx),len(tx[0]),len(tx[1]))\n\n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            test_acc.append(model_train(model,tx[train_i], ty[train_i],te_x[test_i], te_y[test_i], categories))\n        \n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\nprint(len(embedding),embedding_dim,vocab_size)\n \nseq_length = 41  # seq_length = 37  TREC", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, x_val, y_val, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 400  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    print_per_batch = 100\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.dropoutKeepProb] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, model, x_val, y_val, model.loss, model.acc, 64)\n                if acc_val > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_val\n                    last_improved = total_batch\n                    # saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, loss_val, acc_val, duration, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"%s:%s  %s\" % (total_batch, last_improved, \"No optimization for a long time, auto-stopping...\"))\n                \n                test_data_len = len(x_val)\n                test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n                y_test_cls = np.argmax(y_val, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.zeros(shape=len(x_val), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, test_data_len)\n                    feed_dict = {\n                        model.inputX: x_val[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_test_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            wordEmbedding = tf.Variable(initial_value=wordEmbedding)\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nlstm = AdversarailLSTM(embedding)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:128\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nsplit_info = {\n    # \"random\": False,\n    # \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "bundle\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.821, Train Acc: 56.25%, Val Loss:  3.326, Val Acc: 46.08%, Time: 16.88s *\nEpoch: 2\nIter: 200, Train Loss:  1.663, Train Acc: 76.56%, Val Loss:  2.663, Val Acc: 58.87%, Time: 1.40s *\nIter: 300, Train Loss:  1.456, Train Acc: 71.88%, Val Loss:  2.424, Val Acc: 61.19%, Time: 13.95s *\nEpoch: 3\nIter: 400, Train Loss: 0.7608, Train Acc: 89.06%, Val Loss:  2.165, Val Acc: 66.06%, Time: 1.63s *\nIter: 500, Train Loss:  1.042, Train Acc: 82.81%, Val Loss:  2.175, Val Acc: 68.10%, Time: 14.18s *\nEpoch: 4\nIter: 600, Train Loss: 0.5504, Train Acc: 93.75%, Val Loss:  2.243, Val Acc: 69.62%, Time: 1.87s *\nIter: 700, Train Loss: 0.3786, Train Acc: 93.75%, Val Loss:   2.49, Val Acc: 66.86%, Time: 14.42s \nEpoch: 5\nIter: 800, Train Loss: 0.2374, Train Acc: 96.88%, Val Loss:   2.22, Val Acc: 69.26%, Time: 2.09s \nIter: 900, Train Loss: 0.8383, Train Acc: 93.75%, Val Loss:  2.385, Val Acc: 70.28%, Time: 14.63s *\nEpoch: 6\nIter: 1000, Train Loss: 0.3131, Train Acc: 96.88%, Val Loss:  2.292, Val Acc: 71.88%, Time: 2.31s *\nIter: 1100, Train Loss: 0.3493, Train Acc: 95.31%, Val Loss:   2.38, Val Acc: 69.99%, Time: 14.89s \nEpoch: 7\nIter: 1200, Train Loss:  0.105, Train Acc: 100.00%, Val Loss:  2.321, Val Acc: 72.38%, Time: 2.54s *\nIter: 1300, Train Loss: 0.1121, Train Acc: 98.44%, Val Loss:  2.476, Val Acc: 69.84%, Time: 15.08s \nEpoch: 8\nIter: 1400, Train Loss: 0.1361, Train Acc: 96.88%, Val Loss:   2.56, Val Acc: 71.58%, Time: 2.77s \nIter: 1500, Train Loss: 0.07474, Train Acc: 100.00%, Val Loss:  2.499, Val Acc: 71.22%, Time: 15.26s \nEpoch: 9\nIter: 1600, Train Loss: 0.2883, Train Acc: 96.88%, Val Loss:  2.666, Val Acc: 69.62%, Time: 3.02s \n1601:1200  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.75      0.80      0.78       141\n                   Filter       0.41      0.54      0.47        97\n    Compute Derived Value       0.51      0.65      0.57       137\n            Find Extremum       0.66      0.64      0.65       201\n                     Sort       0.87      0.72      0.79       116\n          Determine Range       0.74      0.58      0.65       120\nCharacterize Distribution       0.85      0.76      0.80       172\n           Find Anomalies       0.56      0.62      0.59        76\n                  Cluster       0.81      0.82      0.82       131\n                Correlate       0.81      0.72      0.76       185\n\n                micro avg       0.69      0.69      0.69      1376\n                macro avg       0.70      0.69      0.69      1376\n             weighted avg       0.71      0.69      0.70      1376\n\nConfusion Matrix...\n[[113   2   9  15   1   0   0   1   0   0]\n [ 12  52  22   0   0   5   1   3   1   1]\n [ 15   9  89  14   1   8   0   0   0   1]\n [  2  11  35 129  10   1   1  10   1   1]\n [  0  11   2   5  84   1   1   1  10   1]\n [  4  14   2  19   1  70  10   0   0   0]\n [  2   1   2   0   0  10 131   1   8  17]\n [  0   2   8  14   0   0   1  47   2   2]\n [  2   1   0   0   0   0   9   3 108   8]\n [  0  23   6   0   0   0   1  18   4 133]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.778, Train Acc: 51.56%, Val Loss:  3.077, Val Acc: 50.54%, Time: 13.52s *\nEpoch: 2\nIter: 200, Train Loss:  1.443, Train Acc: 78.12%, Val Loss:  2.263, Val Acc: 65.02%, Time: 1.40s *\nIter: 300, Train Loss:  1.027, Train Acc: 84.38%, Val Loss:  2.347, Val Acc: 62.01%, Time: 13.97s \nEpoch: 3\nIter: 400, Train Loss:  0.816, Train Acc: 89.06%, Val Loss:  2.053, Val Acc: 67.74%, Time: 1.64s *\nIter: 500, Train Loss: 0.4736, Train Acc: 92.19%, Val Loss:  2.086, Val Acc: 68.24%, Time: 14.23s *\nEpoch: 4\nIter: 600, Train Loss: 0.5776, Train Acc: 90.62%, Val Loss:  1.983, Val Acc: 71.76%, Time: 1.86s *\nIter: 700, Train Loss: 0.5585, Train Acc: 92.19%, Val Loss:  1.888, Val Acc: 71.76%, Time: 14.40s *\nEpoch: 5\nIter: 800, Train Loss: 0.4958, Train Acc: 92.19%, Val Loss:  2.091, Val Acc: 71.54%, Time: 2.08s \nIter: 900, Train Loss: 0.4923, Train Acc: 92.19%, Val Loss:  2.127, Val Acc: 71.40%, Time: 14.65s \nEpoch: 6\nIter: 1000, Train Loss: 0.3249, Train Acc: 93.75%, Val Loss:  2.238, Val Acc: 71.11%, Time: 2.31s \nIter: 1100, Train Loss: 0.1799, Train Acc: 98.44%, Val Loss:  2.399, Val Acc: 71.33%, Time: 14.90s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.47      0.53      0.50        92\n                   Filter       0.48      0.89      0.63       138\n    Compute Derived Value       0.60      0.80      0.68        98\n            Find Extremum       0.80      0.68      0.73       200\n                     Sort       0.88      0.78      0.83       102\n          Determine Range       0.81      0.64      0.72       109\nCharacterize Distribution       0.82      0.71      0.76       138\n           Find Anomalies       0.77      0.59      0.67       191\n                  Cluster       0.88      0.80      0.84       140\n                Correlate       0.80      0.72      0.76       187\n\n                micro avg       0.71      0.71      0.71      1395\n                macro avg       0.73      0.71      0.71      1395\n             weighted avg       0.75      0.71      0.72      1395\n\nConfusion Matrix...\n[[ 49  16  15   9   1   0   2   0   0   0]\n [  3 123   6   0   0   2   1   3   0   0]\n [  4  13  78   0   0   0   1   1   0   1]\n [  6  36   6 135   2   3   2  10   0   0]\n [ 13   2   3   2  80   2   0   0   0   0]\n [ 14   5   3   9   3  70   0   4   1   0]\n [  0   3   3  14   1   4  98   2   7   6]\n [  4  53   7   0   0   1   5 112   3   6]\n [  1   1   0   0   3   0   1   2 112  20]\n [ 11   3  10   0   1   4   9  11   4 134]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.178, Train Acc: 40.62%, Val Loss:  3.389, Val Acc: 44.19%, Time: 13.46s *\nEpoch: 2\nIter: 200, Train Loss:  1.639, Train Acc: 78.12%, Val Loss:  2.517, Val Acc: 60.80%, Time: 1.41s *\nIter: 300, Train Loss:    1.6, Train Acc: 78.12%, Val Loss:  2.158, Val Acc: 67.00%, Time: 14.00s *\nEpoch: 3\nIter: 400, Train Loss: 0.9766, Train Acc: 85.94%, Val Loss:  2.113, Val Acc: 69.49%, Time: 1.66s *\nIter: 500, Train Loss: 0.9198, Train Acc: 87.50%, Val Loss:  2.123, Val Acc: 68.92%, Time: 14.19s \nEpoch: 4\nIter: 600, Train Loss: 0.3691, Train Acc: 95.31%, Val Loss:  2.025, Val Acc: 71.56%, Time: 1.86s *\nIter: 700, Train Loss: 0.4367, Train Acc: 93.75%, Val Loss:   2.28, Val Acc: 70.92%, Time: 14.45s \nEpoch: 5\nIter: 800, Train Loss: 0.7623, Train Acc: 89.06%, Val Loss:  2.076, Val Acc: 72.49%, Time: 2.09s *\nIter: 900, Train Loss: 0.2438, Train Acc: 96.88%, Val Loss:  2.379, Val Acc: 70.35%, Time: 14.65s \nEpoch: 6\nIter: 1000, Train Loss: 0.1028, Train Acc: 100.00%, Val Loss:  2.261, Val Acc: 71.56%, Time: 2.33s \nIter: 1100, Train Loss: 0.3022, Train Acc: 96.88%, Val Loss:  2.382, Val Acc: 71.99%, Time: 14.85s \nEpoch: 7\nIter: 1200, Train Loss: 0.09193, Train Acc: 100.00%, Val Loss:  2.569, Val Acc: 71.70%, Time: 2.54s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.78      0.69      0.73       181\n                   Filter       0.62      0.89      0.73       159\n    Compute Derived Value       0.68      0.51      0.59       178\n            Find Extremum       0.77      0.85      0.81        98\n                     Sort       0.69      0.75      0.72        93\n          Determine Range       0.75      0.75      0.75       203\nCharacterize Distribution       0.62      0.71      0.66        90\n           Find Anomalies       0.66      0.65      0.66       153\n                  Cluster       0.96      0.75      0.84        96\n                Correlate       0.78      0.71      0.74       152\n\n                micro avg       0.72      0.72      0.72      1403\n                macro avg       0.73      0.73      0.72      1403\n             weighted avg       0.73      0.72      0.72      1403\n\nConfusion Matrix...\n[[124  14   6   8   7   4   0  13   2   3]\n [  1 142   0   0   0   0   0  15   0   1]\n [  6  15  91  12   1  23  11   3   0  16]\n [  0  10   0  83   1   3   0   1   0   0]\n [  0   2  15   1  70   5   0   0   0   0]\n [ 17  12   3   1  12 153   2   1   1   1]\n [  2   3   2   0   5   8  64   3   0   3]\n [  0  25   6   1   0   4  10 100   0   7]\n [  2   1   1   2   1   1  15   1  72   0]\n [  8   5   9   0   5   2   1  14   0 108]]\nFold:  4\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.963, Train Acc: 56.25%, Val Loss:  3.346, Val Acc: 44.54%, Time: 13.63s *\nEpoch: 2\nIter: 200, Train Loss:  1.802, Train Acc: 73.44%, Val Loss:  2.688, Val Acc: 55.81%, Time: 1.58s *\nIter: 300, Train Loss:  1.899, Train Acc: 64.06%, Val Loss:  2.445, Val Acc: 61.93%, Time: 14.16s *\nEpoch: 3\nIter: 400, Train Loss: 0.7006, Train Acc: 90.62%, Val Loss:  2.342, Val Acc: 65.90%, Time: 1.91s *\nIter: 500, Train Loss: 0.5905, Train Acc: 92.19%, Val Loss:  2.354, Val Acc: 66.60%, Time: 14.56s *\nEpoch: 4\nIter: 600, Train Loss: 0.3504, Train Acc: 93.75%, Val Loss:  2.439, Val Acc: 66.39%, Time: 2.26s \nIter: 700, Train Loss: 0.5337, Train Acc: 93.75%, Val Loss:  2.543, Val Acc: 67.57%, Time: 14.89s *\nEpoch: 5\nIter: 800, Train Loss:  0.314, Train Acc: 95.31%, Val Loss:  2.523, Val Acc: 69.24%, Time: 2.59s *\nIter: 900, Train Loss: 0.6006, Train Acc: 93.75%, Val Loss:  2.575, Val Acc: 68.27%, Time: 15.18s \nEpoch: 6\nIter: 1000, Train Loss: 0.1033, Train Acc: 100.00%, Val Loss:  2.738, Val Acc: 68.68%, Time: 2.94s \nIter: 1100, Train Loss: 0.09849, Train Acc: 100.00%, Val Loss:  2.617, Val Acc: 69.31%, Time: 15.55s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1224, Train Acc: 96.88%, Val Loss:  2.825, Val Acc: 67.64%, Time: 3.28s \nIter: 1300, Train Loss: 0.1464, Train Acc: 98.44%, Val Loss:  3.216, Val Acc: 67.78%, Time: 15.89s \nEpoch: 8\nIter: 1400, Train Loss: 0.03336, Train Acc: 100.00%, Val Loss:  2.997, Val Acc: 68.27%, Time: 3.64s \nIter: 1500, Train Loss: 0.2943, Train Acc: 96.88%, Val Loss:  2.917, Val Acc: 68.20%, Time: 16.31s \n1501:1100  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.79      0.70      0.74       131\n                   Filter       0.52      0.53      0.53       105\n    Compute Derived Value       0.53      0.63      0.57       128\n            Find Extremum       0.44      0.80      0.57        95\n                     Sort       0.72      0.66      0.69       133\n          Determine Range       0.51      0.39      0.44       173\nCharacterize Distribution       0.76      0.81      0.79       195\n           Find Anomalies       0.90      0.68      0.78       157\n                  Cluster       0.91      0.68      0.78       173\n                Correlate       0.79      0.95      0.86       147\n\n                micro avg       0.68      0.68      0.68      1437\n                macro avg       0.69      0.68      0.67      1437\n             weighted avg       0.71      0.68      0.68      1437\n\nConfusion Matrix...\n[[ 92   3   7  21   1   2   5   0   0   0]\n [  6  56   2   1   3  25   7   2   3   0]\n [  5   6  81  16   0   8   5   0   0   7]\n [  2   2  11  76   0   0   2   1   0   1]\n [  0   4   1  32  88   3   0   0   4   1]\n [  8  18  22  23  14  67  16   2   1   2]\n [  0   2  19   1   1   3 158   3   2   6]\n [  4  15   8   3   0   0   2 107   1  17]\n [  0   0   2   1  16  22   9   2 118   3]\n [  0   1   1   0   0   1   3   2   0 139]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.175, Train Acc: 48.44%, Val Loss:  3.231, Val Acc: 49.57%, Time: 13.54s *\nEpoch: 2\nIter: 200, Train Loss:  1.344, Train Acc: 82.81%, Val Loss:  2.216, Val Acc: 63.91%, Time: 1.41s *\nIter: 300, Train Loss:  1.212, Train Acc: 84.38%, Val Loss:  2.013, Val Acc: 69.76%, Time: 13.95s *\nEpoch: 3\nIter: 400, Train Loss: 0.8868, Train Acc: 87.50%, Val Loss:  1.973, Val Acc: 72.25%, Time: 1.63s *\nIter: 500, Train Loss: 0.7802, Train Acc: 90.62%, Val Loss:  2.209, Val Acc: 69.97%, Time: 14.18s \nEpoch: 4\nIter: 600, Train Loss: 0.7201, Train Acc: 89.06%, Val Loss:  2.148, Val Acc: 69.47%, Time: 1.86s \nIter: 700, Train Loss: 0.4853, Train Acc: 90.62%, Val Loss:  2.392, Val Acc: 67.97%, Time: 14.42s \nEpoch: 5\nIter: 800, Train Loss: 0.3071, Train Acc: 96.88%, Val Loss:  2.487, Val Acc: 69.97%, Time: 2.07s \n801:400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.47      0.83      0.60       118\n                   Filter       0.68      0.77      0.72       185\n    Compute Derived Value       0.52      0.52      0.52       109\n            Find Extremum       0.89      0.66      0.76       161\n                     Sort       0.85      0.78      0.81       149\n          Determine Range       0.73      0.65      0.69       108\nCharacterize Distribution       0.94      0.74      0.83       136\n           Find Anomalies       0.72      0.66      0.69       108\n                  Cluster       0.58      0.83      0.68       110\n                Correlate       0.89      0.66      0.75       218\n\n                micro avg       0.71      0.71      0.71      1402\n                macro avg       0.73      0.71      0.71      1402\n             weighted avg       0.75      0.71      0.72      1402\n\nConfusion Matrix...\n[[ 98   1   4   0   1  13   0   0   0   1]\n [ 26 142   0  11   0   0   1   3   0   2]\n [ 28   6  57   0   4   4   0   2   8   0]\n [  4  12  27 107  10   1   0   0   0   0]\n [  7   1   0   0 116   0   0   0  25   0]\n [ 24   3   3   0   3  70   0   0   3   2]\n [  8   9   1   1   2   7 101   0   5   2]\n [  0  24   0   0   0   0   0  71   3  10]\n [  3   7   1   1   0   0   1   5  91   1]\n [ 10   3  17   0   1   1   4  18  21 143]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.932, Train Acc: 53.12%, Val Loss:  3.231, Val Acc: 42.98%, Time: 13.56s *\nEpoch: 2\nIter: 200, Train Loss:  1.814, Train Acc: 73.44%, Val Loss:  2.295, Val Acc: 61.24%, Time: 1.41s *\nIter: 300, Train Loss:  1.498, Train Acc: 71.88%, Val Loss:  1.971, Val Acc: 68.00%, Time: 13.95s *\nEpoch: 3\nIter: 400, Train Loss:  0.614, Train Acc: 90.62%, Val Loss:  1.933, Val Acc: 68.51%, Time: 1.63s *\nIter: 500, Train Loss: 0.8023, Train Acc: 89.06%, Val Loss:  2.138, Val Acc: 68.15%, Time: 14.19s \nEpoch: 4\nIter: 600, Train Loss: 0.4356, Train Acc: 95.31%, Val Loss:  2.246, Val Acc: 67.71%, Time: 1.87s \nIter: 700, Train Loss: 0.4047, Train Acc: 92.19%, Val Loss:  2.242, Val Acc: 69.31%, Time: 14.43s *\nEpoch: 5\nIter: 800, Train Loss: 0.4246, Train Acc: 93.75%, Val Loss:  2.285, Val Acc: 70.69%, Time: 2.08s *\nIter: 900, Train Loss: 0.3388, Train Acc: 93.75%, Val Loss:  2.552, Val Acc: 69.02%, Time: 14.63s \nEpoch: 6\nIter: 1000, Train Loss: 0.1971, Train Acc: 95.31%, Val Loss:  2.345, Val Acc: 70.04%, Time: 2.32s \nIter: 1100, Train Loss: 0.1592, Train Acc: 96.88%, Val Loss:  2.633, Val Acc: 69.31%, Time: 14.88s \nEpoch: 7\nIter: 1200, Train Loss: 0.09389, Train Acc: 100.00%, Val Loss:  2.554, Val Acc: 69.31%, Time: 2.54s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.51      0.50      0.51       117\n                   Filter       0.62      0.54      0.58       167\n    Compute Derived Value       0.53      0.40      0.45       154\n            Find Extremum       0.73      0.81      0.77       211\n                     Sort       0.82      0.92      0.86       118\n          Determine Range       0.63      0.74      0.68       157\nCharacterize Distribution       0.89      0.81      0.85        93\n           Find Anomalies       0.63      0.54      0.58       108\n                  Cluster       0.77      0.89      0.83       157\n                Correlate       0.84      0.82      0.83        93\n\n                micro avg       0.69      0.69      0.69      1375\n                macro avg       0.70      0.70      0.69      1375\n             weighted avg       0.69      0.69      0.69      1375\n\nConfusion Matrix...\n[[ 59   0  30   1   7  16   3   0   1   0]\n [  0  90  15   6   1   7   0  26  19   3]\n [ 50  15  61  18   0   4   1   2   1   2]\n [  3  16   3 171   4  12   0   1   0   1]\n [  0   0   0   8 108   0   0   0   2   0]\n [  2   6   4  24   1 116   0   1   1   2]\n [  2   2   1   0   1   9  75   1   2   0]\n [  0  17   1   3   2  19   5  58   2   1]\n [  0   0   1   2   8   0   0   1 140   5]\n [  0   0   0   0   0   1   0   2  14  76]]\nFold:  7\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.026, Train Acc: 57.81%, Val Loss:   3.45, Val Acc: 41.67%, Time: 13.50s *\nEpoch: 2\nIter: 200, Train Loss:  1.616, Train Acc: 78.12%, Val Loss:  2.476, Val Acc: 60.92%, Time: 1.41s *\nIter: 300, Train Loss: 0.9923, Train Acc: 87.50%, Val Loss:  2.281, Val Acc: 64.94%, Time: 13.99s *\nEpoch: 3\nIter: 400, Train Loss: 0.7665, Train Acc: 87.50%, Val Loss:  2.162, Val Acc: 67.03%, Time: 1.64s *\nIter: 500, Train Loss:  1.238, Train Acc: 78.12%, Val Loss:  2.412, Val Acc: 65.01%, Time: 14.22s \nEpoch: 4\nIter: 600, Train Loss: 0.5247, Train Acc: 93.75%, Val Loss:  2.374, Val Acc: 67.60%, Time: 1.87s *\nIter: 700, Train Loss: 0.9574, Train Acc: 82.81%, Val Loss:  2.598, Val Acc: 65.59%, Time: 14.40s \nEpoch: 5\nIter: 800, Train Loss: 0.3413, Train Acc: 96.88%, Val Loss:  2.516, Val Acc: 66.31%, Time: 2.10s \nIter: 900, Train Loss: 0.2888, Train Acc: 96.88%, Val Loss:  2.684, Val Acc: 65.73%, Time: 14.64s \nEpoch: 6\nIter: 1000, Train Loss: 0.2028, Train Acc: 98.44%, Val Loss:  2.703, Val Acc: 68.46%, Time: 2.31s *\nIter: 1100, Train Loss: 0.2359, Train Acc: 96.88%, Val Loss:  2.779, Val Acc: 67.46%, Time: 14.84s \nEpoch: 7\nIter: 1200, Train Loss: 0.06362, Train Acc: 100.00%, Val Loss:  2.961, Val Acc: 66.38%, Time: 2.60s \nIter: 1300, Train Loss: 0.04065, Train Acc: 100.00%, Val Loss:  3.159, Val Acc: 65.37%, Time: 15.15s \nEpoch: 8\nIter: 1400, Train Loss: 0.06625, Train Acc: 100.00%, Val Loss:  3.078, Val Acc: 65.30%, Time: 2.77s \n1401:1000  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.69      0.61      0.65       208\n                   Filter       0.70      0.48      0.57       144\n    Compute Derived Value       0.39      0.57      0.46       122\n            Find Extremum       0.75      0.57      0.65       168\n                     Sort       0.70      0.91      0.79       142\n          Determine Range       0.72      0.62      0.67       158\nCharacterize Distribution       0.68      0.87      0.77        71\n           Find Anomalies       0.52      0.76      0.62       140\n                  Cluster       0.73      0.76      0.75        80\n                Correlate       0.83      0.57      0.68       159\n\n                micro avg       0.65      0.65      0.65      1392\n                macro avg       0.67      0.67      0.66      1392\n             weighted avg       0.68      0.65      0.65      1392\n\nConfusion Matrix...\n[[127   2  55   7   4   0   5   6   1   1]\n [ 12  69  13   9  11   4   0  26   0   0]\n [ 24   1  69   1   5  12   3   6   0   1]\n [  4   1   5  95  20  19   0  19   5   0]\n [  0   2   2   0 129   1   1   0   7   0]\n [ 12  15  13   3   6  98   3   2   3   3]\n [  1   0   5   0   1   0  62   0   2   0]\n [  0   3  11   4   0   2  10 107   0   3]\n [  0   0   1   0   2   0   2   4  61  10]\n [  3   5   3   7   5   0   5  36   4  91]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.177, Train Acc: 57.81%, Val Loss:  3.087, Val Acc: 56.23%, Time: 13.58s *\nEpoch: 2\nIter: 200, Train Loss:  2.119, Train Acc: 70.31%, Val Loss:  2.058, Val Acc: 70.47%, Time: 1.45s *\nIter: 300, Train Loss:  1.227, Train Acc: 84.38%, Val Loss:  1.778, Val Acc: 72.80%, Time: 14.10s *\nEpoch: 3\nIter: 400, Train Loss: 0.7078, Train Acc: 89.06%, Val Loss:  1.694, Val Acc: 74.22%, Time: 1.69s *\nIter: 500, Train Loss:  1.002, Train Acc: 82.81%, Val Loss:  1.851, Val Acc: 72.38%, Time: 14.33s \nEpoch: 4\nIter: 600, Train Loss: 0.4282, Train Acc: 96.88%, Val Loss:  1.745, Val Acc: 72.59%, Time: 1.91s \nIter: 700, Train Loss: 0.3837, Train Acc: 93.75%, Val Loss:  1.702, Val Acc: 75.35%, Time: 14.52s *\nEpoch: 5\nIter: 800, Train Loss: 0.4225, Train Acc: 93.75%, Val Loss:  1.931, Val Acc: 74.01%, Time: 2.13s \nIter: 900, Train Loss:  0.257, Train Acc: 96.88%, Val Loss:  1.997, Val Acc: 73.23%, Time: 14.72s \nEpoch: 6\nIter: 1000, Train Loss: 0.1835, Train Acc: 95.31%, Val Loss:  1.965, Val Acc: 74.08%, Time: 2.36s \nIter: 1100, Train Loss: 0.1697, Train Acc: 98.44%, Val Loss:  1.825, Val Acc: 75.85%, Time: 14.94s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1351, Train Acc: 100.00%, Val Loss:   2.08, Val Acc: 73.87%, Time: 2.66s \nIter: 1300, Train Loss: 0.1319, Train Acc: 98.44%, Val Loss:   2.43, Val Acc: 71.39%, Time: 15.25s \nEpoch: 8\nIter: 1400, Train Loss: 0.1378, Train Acc: 98.44%, Val Loss:  2.093, Val Acc: 73.30%, Time: 2.83s \nIter: 1500, Train Loss: 0.06489, Train Acc: 98.44%, Val Loss:  2.127, Val Acc: 74.50%, Time: 15.42s \n1501:1100  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.84      0.58      0.69       115\n                   Filter       0.66      0.66      0.66       146\n    Compute Derived Value       0.74      0.89      0.81       213\n            Find Extremum       0.88      0.71      0.78       168\n                     Sort       0.84      0.88      0.86       113\n          Determine Range       0.37      0.52      0.43        75\nCharacterize Distribution       0.67      0.80      0.73        79\n           Find Anomalies       0.90      0.59      0.71       191\n                  Cluster       0.84      0.83      0.83       195\n                Correlate       0.71      0.97      0.82       117\n\n                micro avg       0.75      0.75      0.75      1412\n                macro avg       0.74      0.74      0.73      1412\n             weighted avg       0.77      0.75      0.75      1412\n\nConfusion Matrix...\n[[ 67   6  39   1   0   0   1   0   1   0]\n [  2  97   3   8   1  25   7   2   0   1]\n [  4   0 190   0   0   4   9   0   0   6]\n [  0   1  12 119  11  21   1   2   1   0]\n [  1   0   2   5  99   4   1   0   1   0]\n [  1  27   2   3   0  39   0   2   1   0]\n [  0   0   6   0   4   3  63   0   1   2]\n [  0  14   3   0   0   1   8 112  25  28]\n [  5   1   1   0   3   8   4   3 161   9]\n [  0   0   0   0   0   0   0   4   0 113]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.368, Train Acc: 48.44%, Val Loss:  3.423, Val Acc: 46.32%, Time: 13.52s *\nEpoch: 2\nIter: 200, Train Loss:  1.139, Train Acc: 87.50%, Val Loss:   2.49, Val Acc: 59.89%, Time: 1.41s *\nIter: 300, Train Loss:  1.183, Train Acc: 82.81%, Val Loss:  1.935, Val Acc: 70.59%, Time: 13.99s *\nEpoch: 3\nIter: 400, Train Loss: 0.6473, Train Acc: 90.62%, Val Loss:  2.004, Val Acc: 70.88%, Time: 1.65s *\nIter: 500, Train Loss: 0.7976, Train Acc: 85.94%, Val Loss:  1.748, Val Acc: 72.81%, Time: 14.18s *\nEpoch: 4\nIter: 600, Train Loss: 0.7203, Train Acc: 89.06%, Val Loss:  1.755, Val Acc: 72.59%, Time: 1.85s \nIter: 700, Train Loss: 0.2391, Train Acc: 98.44%, Val Loss:  1.943, Val Acc: 73.09%, Time: 14.43s *\nEpoch: 5\nIter: 800, Train Loss: 0.3768, Train Acc: 95.31%, Val Loss:  1.795, Val Acc: 74.23%, Time: 2.07s *\nIter: 900, Train Loss: 0.3957, Train Acc: 96.88%, Val Loss:  1.893, Val Acc: 73.73%, Time: 14.67s \nEpoch: 6\nIter: 1000, Train Loss: 0.3286, Train Acc: 93.75%, Val Loss:  1.912, Val Acc: 74.16%, Time: 2.32s \nIter: 1100, Train Loss: 0.3228, Train Acc: 96.88%, Val Loss:  1.979, Val Acc: 73.88%, Time: 14.89s \nEpoch: 7\nIter: 1200, Train Loss: 0.1162, Train Acc: 98.44%, Val Loss:  1.917, Val Acc: 75.66%, Time: 2.55s *\nIter: 1300, Train Loss: 0.09493, Train Acc: 100.00%, Val Loss:  2.119, Val Acc: 75.37%, Time: 15.09s \nEpoch: 8\nIter: 1400, Train Loss: 0.07538, Train Acc: 100.00%, Val Loss:  2.045, Val Acc: 73.52%, Time: 2.78s \nIter: 1500, Train Loss: 0.1162, Train Acc: 98.44%, Val Loss:  2.153, Val Acc: 75.66%, Time: 15.30s \nEpoch: 9\nIter: 1600, Train Loss: 0.02607, Train Acc: 100.00%, Val Loss:  2.188, Val Acc: 76.52%, Time: 3.01s *\nIter: 1700, Train Loss: 0.02358, Train Acc: 100.00%, Val Loss:  2.317, Val Acc: 75.37%, Time: 15.54s \nEpoch: 10\nIter: 1800, Train Loss: 0.03696, Train Acc: 98.44%, Val Loss:  2.135, Val Acc: 74.23%, Time: 3.22s \nIter: 1900, Train Loss: 0.1971, Train Acc: 98.44%, Val Loss:  2.302, Val Acc: 74.59%, Time: 15.80s \nEpoch: 11\nIter: 2000, Train Loss: 0.02018, Train Acc: 100.00%, Val Loss:  2.404, Val Acc: 76.02%, Time: 3.45s \n2001:1600  No optimization for a long time, auto-stopping...\n", "name": "stdout"}, {"output_type": "stream", "text": "Precision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.63      0.60      0.61       151\n                   Filter       0.64      0.73      0.68       147\n    Compute Derived Value       0.64      0.68      0.66       142\n            Find Extremum       0.81      0.80      0.81       163\n                     Sort       0.96      0.79      0.87       168\n          Determine Range       0.69      0.84      0.76       126\nCharacterize Distribution       0.87      0.75      0.81       165\n           Find Anomalies       0.76      0.84      0.80       140\n                  Cluster       0.81      0.85      0.83        78\n                Correlate       0.85      0.79      0.82       121\n\n                micro avg       0.76      0.76      0.76      1401\n                macro avg       0.77      0.77      0.76      1401\n             weighted avg       0.77      0.76      0.76      1401\n\nConfusion Matrix...\n[[ 90  27  16   3   0   8   6   1   0   0]\n [ 11 107   3   0   0   5   0  16   0   5]\n [ 18  16  96   1   0   2   7   0   0   2]\n [  8   1   3 131   1  13   0   6   0   0]\n [ 10   2   4   9 132   1   0   0   9   1]\n [  1   2   6   1   2 106   4   1   3   0]\n [  2   2  14   4   0   8 124   2   2   7]\n [  2   5   8   3   2   1   1 118   0   0]\n [  0   0   0   7   0   3   0   0  66   2]\n [  0   5   0   2   0   6   1  11   1  95]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.098, Train Acc: 60.94%, Val Loss:  3.517, Val Acc: 43.07%, Time: 13.60s *\nEpoch: 2\nIter: 200, Train Loss:   1.74, Train Acc: 78.12%, Val Loss:   2.45, Val Acc: 60.12%, Time: 1.57s *\nIter: 300, Train Loss:  1.461, Train Acc: 78.12%, Val Loss:  2.253, Val Acc: 62.34%, Time: 14.19s *\nEpoch: 3\nIter: 400, Train Loss: 0.7518, Train Acc: 87.50%, Val Loss:  2.162, Val Acc: 65.46%, Time: 1.91s *\nIter: 500, Train Loss: 0.9054, Train Acc: 85.94%, Val Loss:  2.436, Val Acc: 64.63%, Time: 14.61s \nEpoch: 4\nIter: 600, Train Loss: 0.7062, Train Acc: 92.19%, Val Loss:   2.58, Val Acc: 64.42%, Time: 2.26s \nIter: 700, Train Loss: 0.3915, Train Acc: 92.19%, Val Loss:  2.835, Val Acc: 64.70%, Time: 14.89s \nEpoch: 5\nIter: 800, Train Loss: 0.3223, Train Acc: 93.75%, Val Loss:  2.959, Val Acc: 63.73%, Time: 2.60s \n801:400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.44      0.77      0.56       110\n                   Filter       0.47      0.46      0.46       164\n    Compute Derived Value       0.57      0.39      0.46       254\n            Find Extremum       0.80      0.70      0.74       195\n                     Sort       0.91      0.69      0.79        72\n          Determine Range       0.65      0.75      0.70        93\nCharacterize Distribution       0.90      0.74      0.82       190\n           Find Anomalies       0.57      0.78      0.66       109\n                  Cluster       0.76      0.79      0.78       110\n                Correlate       0.57      0.63      0.60       145\n\n                micro avg       0.64      0.64      0.64      1442\n                macro avg       0.66      0.67      0.66      1442\n             weighted avg       0.66      0.64      0.64      1442\n\nConfusion Matrix...\n[[ 85   8  15   0   0   2   0   0   0   0]\n [ 40  75  16  10   3  10   0   7   1   2]\n [ 45  27  99   8   0   7   0  37   3  28]\n [ 13  18  22 136   2   1   0   3   0   0]\n [  1   0   0   0  50   8   0   0  13   0]\n [  3  16   2   1   0  70   0   1   0   0]\n [  5   3   8   0   0   2 141   5   9  17]\n [  0  11   0   0   0   2   0  85   0  11]\n [  1   0   0   0   0   1   2   9  87  10]\n [  2   1  13  16   0   5  13   3   1  91]]\n[0.6947674418604651, 0.7103942652329749, 0.7177476835352815, 0.6833681280445373, 0.7104136947218259, 0.6938181818181818, 0.6522988505747126, 0.7507082152974505, 0.7601713062098501, 0.6373092926490985]\n0.7010997059944378, 0.03641293707651303, 0.03838260581939135, 0.0013259019865380973\ntable\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.016, Train Acc: 46.88%, Val Loss:  3.334, Val Acc: 47.04%, Time: 14.13s *\nEpoch: 2\nIter: 200, Train Loss:  1.702, Train Acc: 76.56%, Val Loss:  2.392, Val Acc: 62.90%, Time: 3.38s *\nIter: 300, Train Loss:  1.292, Train Acc: 84.38%, Val Loss:  2.195, Val Acc: 67.10%, Time: 16.56s *\nEpoch: 3\nIter: 400, Train Loss: 0.7103, Train Acc: 89.06%, Val Loss:  2.066, Val Acc: 69.62%, Time: 4.98s *\nIter: 500, Train Loss:  1.141, Train Acc: 84.38%, Val Loss:  2.235, Val Acc: 69.58%, Time: 18.13s \nEpoch: 4\nIter: 600, Train Loss: 0.5708, Train Acc: 89.06%, Val Loss:  2.638, Val Acc: 65.98%, Time: 6.61s \nIter: 700, Train Loss: 0.2188, Train Acc: 100.00%, Val Loss:  2.218, Val Acc: 70.46%, Time: 19.77s *\nEpoch: 5\nIter: 800, Train Loss: 0.3956, Train Acc: 95.31%, Val Loss:    2.7, Val Acc: 66.26%, Time: 8.17s \nIter: 900, Train Loss: 0.3764, Train Acc: 93.75%, Val Loss:  2.468, Val Acc: 68.64%, Time: 21.34s \nEpoch: 6\nIter: 1000, Train Loss: 0.09879, Train Acc: 100.00%, Val Loss:  2.744, Val Acc: 66.87%, Time: 9.75s \nIter: 1100, Train Loss: 0.1963, Train Acc: 96.88%, Val Loss:   2.67, Val Acc: 66.26%, Time: 22.94s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.65      0.43      0.52       205\n                   Filter       0.70      0.50      0.59       232\n    Compute Derived Value       0.38      0.55      0.45       213\n            Find Extremum       0.73      0.66      0.69       210\n                     Sort       0.64      0.89      0.75       197\n          Determine Range       0.64      0.43      0.52       222\nCharacterize Distribution       0.66      0.90      0.76       218\n           Find Anomalies       0.93      0.52      0.66       209\n                  Cluster       0.75      0.92      0.82       212\n                Correlate       0.74      0.82      0.78       225\n\n                micro avg       0.66      0.66      0.66      2143\n                macro avg       0.68      0.66      0.65      2143\n             weighted avg       0.68      0.66      0.65      2143\n\nConfusion Matrix...\n[[ 89   2  40  27  11   8  14   0   6   8]\n [  9 117  23  13  22  16   5   2  21   4]\n [ 16   4 118   5  14   5  23   3   0  25]\n [  2   2  47 138   5   8   3   0   0   5]\n [  0   0  10   4 176   1   1   0   5   0]\n [ 12  35  33   0  25  96  13   0   2   6]\n [  4   1   7   0   8   2 196   0   0   0]\n [  1   5  13   3   3  13  37 108  11  15]\n [  2   0   1   0   9   0   3   1 194   2]\n [  1   0  16   0   0   0   2   2  20 184]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.846, Train Acc: 60.94%, Val Loss:  3.291, Val Acc: 42.85%, Time: 13.58s *\nEpoch: 2\nIter: 200, Train Loss:  2.063, Train Acc: 68.75%, Val Loss:  2.515, Val Acc: 58.97%, Time: 1.62s *\nIter: 300, Train Loss:  1.406, Train Acc: 82.81%, Val Loss:  2.321, Val Acc: 64.24%, Time: 14.26s *\nEpoch: 3\nIter: 400, Train Loss: 0.7212, Train Acc: 85.94%, Val Loss:    2.3, Val Acc: 66.13%, Time: 1.97s *\nIter: 500, Train Loss: 0.9848, Train Acc: 84.38%, Val Loss:  2.312, Val Acc: 66.80%, Time: 14.64s *\nEpoch: 4\nIter: 600, Train Loss: 0.9718, Train Acc: 85.94%, Val Loss:  2.123, Val Acc: 71.12%, Time: 2.31s *\nIter: 700, Train Loss: 0.5343, Train Acc: 95.31%, Val Loss:  2.314, Val Acc: 69.70%, Time: 14.97s \nEpoch: 5\nIter: 800, Train Loss:  0.496, Train Acc: 90.62%, Val Loss:  2.283, Val Acc: 69.30%, Time: 2.64s \nIter: 900, Train Loss: 0.3819, Train Acc: 93.75%, Val Loss:  2.414, Val Acc: 69.64%, Time: 15.35s \nEpoch: 6\nIter: 1000, Train Loss: 0.1703, Train Acc: 98.44%, Val Loss:  2.302, Val Acc: 71.93%, Time: 2.98s *\nIter: 1100, Train Loss: 0.5311, Train Acc: 90.62%, Val Loss:  2.429, Val Acc: 71.46%, Time: 15.69s \nEpoch: 7\nIter: 1200, Train Loss: 0.1362, Train Acc: 100.00%, Val Loss:   2.43, Val Acc: 72.33%, Time: 3.31s *\nIter: 1300, Train Loss: 0.1767, Train Acc: 95.31%, Val Loss:  2.745, Val Acc: 68.56%, Time: 15.95s \nEpoch: 8\nIter: 1400, Train Loss: 0.1269, Train Acc: 96.88%, Val Loss:  2.474, Val Acc: 70.58%, Time: 3.66s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1500, Train Loss: 0.1566, Train Acc: 98.44%, Val Loss:  2.376, Val Acc: 71.39%, Time: 16.34s \nEpoch: 9\nIter: 1600, Train Loss: 0.01279, Train Acc: 100.00%, Val Loss:    2.7, Val Acc: 72.06%, Time: 4.03s \n1601:1200  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.80      0.55      0.65       150\n                   Filter       0.58      0.55      0.57       137\n    Compute Derived Value       0.54      0.72      0.61       185\n            Find Extremum       0.84      0.79      0.81       192\n                     Sort       0.76      0.78      0.77       157\n          Determine Range       0.70      0.76      0.73       163\nCharacterize Distribution       0.85      0.76      0.80        98\n           Find Anomalies       0.72      0.82      0.77       120\n                  Cluster       0.95      0.74      0.83       129\n                Correlate       0.72      0.76      0.74       151\n\n                micro avg       0.72      0.72      0.72      1482\n                macro avg       0.75      0.72      0.73      1482\n             weighted avg       0.74      0.72      0.72      1482\n\nConfusion Matrix...\n[[ 82  30  30   1   1   5   1   0   0   0]\n [  0  76  29   6   0   5   0   7   1  13]\n [  3   0 133   8   2  25   1  10   1   2]\n [  1   1   5 151  14   4   3  11   0   2]\n [ 10   5   6   5 122   3   1   3   1   1]\n [  2  13   6   7   6 124   1   1   1   2]\n [  1   0   5   0   0   1  74   0   0  17]\n [  0   4  13   1   0   1   0  98   0   3]\n [  3   0   4   0  16   5   1   0  96   4]\n [  1   1  17   0   0   5   5   6   1 115]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.009, Train Acc: 59.38%, Val Loss:  3.204, Val Acc: 47.12%, Time: 14.55s *\nEpoch: 2\nIter: 200, Train Loss:   1.32, Train Acc: 84.38%, Val Loss:  2.548, Val Acc: 61.58%, Time: 4.71s *\nIter: 300, Train Loss: 0.8538, Train Acc: 87.50%, Val Loss:  2.195, Val Acc: 66.09%, Time: 18.26s *\nEpoch: 3\nIter: 400, Train Loss: 0.7388, Train Acc: 89.06%, Val Loss:  2.359, Val Acc: 66.32%, Time: 7.18s *\nIter: 500, Train Loss: 0.5801, Train Acc: 90.62%, Val Loss:  2.385, Val Acc: 67.36%, Time: 20.79s *\nEpoch: 4\nIter: 600, Train Loss: 0.4415, Train Acc: 95.31%, Val Loss:  2.576, Val Acc: 68.78%, Time: 9.70s *\nIter: 700, Train Loss: 0.7192, Train Acc: 92.19%, Val Loss:  2.427, Val Acc: 69.08%, Time: 23.33s *\nEpoch: 5\nIter: 800, Train Loss: 0.4117, Train Acc: 95.31%, Val Loss:   2.53, Val Acc: 67.81%, Time: 12.25s \nEpoch: 6\nIter: 900, Train Loss:  0.191, Train Acc: 98.44%, Val Loss:  2.571, Val Acc: 68.86%, Time: 3.36s \nIter: 1000, Train Loss: 0.1949, Train Acc: 96.88%, Val Loss:  2.847, Val Acc: 67.18%, Time: 16.94s \nEpoch: 7\nIter: 1100, Train Loss: 0.1632, Train Acc: 96.88%, Val Loss:  2.892, Val Acc: 68.26%, Time: 5.86s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.58      0.60      0.59       252\n                   Filter       0.62      0.61      0.62       238\n    Compute Derived Value       0.65      0.62      0.64       263\n            Find Extremum       0.69      0.76      0.72       290\n                     Sort       0.88      0.83      0.85       258\n          Determine Range       0.73      0.52      0.61       258\nCharacterize Distribution       0.87      0.60      0.71       280\n           Find Anomalies       0.59      0.65      0.62       275\n                  Cluster       0.72      0.80      0.76       269\n                Correlate       0.64      0.84      0.72       295\n\n                micro avg       0.69      0.69      0.69      2678\n                macro avg       0.70      0.68      0.68      2678\n             weighted avg       0.70      0.69      0.69      2678\n\nConfusion Matrix...\n[[151   5  19  24  10   9   1   2   6  25]\n [  8 146  17   8   0   0   1  47   5   6]\n [ 36  30 164   7   1   2   0  17   1   5]\n [ 15  18  15 220   8   6   1   2   3   2]\n [  1   0   0  26 215   0   2   3  11   0]\n [ 31  30  12  21   4 135   2  14   8   1]\n [  8   1  15   3   0  13 167   9  25  39]\n [  7   2   2  11   0   1   1 178  19  54]\n [  1   1   2   1   7  16   1  16 216   8]\n [  2   2   6   0   0   2  15  13   8 247]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.242, Train Acc: 42.19%, Val Loss:   3.24, Val Acc: 52.17%, Time: 13.30s *\nIter: 200, Train Loss:  2.022, Train Acc: 68.75%, Val Loss:  2.139, Val Acc: 66.73%, Time: 25.61s *\nEpoch: 2\nIter: 300, Train Loss:  1.023, Train Acc: 85.94%, Val Loss:  1.672, Val Acc: 75.80%, Time: 11.87s *\nIter: 400, Train Loss: 0.9392, Train Acc: 87.50%, Val Loss:  1.651, Val Acc: 74.64%, Time: 24.23s \nEpoch: 3\nIter: 500, Train Loss:  0.862, Train Acc: 82.81%, Val Loss:  1.754, Val Acc: 74.54%, Time: 11.41s \nIter: 600, Train Loss: 0.8925, Train Acc: 85.94%, Val Loss:  1.587, Val Acc: 78.11%, Time: 23.75s *\nEpoch: 4\nIter: 700, Train Loss: 0.3344, Train Acc: 96.88%, Val Loss:  1.855, Val Acc: 75.51%, Time: 10.96s \nIter: 800, Train Loss: 0.2377, Train Acc: 98.44%, Val Loss:  1.746, Val Acc: 78.69%, Time: 23.30s *\nEpoch: 5\nIter: 900, Train Loss: 0.2907, Train Acc: 95.31%, Val Loss:  2.055, Val Acc: 74.25%, Time: 10.53s \nIter: 1000, Train Loss: 0.2744, Train Acc: 95.31%, Val Loss:  1.995, Val Acc: 75.99%, Time: 22.84s \nEpoch: 6\nIter: 1100, Train Loss: 0.1283, Train Acc: 100.00%, Val Loss:  1.972, Val Acc: 75.89%, Time: 10.03s \nIter: 1200, Train Loss: 0.3859, Train Acc: 96.88%, Val Loss:  2.011, Val Acc: 75.70%, Time: 22.35s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.69      0.61        80\n                   Filter       0.68      0.57      0.62       113\n    Compute Derived Value       0.86      0.62      0.72       107\n            Find Extremum       0.84      0.85      0.84       178\n                     Sort       0.83      0.68      0.75        85\n          Determine Range       0.71      0.69      0.70       101\nCharacterize Distribution       0.69      0.81      0.74        95\n           Find Anomalies       0.82      0.78      0.80        94\n                  Cluster       0.85      0.95      0.90        91\n                Correlate       0.75      0.91      0.82        93\n\n                micro avg       0.76      0.76      0.76      1037\n                macro avg       0.76      0.75      0.75      1037\n             weighted avg       0.76      0.76      0.76      1037\n\nConfusion Matrix...\n[[ 55   1   3   2   0   2  16   0   0   1]\n [ 19  64   1   0   4  14   0   9   1   1]\n [ 13   1  66   9   0   0   5   1   2  10]\n [  2   5   1 151   1   1   4   1   9   3]\n [  8   4   0   9  58   4   0   1   1   0]\n [  4   0   4   8   4  70   2   3   1   5]\n [  0   1   2   1   3   7  77   1   1   2]\n [  0  16   0   0   0   1   0  73   0   4]\n [  0   0   0   0   0   0   2   0  86   3]\n [  0   2   0   0   0   0   6   0   0  85]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.686, Train Acc: 59.38%, Val Loss:   3.63, Val Acc: 38.61%, Time: 12.98s *\nIter: 200, Train Loss:  1.968, Train Acc: 64.06%, Val Loss:  2.761, Val Acc: 53.48%, Time: 25.07s *\nEpoch: 2\nIter: 300, Train Loss:  1.041, Train Acc: 82.81%, Val Loss:  2.492, Val Acc: 58.25%, Time: 11.11s *\nIter: 400, Train Loss: 0.7555, Train Acc: 87.50%, Val Loss:   2.32, Val Acc: 63.57%, Time: 23.13s *\nEpoch: 3\nIter: 500, Train Loss: 0.7467, Train Acc: 84.38%, Val Loss:  2.414, Val Acc: 61.39%, Time: 10.25s \nIter: 600, Train Loss: 0.6981, Train Acc: 87.50%, Val Loss:  2.399, Val Acc: 64.67%, Time: 22.27s *\nEpoch: 4\nIter: 700, Train Loss: 0.6529, Train Acc: 95.31%, Val Loss:  2.397, Val Acc: 64.80%, Time: 9.38s *\nIter: 800, Train Loss:  0.537, Train Acc: 92.19%, Val Loss:  2.297, Val Acc: 66.71%, Time: 21.44s *\nEpoch: 5\nIter: 900, Train Loss: 0.3479, Train Acc: 96.88%, Val Loss:   2.42, Val Acc: 64.67%, Time: 8.41s \nIter: 1000, Train Loss: 0.2043, Train Acc: 98.44%, Val Loss:  2.426, Val Acc: 67.53%, Time: 20.47s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 6\nIter: 1100, Train Loss: 0.2989, Train Acc: 96.88%, Val Loss:  2.439, Val Acc: 67.26%, Time: 7.49s \nIter: 1200, Train Loss: 0.5312, Train Acc: 95.31%, Val Loss:  2.615, Val Acc: 65.62%, Time: 19.58s \nEpoch: 7\nIter: 1300, Train Loss: 0.4035, Train Acc: 96.88%, Val Loss:  2.526, Val Acc: 66.98%, Time: 6.58s \nIter: 1400, Train Loss: 0.06921, Train Acc: 100.00%, Val Loss:  2.778, Val Acc: 66.71%, Time: 18.59s \n1401:1000  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.40      0.49      0.44        79\n                   Filter       0.56      0.44      0.49       119\n    Compute Derived Value       0.77      0.55      0.64       111\n            Find Extremum       0.70      0.86      0.77        74\n                     Sort       0.60      1.00      0.75        29\n          Determine Range       0.66      0.82      0.73        73\nCharacterize Distribution       0.77      0.88      0.82        26\n           Find Anomalies       0.70      0.87      0.77        77\n                  Cluster       0.89      0.63      0.74        54\n                Correlate       0.78      0.58      0.67        91\n\n                micro avg       0.66      0.66      0.66       733\n                macro avg       0.68      0.71      0.68       733\n             weighted avg       0.67      0.66      0.65       733\n\nConfusion Matrix...\n[[39  9  8  3  9  7  0  0  1  3]\n [49 52  1  0  0 15  0  1  0  1]\n [ 4 10 61 20  0  7  2  5  2  0]\n [ 0  6  0 64  4  0  0  0  0  0]\n [ 0  0  0  0 29  0  0  0  0  0]\n [ 2  5  1  0  0 60  4  0  1  0]\n [ 0  0  0  0  2  1 23  0  0  0]\n [ 0  7  0  3  0  0  0 67  0  0]\n [ 0  1  0  0  2  1  0  5 34 11]\n [ 4  3  8  2  2  0  1 18  0 53]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.634, Train Acc: 62.50%, Val Loss:  3.224, Val Acc: 43.48%, Time: 14.00s *\nEpoch: 2\nIter: 200, Train Loss:  1.567, Train Acc: 78.12%, Val Loss:  2.727, Val Acc: 56.93%, Time: 2.92s *\nIter: 300, Train Loss:  1.386, Train Acc: 76.56%, Val Loss:   2.56, Val Acc: 60.59%, Time: 15.91s *\nEpoch: 3\nIter: 400, Train Loss:  1.042, Train Acc: 85.94%, Val Loss:  2.329, Val Acc: 63.32%, Time: 4.15s *\nIter: 500, Train Loss: 0.9119, Train Acc: 89.06%, Val Loss:  2.474, Val Acc: 63.94%, Time: 17.23s *\nEpoch: 4\nIter: 600, Train Loss: 0.4789, Train Acc: 93.75%, Val Loss:   2.46, Val Acc: 65.33%, Time: 5.37s *\nIter: 700, Train Loss: 0.3131, Train Acc: 95.31%, Val Loss:  2.792, Val Acc: 61.88%, Time: 18.45s \nEpoch: 5\nIter: 800, Train Loss: 0.4394, Train Acc: 93.75%, Val Loss:  2.854, Val Acc: 62.29%, Time: 6.66s \nIter: 900, Train Loss: 0.7102, Train Acc: 87.50%, Val Loss:  3.009, Val Acc: 60.38%, Time: 19.66s \nEpoch: 6\nIter: 1000, Train Loss: 0.1678, Train Acc: 98.44%, Val Loss:  3.705, Val Acc: 60.79%, Time: 7.90s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.34      0.48      0.40       164\n                   Filter       0.27      0.55      0.36       192\n    Compute Derived Value       0.69      0.45      0.54       261\n            Find Extremum       0.63      0.57      0.60       243\n                     Sort       0.74      0.95      0.83       170\n          Determine Range       0.76      0.57      0.65       162\nCharacterize Distribution       0.88      0.73      0.80       218\n           Find Anomalies       0.84      0.58      0.69       183\n                  Cluster       0.86      0.78      0.82       152\n                Correlate       0.72      0.52      0.61       196\n\n                micro avg       0.61      0.61      0.61      1941\n                macro avg       0.67      0.62      0.63      1941\n             weighted avg       0.67      0.61      0.62      1941\n\nConfusion Matrix...\n[[ 79  59  12   8   0   2   1   1   2   0]\n [ 41 106  14   2   4   5   0  10   0  10]\n [ 50  41 117  41   4   0   1   0   1   6]\n [ 22  49   0 138  26   2   1   2   1   2]\n [  1   6   0   0 161   0   0   0   2   0]\n [ 19  20   0   1  10  93  11   3   3   2]\n [  6   1  13  14   4   6 159   0   8   7]\n [  1  62   1   0   0   2   4 107   1   5]\n [  5   4   1   0   2  10   2   2 119   7]\n [  5  49  11  16   6   2   2   2   1 102]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.949, Train Acc: 51.56%, Val Loss:  3.249, Val Acc: 46.18%, Time: 13.15s *\nIter: 200, Train Loss:  1.601, Train Acc: 76.56%, Val Loss:  2.367, Val Acc: 65.05%, Time: 25.31s *\nEpoch: 2\nIter: 300, Train Loss:  1.215, Train Acc: 84.38%, Val Loss:   1.88, Val Acc: 73.15%, Time: 11.45s *\nIter: 400, Train Loss:  1.108, Train Acc: 82.81%, Val Loss:  1.902, Val Acc: 71.41%, Time: 23.58s \nEpoch: 3\nIter: 500, Train Loss:  1.126, Train Acc: 82.81%, Val Loss:  1.716, Val Acc: 74.88%, Time: 10.84s *\nIter: 600, Train Loss:  0.922, Train Acc: 87.50%, Val Loss:  1.761, Val Acc: 74.65%, Time: 23.05s \nEpoch: 4\nIter: 700, Train Loss: 0.8282, Train Acc: 87.50%, Val Loss:  1.712, Val Acc: 77.78%, Time: 10.12s *\nIter: 800, Train Loss: 0.3916, Train Acc: 95.31%, Val Loss:  1.735, Val Acc: 75.93%, Time: 22.32s \nEpoch: 5\nIter: 900, Train Loss: 0.2769, Train Acc: 96.88%, Val Loss:   1.85, Val Acc: 75.35%, Time: 9.41s \nIter: 1000, Train Loss: 0.3445, Train Acc: 95.31%, Val Loss:  2.023, Val Acc: 73.84%, Time: 21.55s \nEpoch: 6\nIter: 1100, Train Loss: 0.1218, Train Acc: 98.44%, Val Loss:  2.061, Val Acc: 74.31%, Time: 8.77s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.69      0.26      0.38        77\n                   Filter       0.51      0.94      0.66        63\n    Compute Derived Value       0.54      0.78      0.64        86\n            Find Extremum       0.99      0.73      0.84       135\n                     Sort       0.97      0.61      0.75        61\n          Determine Range       0.83      0.96      0.89        57\nCharacterize Distribution       0.86      0.61      0.72       111\n           Find Anomalies       0.83      0.98      0.90        59\n                  Cluster       0.95      0.77      0.85        81\n                Correlate       0.70      0.93      0.79       134\n\n                micro avg       0.75      0.75      0.75       864\n                macro avg       0.79      0.76      0.74       864\n             weighted avg       0.80      0.75      0.75       864\n\nConfusion Matrix...\n[[ 20   9  40   0   0   4   0   0   0   4]\n [  1  59   2   0   0   0   0   0   0   1]\n [  1   3  67   0   0   4   0   3   0   8]\n [  0  23   6  99   1   1   4   0   0   1]\n [  2  13   2   0  37   2   2   0   3   0]\n [  1   0   0   0   0  55   0   0   0   1]\n [  1   7   5   0   0   0  68   0   0  30]\n [  0   1   0   0   0   0   0  58   0   0]\n [  3   0   0   0   0   0   2   5  62   9]\n [  0   0   2   1   0   0   3   4   0 124]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:   3.21, Train Acc: 48.44%, Val Loss:   3.59, Val Acc: 38.54%, Time: 13.11s *\nIter: 200, Train Loss:  1.786, Train Acc: 75.00%, Val Loss:    2.7, Val Acc: 55.23%, Time: 25.26s *\nEpoch: 2\nIter: 300, Train Loss:  1.108, Train Acc: 84.38%, Val Loss:  2.239, Val Acc: 63.15%, Time: 11.64s *\nIter: 400, Train Loss:  1.146, Train Acc: 85.94%, Val Loss:  2.324, Val Acc: 62.83%, Time: 23.87s \nEpoch: 3\nIter: 500, Train Loss: 0.5744, Train Acc: 93.75%, Val Loss:  2.312, Val Acc: 67.05%, Time: 11.12s *\nIter: 600, Train Loss: 0.7984, Train Acc: 90.62%, Val Loss:  2.119, Val Acc: 68.11%, Time: 23.29s *\nEpoch: 4\nIter: 700, Train Loss: 0.3069, Train Acc: 96.88%, Val Loss:   2.59, Val Acc: 65.58%, Time: 10.50s \nIter: 800, Train Loss: 0.6531, Train Acc: 92.19%, Val Loss:  2.461, Val Acc: 68.64%, Time: 22.75s *\nEpoch: 5\nIter: 900, Train Loss:  0.465, Train Acc: 93.75%, Val Loss:  2.529, Val Acc: 67.05%, Time: 9.97s \nIter: 1000, Train Loss: 0.4056, Train Acc: 92.19%, Val Loss:  2.637, Val Acc: 66.42%, Time: 22.21s \nEpoch: 6\nIter: 1100, Train Loss: 0.0901, Train Acc: 100.00%, Val Loss:  3.015, Val Acc: 65.15%, Time: 9.37s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1200, Train Loss: 0.09278, Train Acc: 100.00%, Val Loss:  2.608, Val Acc: 68.32%, Time: 21.58s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.54      0.54        91\n                   Filter       0.58      0.58      0.58        88\n    Compute Derived Value       0.66      0.62      0.64       102\n            Find Extremum       0.65      0.69      0.67        95\n                     Sort       0.81      0.83      0.82        88\n          Determine Range       0.76      0.64      0.70        95\nCharacterize Distribution       0.71      0.93      0.80        94\n           Find Anomalies       0.61      0.64      0.63        90\n                  Cluster       0.76      0.72      0.74       112\n                Correlate       0.73      0.62      0.67        92\n\n                micro avg       0.68      0.68      0.68       947\n                macro avg       0.68      0.68      0.68       947\n             weighted avg       0.68      0.68      0.68       947\n\nConfusion Matrix...\n[[49  2 18  0  0  2  3 13  3  1]\n [17 51  0  2  1  1  0  4  8  4]\n [24  0 63  0  0  0  5  0  2  8]\n [ 0 12  2 66  4 10  0  0  1  0]\n [ 0  0  6  5 73  4  0  0  0  0]\n [ 0 11  1 10  7 61  1  2  0  2]\n [ 0  0  1  1  2  1 87  0  1  1]\n [ 0 11  3 16  0  0  1 58  0  1]\n [ 0  1  1  1  2  1 18  3 81  4]\n [ 0  0  1  0  1  0  8 15 10 57]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.926, Train Acc: 54.69%, Val Loss:  3.003, Val Acc: 52.95%, Time: 13.34s *\nIter: 200, Train Loss:  2.246, Train Acc: 67.19%, Val Loss:   2.18, Val Acc: 64.31%, Time: 25.78s *\nEpoch: 2\nIter: 300, Train Loss:  1.449, Train Acc: 79.69%, Val Loss:  2.088, Val Acc: 66.50%, Time: 12.27s *\nIter: 400, Train Loss: 0.9095, Train Acc: 84.38%, Val Loss:  2.057, Val Acc: 70.03%, Time: 24.63s *\nEpoch: 3\nIter: 500, Train Loss:   0.84, Train Acc: 87.50%, Val Loss:  1.973, Val Acc: 69.11%, Time: 12.15s \nIter: 600, Train Loss:   0.61, Train Acc: 92.19%, Val Loss:  1.991, Val Acc: 70.96%, Time: 24.53s *\nEpoch: 4\nIter: 700, Train Loss: 0.4649, Train Acc: 96.88%, Val Loss:  2.159, Val Acc: 69.53%, Time: 12.05s \nIter: 800, Train Loss: 0.3117, Train Acc: 95.31%, Val Loss:  2.092, Val Acc: 70.88%, Time: 24.38s \nEpoch: 5\nIter: 900, Train Loss: 0.3414, Train Acc: 96.88%, Val Loss:   2.28, Val Acc: 70.54%, Time: 11.95s \nIter: 1000, Train Loss: 0.6634, Train Acc: 90.62%, Val Loss:  2.329, Val Acc: 69.36%, Time: 24.35s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.65      0.67      0.66       172\n                   Filter       0.64      0.78      0.70       160\n    Compute Derived Value       0.33      0.23      0.27       112\n            Find Extremum       0.83      0.55      0.66       122\n                     Sort       0.80      0.95      0.87        66\n          Determine Range       0.70      0.47      0.56        97\nCharacterize Distribution       0.88      0.91      0.89        96\n           Find Anomalies       0.90      0.67      0.77       155\n                  Cluster       0.73      0.68      0.70        59\n                Correlate       0.60      0.99      0.75       149\n\n                micro avg       0.69      0.69      0.69      1188\n                macro avg       0.71      0.69      0.68      1188\n             weighted avg       0.70      0.69      0.68      1188\n\nConfusion Matrix...\n[[115  15   3   0   0   1   4   0  10  24]\n [  3 124   1   9   1  10   0  11   0   1]\n [ 23  21  26   0   0   0   4   0   0  38]\n [  5   7  27  67  13   0   0   1   0   2]\n [  3   0   0   0  63   0   0   0   0   0]\n [ 18  12  13   1   0  46   0   0   2   5]\n [  0   0   4   0   0   3  87   0   0   2]\n [  5  13   3   1   2   5   2 104   3  17]\n [  5   1   0   3   0   1   2   0  40   7]\n [  0   1   1   0   0   0   0   0   0 147]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:   2.97, Train Acc: 54.69%, Val Loss:  3.317, Val Acc: 49.02%, Time: 13.15s *\nIter: 200, Train Loss:  1.455, Train Acc: 82.81%, Val Loss:  2.503, Val Acc: 63.60%, Time: 25.43s *\nEpoch: 2\nIter: 300, Train Loss: 0.9887, Train Acc: 84.38%, Val Loss:  2.426, Val Acc: 60.86%, Time: 11.75s \nIter: 400, Train Loss: 0.6022, Train Acc: 93.75%, Val Loss:  2.146, Val Acc: 65.66%, Time: 23.98s *\nEpoch: 3\nIter: 500, Train Loss: 0.6811, Train Acc: 90.62%, Val Loss:   2.16, Val Acc: 67.03%, Time: 11.29s *\nIter: 600, Train Loss: 0.9403, Train Acc: 87.50%, Val Loss:  2.336, Val Acc: 66.63%, Time: 23.51s \nEpoch: 4\nIter: 700, Train Loss: 0.5382, Train Acc: 92.19%, Val Loss:  2.342, Val Acc: 66.24%, Time: 10.91s \nIter: 800, Train Loss: 0.4601, Train Acc: 92.19%, Val Loss:  2.421, Val Acc: 64.97%, Time: 23.08s \nEpoch: 5\nIter: 900, Train Loss: 0.1419, Train Acc: 98.44%, Val Loss:  2.585, Val Acc: 67.03%, Time: 10.36s \n901:500  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.83      0.66      0.73        94\n                   Filter       0.57      0.56      0.57       110\n    Compute Derived Value       0.62      0.44      0.52        95\n            Find Extremum       0.86      0.83      0.84       121\n                     Sort       0.78      0.55      0.64        95\n          Determine Range       0.59      0.65      0.62        94\nCharacterize Distribution       0.43      0.84      0.57        93\n           Find Anomalies       0.67      0.58      0.62       111\n                  Cluster       0.77      0.68      0.72       111\n                Correlate       0.75      0.84      0.79        98\n\n                micro avg       0.66      0.66      0.66      1022\n                macro avg       0.69      0.66      0.66      1022\n             weighted avg       0.69      0.66      0.67      1022\n\nConfusion Matrix...\n[[ 62   1  11   0   0   3  16   0   1   0]\n [  2  62   1   2   2   6   7  13  10   5]\n [  1   5  42   0   2  15  22   4   2   2]\n [  4   3   1 100   2   6   5   0   0   0]\n [  2   0   0   2  52   4  23   0   7   5]\n [  1   0   2  10   1  61  16   0   1   2]\n [  0   0   2   0   2   7  78   2   1   1]\n [  0  33   7   1   0   0   4  64   0   2]\n [  3   2   2   0   5   0   8   5  75  11]\n [  0   3   0   1   1   1   1   8   1  82]]\n[0.6607559496033598, 0.7226720647773279, 0.686706497386109, 0.7569913211186113, 0.6575716234652115, 0.6084492529623905, 0.7511574074074074, 0.6821541710665259, 0.6893939393939394, 0.6634050880626223]\n0.6879257315243504, 0.043116009530042625, 0.045448264577486915, 0.0018589902777947265\n", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nsplit_info = {\n    # \"random\": False,\n    # \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "bundle\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.302, Train Acc: 32.81%, Val Loss:  3.443, Val Acc: 40.69%, Time: 17.16s *\nEpoch: 2\nIter: 200, Train Loss:    1.6, Train Acc: 73.44%, Val Loss:  2.404, Val Acc: 60.24%, Time: 1.72s *\nIter: 300, Train Loss:  1.362, Train Acc: 76.56%, Val Loss:  2.065, Val Acc: 67.15%, Time: 17.61s *\nEpoch: 3\nIter: 400, Train Loss: 0.6613, Train Acc: 89.06%, Val Loss:  2.011, Val Acc: 69.16%, Time: 2.01s *\nIter: 500, Train Loss: 0.8318, Train Acc: 82.81%, Val Loss:   2.01, Val Acc: 71.03%, Time: 17.97s *\nEpoch: 4\nIter: 600, Train Loss: 0.6473, Train Acc: 89.06%, Val Loss:   1.96, Val Acc: 72.90%, Time: 2.27s *\nIter: 700, Train Loss: 0.6764, Train Acc: 85.94%, Val Loss:   1.98, Val Acc: 73.62%, Time: 18.18s *\nEpoch: 5\nIter: 800, Train Loss: 0.4024, Train Acc: 93.75%, Val Loss:  2.069, Val Acc: 73.04%, Time: 2.58s \nIter: 900, Train Loss: 0.4899, Train Acc: 92.19%, Val Loss:  2.173, Val Acc: 73.26%, Time: 18.58s \nEpoch: 6\nIter: 1000, Train Loss: 0.3521, Train Acc: 96.88%, Val Loss:  2.164, Val Acc: 72.18%, Time: 2.89s \nIter: 1100, Train Loss: 0.1407, Train Acc: 98.44%, Val Loss:  2.294, Val Acc: 72.61%, Time: 18.81s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.64      0.66      0.65       115\n                   Filter       0.50      0.51      0.50       113\n    Compute Derived Value       0.72      0.72      0.72       165\n            Find Extremum       0.64      0.79      0.71       150\n                     Sort       0.76      0.85      0.80       116\n          Determine Range       0.55      0.53      0.54       110\nCharacterize Distribution       0.92      0.72      0.81       142\n           Find Anomalies       0.89      0.68      0.77       169\n                  Cluster       0.87      0.90      0.89       216\n                Correlate       0.68      0.75      0.71        95\n\n                micro avg       0.73      0.73      0.73      1391\n                macro avg       0.72      0.71      0.71      1391\n             weighted avg       0.74      0.73      0.73      1391\n\nConfusion Matrix...\n[[ 76   2  12  21   3   1   0   0   0   0]\n [ 12  58   4  18   9   6   0   5   1   0]\n [ 20   3 118  10   3   5   1   0   3   2]\n [  0   1  28 119   2   0   0   0   0   0]\n [  2   1   0   0  99   1   1   0  12   0]\n [  5  15   0  14   7  58   5   1   5   0]\n [  2   5   1   2   2   6 102   0   3  19]\n [  1  21   1   1   2  21   1 115   3   3]\n [  0   3   1   0   4   0   1   2 195  10]\n [  1   8   0   0   0   7   0   6   2  71]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.417, Train Acc: 46.88%, Val Loss:  3.276, Val Acc: 40.19%, Time: 17.06s *\nEpoch: 2\nIter: 200, Train Loss:  1.648, Train Acc: 76.56%, Val Loss:  2.067, Val Acc: 67.26%, Time: 1.55s *\nIter: 300, Train Loss:  1.737, Train Acc: 71.88%, Val Loss:  1.829, Val Acc: 71.17%, Time: 17.41s *\nEpoch: 3\nIter: 400, Train Loss: 0.7389, Train Acc: 87.50%, Val Loss:   1.92, Val Acc: 72.64%, Time: 1.72s *\nIter: 500, Train Loss: 0.4517, Train Acc: 95.31%, Val Loss:  1.714, Val Acc: 75.66%, Time: 17.71s *\nEpoch: 4\nIter: 600, Train Loss: 0.9471, Train Acc: 89.06%, Val Loss:  2.013, Val Acc: 72.49%, Time: 1.86s \nIter: 700, Train Loss: 0.3346, Train Acc: 93.75%, Val Loss:  1.718, Val Acc: 75.59%, Time: 17.81s \nEpoch: 5\nIter: 800, Train Loss: 0.6124, Train Acc: 90.62%, Val Loss:  1.764, Val Acc: 76.55%, Time: 2.00s *\nIter: 900, Train Loss: 0.1527, Train Acc: 100.00%, Val Loss:  1.989, Val Acc: 76.25%, Time: 17.90s \nEpoch: 6\nIter: 1000, Train Loss: 0.3659, Train Acc: 95.31%, Val Loss:  1.866, Val Acc: 75.44%, Time: 2.11s \nIter: 1100, Train Loss: 0.1408, Train Acc: 98.44%, Val Loss:  2.044, Val Acc: 75.44%, Time: 17.98s \nEpoch: 7\nIter: 1200, Train Loss: 0.1112, Train Acc: 100.00%, Val Loss:  1.993, Val Acc: 76.47%, Time: 2.32s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.66      0.97      0.79       128\n                   Filter       0.73      0.67      0.70       169\n    Compute Derived Value       0.86      0.60      0.70       166\n            Find Extremum       0.68      0.90      0.78       172\n                     Sort       0.87      0.88      0.88       113\n          Determine Range       0.88      0.64      0.74       187\nCharacterize Distribution       0.91      0.73      0.81        92\n           Find Anomalies       0.65      0.70      0.67       109\n                  Cluster       0.89      0.81      0.85       136\n                Correlate       0.66      0.85      0.74        84\n\n                micro avg       0.76      0.76      0.76      1356\n                macro avg       0.78      0.77      0.77      1356\n             weighted avg       0.78      0.76      0.76      1356\n\nConfusion Matrix...\n[[124   3   1   0   0   0   0   0   0   0]\n [ 13 113   2   5   0   2   2  26   0   6]\n [ 34   0  99  30   0   0   0   0   0   3]\n [  3   3   0 155  11   0   0   0   0   0]\n [  0   1   2   4 100   0   1   2   3   0]\n [  9  24   8  18   1 120   1   1   2   3]\n [  2   1   0   0   0   3  67   2   3  14]\n [  0   9   1  15   0   0   0  76   2   6]\n [  2   1   1   0   2  11   1   4 110   4]\n [  0   0   1   0   1   0   2   6   3  71]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.088, Train Acc: 53.12%, Val Loss:  3.395, Val Acc: 45.16%, Time: 17.17s *\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-11-09febbc132b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_acc_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-3-39a028c39de0>\u001b[0m in \u001b[0;36mtrain_split_data\u001b[0;34m(model, train_data, test_data, split_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-6-e2a195a368b4>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, x_val, y_val, categories)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \u8fd0\u884c\u4f18\u5316\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"]}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nsplit_info = {\n    # \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "expert\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.775, Train Acc: 65.62%, Val Loss:  3.457, Val Acc: 42.28%, Time: 16.90s *\nIter: 200, Train Loss:  1.661, Train Acc: 80.00%, Val Loss:  2.836, Val Acc: 54.66%, Time: 32.69s *\nEpoch: 2\nIter: 300, Train Loss:  1.506, Train Acc: 79.69%, Val Loss:  2.706, Val Acc: 61.74%, Time: 15.86s *\nIter: 400, Train Loss:  1.065, Train Acc: 85.45%, Val Loss:  2.616, Val Acc: 62.94%, Time: 31.60s *\nEpoch: 3\nIter: 500, Train Loss:  0.544, Train Acc: 93.75%, Val Loss:  2.638, Val Acc: 62.06%, Time: 15.81s \nIter: 600, Train Loss:  1.217, Train Acc: 89.09%, Val Loss:   2.65, Val Acc: 63.75%, Time: 31.55s *\nEpoch: 4\nIter: 700, Train Loss: 0.6386, Train Acc: 92.19%, Val Loss:  2.684, Val Acc: 65.19%, Time: 15.65s *\nIter: 800, Train Loss: 0.5776, Train Acc: 90.91%, Val Loss:  2.863, Val Acc: 64.71%, Time: 31.40s \nEpoch: 5\nIter: 900, Train Loss: 0.9272, Train Acc: 92.19%, Val Loss:  2.832, Val Acc: 64.15%, Time: 15.81s \nIter: 1000, Train Loss: 0.2312, Train Acc: 96.36%, Val Loss:  3.073, Val Acc: 62.22%, Time: 31.41s \nEpoch: 6\nIter: 1100, Train Loss: 0.1929, Train Acc: 96.88%, Val Loss:  3.331, Val Acc: 61.98%, Time: 15.64s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.61      0.54      0.57       110\n                   Filter       0.70      0.47      0.56       151\n    Compute Derived Value       0.55      0.66      0.60       119\n            Find Extremum       0.45      0.77      0.57       123\n                     Sort       0.79      0.89      0.84       117\n          Determine Range       0.64      0.64      0.64       143\nCharacterize Distribution       0.83      0.50      0.63       119\n           Find Anomalies       0.70      0.57      0.63       124\n                  Cluster       0.67      0.48      0.56       120\n                Correlate       0.54      0.72      0.62       118\n\n                micro avg       0.62      0.62      0.62      1244\n                macro avg       0.65      0.62      0.62      1244\n             weighted avg       0.65      0.62      0.62      1244\n\nConfusion Matrix...\n[[ 59   1  18  17   1   0   1   1   1  11]\n [ 12  71  18  11   3  22   1  12   0   1]\n [  6   2  78  22   0   1   0   1   0   9]\n [  1   0  12  95   2   0   0   7   5   1]\n [  0   1   0  10 104   1   0   0   1   0]\n [  2   1  11  21   6  92   2   2   1   5]\n [  2   1   5  14   5  11  60   3  15   3]\n [  2  15   0   5   1   3   0  71   4  23]\n [  8   6   1   0   2  14   8   4  58  19]\n [  4   3   0  17   8   0   0   0   1  85]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:   2.96, Train Acc: 45.31%, Val Loss:  3.199, Val Acc: 48.58%, Time: 16.70s *\nIter: 200, Train Loss:  1.232, Train Acc: 77.42%, Val Loss:  1.986, Val Acc: 72.00%, Time: 32.47s *\nEpoch: 2\nIter: 300, Train Loss:  1.304, Train Acc: 82.81%, Val Loss:  1.871, Val Acc: 72.24%, Time: 15.89s *\nIter: 400, Train Loss:  1.098, Train Acc: 77.42%, Val Loss:  1.643, Val Acc: 76.89%, Time: 31.56s *\nEpoch: 3\nIter: 500, Train Loss: 0.7606, Train Acc: 89.06%, Val Loss:  1.449, Val Acc: 79.42%, Time: 15.75s *\nIter: 600, Train Loss: 0.5687, Train Acc: 93.55%, Val Loss:  1.443, Val Acc: 78.63%, Time: 31.54s \nEpoch: 4\nIter: 700, Train Loss: 0.6093, Train Acc: 93.75%, Val Loss:   1.54, Val Acc: 77.68%, Time: 15.68s \nIter: 800, Train Loss: 0.2636, Train Acc: 100.00%, Val Loss:  1.744, Val Acc: 77.21%, Time: 31.41s \nEpoch: 5\nIter: 900, Train Loss: 0.3035, Train Acc: 95.31%, Val Loss:   1.71, Val Acc: 76.89%, Time: 15.87s \n901:500  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.52      0.89      0.66       130\n                   Filter       0.63      0.80      0.71       128\n    Compute Derived Value       0.88      0.54      0.67       119\n            Find Extremum       0.88      0.92      0.90       122\n                     Sort       0.93      0.79      0.85       117\n          Determine Range       0.79      0.52      0.63       123\nCharacterize Distribution       0.83      0.76      0.79       142\n           Find Anomalies       0.92      0.69      0.79       137\n                  Cluster       0.82      0.83      0.82       121\n                Correlate       0.82      0.96      0.88       129\n\n                micro avg       0.77      0.77      0.77      1268\n                macro avg       0.80      0.77      0.77      1268\n             weighted avg       0.80      0.77      0.77      1268\n\nConfusion Matrix...\n[[116   1   0  12   0   0   0   0   1   0]\n [ 23 102   0   0   0   3   0   0   0   0]\n [ 42   3  64   2   1   3   0   1   2   1]\n [  3   1   0 112   0   0   1   3   0   2]\n [  2  10   0   1  92   7   0   0   5   0]\n [ 12  34   2   0   2  64   5   0   4   0]\n [  8  10   0   0   1   2 108   1  10   2]\n [  4   0   7   0   0   1  12  94   0  19]\n [  6   0   0   0   3   1   4   3 100   4]\n [  5   0   0   0   0   0   0   0   0 124]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:   3.11, Train Acc: 48.44%, Val Loss:  3.318, Val Acc: 47.96%, Time: 16.67s *\nIter: 200, Train Loss:  2.061, Train Acc: 62.50%, Val Loss:  2.511, Val Acc: 59.09%, Time: 32.23s *\nEpoch: 2\nIter: 300, Train Loss:  1.545, Train Acc: 73.44%, Val Loss:  2.171, Val Acc: 65.31%, Time: 15.22s *\nIter: 400, Train Loss:  1.058, Train Acc: 82.81%, Val Loss:   2.31, Val Acc: 66.88%, Time: 30.83s *\nEpoch: 3\nIter: 500, Train Loss: 0.6374, Train Acc: 92.19%, Val Loss:  2.228, Val Acc: 69.85%, Time: 14.68s *\nIter: 600, Train Loss: 0.8223, Train Acc: 90.62%, Val Loss:  2.247, Val Acc: 71.71%, Time: 30.30s *\nEpoch: 4\nIter: 700, Train Loss: 0.2771, Train Acc: 95.31%, Val Loss:  2.554, Val Acc: 68.74%, Time: 14.33s \nIter: 800, Train Loss: 0.8159, Train Acc: 85.94%, Val Loss:  2.739, Val Acc: 65.31%, Time: 30.07s \nEpoch: 5\nIter: 900, Train Loss: 0.3938, Train Acc: 95.31%, Val Loss:  2.564, Val Acc: 66.51%, Time: 13.87s \nIter: 1000, Train Loss: 0.2399, Train Acc: 95.31%, Val Loss:  2.658, Val Acc: 68.74%, Time: 29.54s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.48      0.50      0.49       116\n                   Filter       0.65      0.83      0.73       114\n    Compute Derived Value       0.67      0.56      0.61       124\n            Find Extremum       0.92      0.77      0.84       112\n                     Sort       0.93      0.84      0.88       117\n          Determine Range       0.54      0.42      0.47        93\nCharacterize Distribution       0.88      0.92      0.90        91\n           Find Anomalies       0.84      0.52      0.65       101\n                  Cluster       0.63      0.78      0.70        99\n                Correlate       0.55      0.77      0.65       111\n\n                micro avg       0.69      0.69      0.69      1078\n                macro avg       0.71      0.69      0.69      1078\n             weighted avg       0.71      0.69      0.69      1078\n\nConfusion Matrix...\n[[58 15 28  1  0  0  1  1  4  8]\n [ 9 95  2  5  0  1  1  0  1  0]\n [15  8 70  0  0  2  0  3  0 26]\n [ 7  9  0 86  1  4  0  0  1  4]\n [ 0  0  1  0 98  3  1  0 13  1]\n [27 10  2  0  0 39  7  0  3  5]\n [ 1  0  0  0  0  3 84  0  2  1]\n [ 0  8  0  0  1 17  0 53  7 15]\n [ 3  0  0  0  5  1  1  3 77  9]\n [ 1  2  2  1  0  2  0  3 14 86]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.119, Train Acc: 51.56%, Val Loss:  3.308, Val Acc: 43.47%, Time: 17.01s *\nEpoch: 2\nIter: 200, Train Loss:  1.786, Train Acc: 75.00%, Val Loss:   2.02, Val Acc: 69.76%, Time: 1.53s *\nIter: 300, Train Loss:   1.36, Train Acc: 81.25%, Val Loss:  1.636, Val Acc: 74.24%, Time: 17.41s *\nEpoch: 3\nIter: 400, Train Loss: 0.8187, Train Acc: 87.50%, Val Loss:  1.676, Val Acc: 75.46%, Time: 1.67s *\nIter: 500, Train Loss: 0.7471, Train Acc: 90.62%, Val Loss:   1.78, Val Acc: 72.95%, Time: 17.49s \nEpoch: 4\nIter: 600, Train Loss: 0.7725, Train Acc: 89.06%, Val Loss:   1.67, Val Acc: 74.92%, Time: 1.77s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 700, Train Loss: 0.3203, Train Acc: 95.31%, Val Loss:  1.699, Val Acc: 75.23%, Time: 17.44s \nEpoch: 5\nIter: 800, Train Loss: 0.3017, Train Acc: 96.88%, Val Loss:  1.653, Val Acc: 77.58%, Time: 1.96s *\nIter: 900, Train Loss: 0.1636, Train Acc: 100.00%, Val Loss:   1.75, Val Acc: 76.44%, Time: 17.71s \nEpoch: 6\nIter: 1000, Train Loss: 0.2408, Train Acc: 95.31%, Val Loss:   1.76, Val Acc: 77.13%, Time: 2.06s \nIter: 1100, Train Loss: 0.2898, Train Acc: 95.31%, Val Loss:  1.736, Val Acc: 75.53%, Time: 17.93s \nEpoch: 7\nIter: 1200, Train Loss: 0.1073, Train Acc: 100.00%, Val Loss:   1.65, Val Acc: 78.27%, Time: 2.24s *\nIter: 1300, Train Loss: 0.3604, Train Acc: 92.19%, Val Loss:  1.804, Val Acc: 78.65%, Time: 18.06s *\nEpoch: 8\nIter: 1400, Train Loss: 0.05368, Train Acc: 100.00%, Val Loss:  2.015, Val Acc: 75.30%, Time: 2.36s \nIter: 1500, Train Loss: 0.1219, Train Acc: 98.44%, Val Loss:  1.983, Val Acc: 76.82%, Time: 18.11s \nEpoch: 9\nIter: 1600, Train Loss: 0.0948, Train Acc: 98.44%, Val Loss:  1.873, Val Acc: 78.72%, Time: 2.51s *\nIter: 1700, Train Loss: 0.0648, Train Acc: 100.00%, Val Loss:  2.159, Val Acc: 74.70%, Time: 18.42s \nEpoch: 10\nIter: 1800, Train Loss: 0.01181, Train Acc: 100.00%, Val Loss:  2.137, Val Acc: 74.32%, Time: 2.63s \nIter: 1900, Train Loss: 0.1036, Train Acc: 98.44%, Val Loss:  2.137, Val Acc: 75.84%, Time: 18.34s \nEpoch: 11\nIter: 2000, Train Loss: 0.2895, Train Acc: 96.88%, Val Loss:  2.182, Val Acc: 76.60%, Time: 2.87s \n2001:1600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.87      0.63      0.73       141\n                   Filter       0.69      0.81      0.75       132\n    Compute Derived Value       0.52      0.71      0.61       133\n            Find Extremum       0.89      0.80      0.85       126\n                     Sort       0.93      0.69      0.79       118\n          Determine Range       0.76      0.84      0.80       115\nCharacterize Distribution       0.77      0.94      0.85       122\n           Find Anomalies       0.94      0.54      0.69       144\n                  Cluster       0.78      0.79      0.78       156\n                Correlate       0.76      0.92      0.83       129\n\n                micro avg       0.76      0.76      0.76      1316\n                macro avg       0.79      0.77      0.77      1316\n             weighted avg       0.79      0.76      0.76      1316\n\nConfusion Matrix...\n[[ 89   8  34   1   0   3   3   0   0   3]\n [  3 107   4   1   1  12   1   1   2   0]\n [  5   9  95   0   0   7  14   0   0   3]\n [  2  10   9 101   2   0   2   0   0   0]\n [  2   6   3   0  81   2   3   0  21   0]\n [  1   3   2   6   0  97   2   0   3   1]\n [  0   0   2   0   1   0 115   2   2   0]\n [  0   8  26   3   0   2   1  78   6  20]\n [  0   2   1   1   2   5   9   1 124  11]\n [  0   2   5   0   0   0   0   1   2 119]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.955, Train Acc: 50.00%, Val Loss:  3.219, Val Acc: 45.38%, Time: 16.73s *\nIter: 200, Train Loss:  1.493, Train Acc: 79.69%, Val Loss:  2.643, Val Acc: 59.08%, Time: 32.44s *\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-03e637d93fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_acc_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-3-39a028c39de0>\u001b[0m in \u001b[0;36mtrain_split_data\u001b[0;34m(model, train_data, test_data, split_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-6-e2a195a368b4>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, x_val, y_val, categories)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \u8fd0\u884c\u4f18\u5316\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"]}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "random\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.743, Train Acc: 54.69%, Val Loss:  3.002, Val Acc: 52.49%, Time: 18.40s *\nEpoch: 2\nIter: 200, Train Loss:  2.108, Train Acc: 65.62%, Val Loss:  1.883, Val Acc: 70.37%, Time: 1.72s *\nIter: 300, Train Loss:  1.015, Train Acc: 87.50%, Val Loss:  1.525, Val Acc: 75.57%, Time: 17.80s *\nEpoch: 3\nIter: 400, Train Loss: 0.9277, Train Acc: 85.94%, Val Loss:  1.245, Val Acc: 82.26%, Time: 2.04s *\nIter: 500, Train Loss: 0.6279, Train Acc: 90.62%, Val Loss:  1.102, Val Acc: 83.97%, Time: 17.91s *\nEpoch: 4\nIter: 600, Train Loss: 0.7389, Train Acc: 92.19%, Val Loss:  1.029, Val Acc: 84.47%, Time: 2.30s *\nIter: 700, Train Loss: 0.2953, Train Acc: 95.31%, Val Loss: 0.8694, Val Acc: 87.25%, Time: 18.37s *\nEpoch: 5\nIter: 800, Train Loss: 0.2237, Train Acc: 96.88%, Val Loss:  0.823, Val Acc: 87.68%, Time: 2.62s *\nIter: 900, Train Loss: 0.2388, Train Acc: 96.88%, Val Loss: 0.8893, Val Acc: 85.75%, Time: 18.65s \nEpoch: 6\nIter: 1000, Train Loss: 0.2025, Train Acc: 96.88%, Val Loss: 0.7903, Val Acc: 88.46%, Time: 2.90s *\nIter: 1100, Train Loss: 0.3846, Train Acc: 96.88%, Val Loss: 0.8215, Val Acc: 87.25%, Time: 19.00s \nEpoch: 7\nIter: 1200, Train Loss: 0.2444, Train Acc: 95.31%, Val Loss:  0.709, Val Acc: 89.25%, Time: 3.19s *\nIter: 1300, Train Loss: 0.2555, Train Acc: 95.31%, Val Loss: 0.7592, Val Acc: 88.18%, Time: 19.22s \nEpoch: 8\nIter: 1400, Train Loss: 0.09821, Train Acc: 98.44%, Val Loss: 0.7507, Val Acc: 89.53%, Time: 3.50s *\nIter: 1500, Train Loss: 0.0509, Train Acc: 100.00%, Val Loss: 0.7769, Val Acc: 89.17%, Time: 19.55s \nEpoch: 9\nIter: 1600, Train Loss: 0.1576, Train Acc: 98.44%, Val Loss:   0.78, Val Acc: 88.68%, Time: 3.80s \nIter: 1700, Train Loss: 0.1981, Train Acc: 96.88%, Val Loss: 0.7226, Val Acc: 89.74%, Time: 19.76s *\nEpoch: 10\nIter: 1800, Train Loss: 0.2195, Train Acc: 96.88%, Val Loss: 0.7705, Val Acc: 90.03%, Time: 4.06s *\nIter: 1900, Train Loss: 0.4074, Train Acc: 96.88%, Val Loss: 0.7219, Val Acc: 90.46%, Time: 20.13s *\nEpoch: 11\nIter: 2000, Train Loss: 0.03114, Train Acc: 100.00%, Val Loss: 0.7271, Val Acc: 90.46%, Time: 4.34s *\nIter: 2100, Train Loss: 0.1308, Train Acc: 98.44%, Val Loss: 0.8004, Val Acc: 89.60%, Time: 20.37s \nEpoch: 12\nIter: 2200, Train Loss: 0.009929, Train Acc: 100.00%, Val Loss: 0.7513, Val Acc: 89.96%, Time: 4.73s \nIter: 2300, Train Loss: 0.09077, Train Acc: 98.44%, Val Loss: 0.8035, Val Acc: 88.82%, Time: 20.82s \nEpoch: 13\nIter: 2400, Train Loss: 0.04189, Train Acc: 100.00%, Val Loss: 0.8181, Val Acc: 89.60%, Time: 4.90s \n2401:2000  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.96      0.87      0.91       139\n                   Filter       0.82      0.88      0.85       154\n    Compute Derived Value       0.82      0.91      0.86       162\n            Find Extremum       0.94      0.93      0.93       163\n                     Sort       0.92      0.90      0.91       123\n          Determine Range       0.92      0.78      0.84       140\nCharacterize Distribution       0.96      0.87      0.92       118\n           Find Anomalies       0.84      0.96      0.90       135\n                  Cluster       0.86      0.98      0.92       100\n                Correlate       0.96      0.89      0.92       170\n\n                micro avg       0.89      0.89      0.89      1404\n                macro avg       0.90      0.90      0.90      1404\n             weighted avg       0.90      0.89      0.89      1404\n\nConfusion Matrix...\n[[121   2  12   1   0   1   1   1   0   0]\n [  1 136   1   0   3   1   0   9   0   3]\n [  0   7 147   2   0   3   0   2   1   0]\n [  1   2   3 151   3   1   0   1   0   1]\n [  0   4   0   2 111   0   0   0   6   0]\n [  1  11   8   3   3 109   2   1   2   0]\n [  2   1   0   1   0   3 103   1   4   3]\n [  0   3   2   0   0   0   1 129   0   0]\n [  0   0   0   0   0   0   0   2  98   0]\n [  0   0   7   1   1   0   0   7   3 151]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.082, Train Acc: 51.56%, Val Loss:  3.056, Val Acc: 50.78%, Time: 16.97s *\nEpoch: 2\nIter: 200, Train Loss:   1.43, Train Acc: 81.25%, Val Loss:  1.847, Val Acc: 70.16%, Time: 1.70s *\nIter: 300, Train Loss:  1.068, Train Acc: 87.50%, Val Loss:  1.377, Val Acc: 79.63%, Time: 17.65s *\nEpoch: 3\nIter: 400, Train Loss: 0.6192, Train Acc: 90.62%, Val Loss:  1.129, Val Acc: 82.69%, Time: 2.00s *\nIter: 500, Train Loss: 0.7608, Train Acc: 85.94%, Val Loss: 0.9619, Val Acc: 85.97%, Time: 18.03s *\nEpoch: 4\nIter: 600, Train Loss: 0.5574, Train Acc: 90.62%, Val Loss: 0.8868, Val Acc: 87.04%, Time: 2.32s *\nIter: 700, Train Loss: 0.6058, Train Acc: 89.06%, Val Loss:  0.824, Val Acc: 87.68%, Time: 18.37s *\nEpoch: 5\nIter: 800, Train Loss: 0.3337, Train Acc: 95.31%, Val Loss: 0.8639, Val Acc: 86.75%, Time: 2.60s \nIter: 900, Train Loss:  0.212, Train Acc: 98.44%, Val Loss: 0.7557, Val Acc: 88.75%, Time: 18.65s *\nEpoch: 6\nIter: 1000, Train Loss: 0.3376, Train Acc: 96.88%, Val Loss: 0.7548, Val Acc: 88.11%, Time: 2.91s \nIter: 1100, Train Loss:  0.254, Train Acc: 95.31%, Val Loss: 0.7463, Val Acc: 89.25%, Time: 18.96s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2552, Train Acc: 98.44%, Val Loss: 0.7318, Val Acc: 88.96%, Time: 3.19s \nIter: 1300, Train Loss: 0.04702, Train Acc: 100.00%, Val Loss:  0.697, Val Acc: 89.89%, Time: 19.26s *\nEpoch: 8\nIter: 1400, Train Loss: 0.07616, Train Acc: 98.44%, Val Loss:  0.707, Val Acc: 90.60%, Time: 3.49s *\nIter: 1500, Train Loss: 0.04269, Train Acc: 100.00%, Val Loss:  0.702, Val Acc: 89.81%, Time: 19.55s \nEpoch: 9\nIter: 1600, Train Loss: 0.0238, Train Acc: 100.00%, Val Loss: 0.7179, Val Acc: 89.74%, Time: 3.81s \nIter: 1700, Train Loss: 0.03715, Train Acc: 100.00%, Val Loss:  0.749, Val Acc: 89.46%, Time: 19.90s \nEpoch: 10\nIter: 1800, Train Loss: 0.03212, Train Acc: 100.00%, Val Loss:   0.71, Val Acc: 90.46%, Time: 4.06s \n1801:1400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.94      0.92      0.93       138\n                   Filter       0.86      0.84      0.85       140\n    Compute Derived Value       0.83      0.94      0.88       146\n            Find Extremum       0.92      0.96      0.94       175\n                     Sort       0.97      0.92      0.94       108\n          Determine Range       0.85      0.94      0.89       129\nCharacterize Distribution       0.97      0.83      0.89       134\n           Find Anomalies       0.93      0.84      0.88       157\n                  Cluster       0.91      0.93      0.92       132\n                Correlate       0.89      0.90      0.89       145\n\n                micro avg       0.90      0.90      0.90      1404\n                macro avg       0.91      0.90      0.90      1404\n             weighted avg       0.91      0.90      0.90      1404\n\nConfusion Matrix...\n[[127   1   7   1   0   1   1   0   0   0]\n [  3 118   3   4   0   2   0   8   2   0]\n [  1   1 137   2   0   2   0   0   0   3]\n [  1   1   2 168   0   3   0   0   0   0]\n [  1   0   1   2  99   2   1   0   2   0]\n [  0   4   1   1   1 121   1   0   0   0]\n [  0   1   7   2   0   6 111   0   2   5]\n [  0  10   5   2   1   2   1 132   1   3]\n [  0   1   0   0   1   0   0   1 123   6]\n [  2   1   2   0   0   3   0   1   5 131]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.264, Train Acc: 48.44%, Val Loss:  3.034, Val Acc: 50.85%, Time: 17.13s *\nEpoch: 2\nIter: 200, Train Loss:  1.566, Train Acc: 76.56%, Val Loss:  2.088, Val Acc: 66.45%, Time: 1.83s *\nIter: 300, Train Loss:  1.129, Train Acc: 81.25%, Val Loss:  1.589, Val Acc: 75.28%, Time: 17.92s *\nEpoch: 3\nIter: 400, Train Loss: 0.9892, Train Acc: 82.81%, Val Loss:   1.38, Val Acc: 79.06%, Time: 2.04s *\nIter: 500, Train Loss: 0.5488, Train Acc: 93.75%, Val Loss:  1.246, Val Acc: 80.91%, Time: 18.16s *\nEpoch: 4\nIter: 600, Train Loss: 0.7951, Train Acc: 82.81%, Val Loss:  1.151, Val Acc: 82.91%, Time: 2.32s *\nIter: 700, Train Loss: 0.6283, Train Acc: 92.19%, Val Loss:  1.117, Val Acc: 83.40%, Time: 18.29s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 5\nIter: 800, Train Loss: 0.3031, Train Acc: 95.31%, Val Loss:  1.025, Val Acc: 85.54%, Time: 2.53s *\nIter: 900, Train Loss: 0.6932, Train Acc: 90.62%, Val Loss:  1.061, Val Acc: 86.68%, Time: 18.44s *\nEpoch: 6\nIter: 1000, Train Loss: 0.3176, Train Acc: 95.31%, Val Loss:   1.09, Val Acc: 85.26%, Time: 2.92s \nIter: 1100, Train Loss: 0.1158, Train Acc: 100.00%, Val Loss:  1.009, Val Acc: 87.54%, Time: 18.91s *\nEpoch: 7\nIter: 1200, Train Loss: 0.6038, Train Acc: 95.31%, Val Loss: 0.9787, Val Acc: 88.03%, Time: 3.16s *\nIter: 1300, Train Loss: 0.09478, Train Acc: 100.00%, Val Loss: 0.9058, Val Acc: 88.11%, Time: 19.27s *\nEpoch: 8\nIter: 1400, Train Loss: 0.2544, Train Acc: 96.88%, Val Loss:  1.006, Val Acc: 87.96%, Time: 3.43s \nIter: 1500, Train Loss: 0.09064, Train Acc: 98.44%, Val Loss:   1.01, Val Acc: 87.18%, Time: 19.49s \nEpoch: 9\nIter: 1600, Train Loss: 0.06282, Train Acc: 100.00%, Val Loss:  0.987, Val Acc: 88.32%, Time: 3.76s *\nIter: 1700, Train Loss: 0.03998, Train Acc: 100.00%, Val Loss:  1.062, Val Acc: 87.82%, Time: 19.82s \nEpoch: 10\nIter: 1800, Train Loss: 0.07881, Train Acc: 98.44%, Val Loss:  1.112, Val Acc: 87.18%, Time: 4.02s \nIter: 1900, Train Loss: 0.1255, Train Acc: 96.88%, Val Loss:  1.045, Val Acc: 86.61%, Time: 19.98s \nEpoch: 11\nIter: 2000, Train Loss: 0.07568, Train Acc: 100.00%, Val Loss:  1.054, Val Acc: 87.75%, Time: 4.34s \n2001:1600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.91      0.89      0.90       141\n                   Filter       0.74      0.91      0.81       133\n    Compute Derived Value       0.81      0.77      0.79       145\n            Find Extremum       0.90      0.88      0.89       160\n                     Sort       0.90      0.91      0.90       123\n          Determine Range       0.88      0.83      0.85       134\nCharacterize Distribution       0.94      0.85      0.89       139\n           Find Anomalies       0.93      0.90      0.91       154\n                  Cluster       0.91      0.88      0.89       119\n                Correlate       0.89      0.94      0.92       156\n\n                micro avg       0.88      0.88      0.88      1404\n                macro avg       0.88      0.88      0.88      1404\n             weighted avg       0.88      0.88      0.88      1404\n\nConfusion Matrix...\n[[126   6   4   4   0   1   0   0   0   0]\n [  1 121   1   1   1   3   0   4   1   0]\n [  6  10 112   2   1   5   1   2   1   5]\n [  1   5   5 140   8   0   0   0   0   1]\n [  2   4   0   1 112   0   0   0   3   1]\n [  1   7   6   4   1 111   2   1   1   0]\n [  1   1   8   1   1   3 118   1   2   3]\n [  0   8   0   1   0   1   3 138   1   2]\n [  0   2   0   1   1   2   1   1 105   6]\n [  1   0   3   0   0   0   1   2   2 147]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.033, Train Acc: 54.69%, Val Loss:  2.973, Val Acc: 51.57%, Time: 17.10s *\nEpoch: 2\nIter: 200, Train Loss:  1.826, Train Acc: 76.56%, Val Loss:  2.036, Val Acc: 67.59%, Time: 1.74s *\nIter: 300, Train Loss:  1.285, Train Acc: 82.81%, Val Loss:  1.464, Val Acc: 78.06%, Time: 17.73s *\nEpoch: 3\nIter: 400, Train Loss:  1.037, Train Acc: 84.38%, Val Loss:  1.205, Val Acc: 80.91%, Time: 2.01s *\nIter: 500, Train Loss:  1.039, Train Acc: 82.81%, Val Loss:  1.041, Val Acc: 83.62%, Time: 18.03s *\nEpoch: 4\nIter: 600, Train Loss: 0.6104, Train Acc: 87.50%, Val Loss: 0.9078, Val Acc: 85.90%, Time: 2.31s *\nIter: 700, Train Loss: 0.4107, Train Acc: 95.31%, Val Loss: 0.9481, Val Acc: 85.83%, Time: 18.22s \nEpoch: 5\nIter: 800, Train Loss: 0.6492, Train Acc: 90.62%, Val Loss: 0.8604, Val Acc: 86.61%, Time: 2.64s *\nIter: 900, Train Loss: 0.3531, Train Acc: 92.19%, Val Loss: 0.7082, Val Acc: 89.53%, Time: 18.52s *\nEpoch: 6\nIter: 1000, Train Loss: 0.1739, Train Acc: 98.44%, Val Loss: 0.7548, Val Acc: 89.46%, Time: 2.92s \nIter: 1100, Train Loss: 0.2065, Train Acc: 96.88%, Val Loss: 0.7325, Val Acc: 89.67%, Time: 19.02s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2962, Train Acc: 95.31%, Val Loss: 0.7225, Val Acc: 89.32%, Time: 3.21s \nIter: 1300, Train Loss: 0.05928, Train Acc: 100.00%, Val Loss: 0.6866, Val Acc: 89.32%, Time: 19.17s \nEpoch: 8\nIter: 1400, Train Loss: 0.1292, Train Acc: 98.44%, Val Loss: 0.6285, Val Acc: 90.88%, Time: 3.48s *\nIter: 1500, Train Loss: 0.2937, Train Acc: 93.75%, Val Loss: 0.7304, Val Acc: 90.17%, Time: 19.45s \nEpoch: 9\nIter: 1600, Train Loss: 0.1637, Train Acc: 96.88%, Val Loss: 0.6958, Val Acc: 90.46%, Time: 3.76s \nIter: 1700, Train Loss: 0.01178, Train Acc: 100.00%, Val Loss: 0.7076, Val Acc: 90.31%, Time: 19.68s \nEpoch: 10\nIter: 1800, Train Loss: 0.04787, Train Acc: 100.00%, Val Loss:  0.659, Val Acc: 91.03%, Time: 3.99s *\nIter: 1900, Train Loss: 0.1927, Train Acc: 98.44%, Val Loss: 0.6438, Val Acc: 91.03%, Time: 20.08s *\nEpoch: 11\nIter: 2000, Train Loss: 0.01296, Train Acc: 100.00%, Val Loss:  0.732, Val Acc: 91.24%, Time: 4.38s *\nIter: 2100, Train Loss: 0.02281, Train Acc: 100.00%, Val Loss: 0.6775, Val Acc: 91.17%, Time: 20.38s \nEpoch: 12\nIter: 2200, Train Loss: 0.1867, Train Acc: 95.31%, Val Loss: 0.6979, Val Acc: 90.88%, Time: 4.63s \nIter: 2300, Train Loss: 0.0793, Train Acc: 98.44%, Val Loss: 0.6635, Val Acc: 91.10%, Time: 20.63s \nEpoch: 13\nIter: 2400, Train Loss: 0.03256, Train Acc: 100.00%, Val Loss: 0.7194, Val Acc: 91.10%, Time: 4.90s \n2401:2000  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.98      0.91      0.94       145\n                   Filter       0.91      0.86      0.89       145\n    Compute Derived Value       0.83      0.95      0.89       161\n            Find Extremum       0.99      0.87      0.93       180\n                     Sort       0.91      0.94      0.93       124\n          Determine Range       0.82      0.92      0.87       123\nCharacterize Distribution       0.92      0.93      0.93       132\n           Find Anomalies       0.92      0.87      0.89       114\n                  Cluster       0.94      0.93      0.93       136\n                Correlate       0.92      0.94      0.93       144\n\n                micro avg       0.91      0.91      0.91      1404\n                macro avg       0.91      0.91      0.91      1404\n             weighted avg       0.92      0.91      0.91      1404\n\nConfusion Matrix...\n[[132   1   4   0   1   4   1   0   0   2]\n [  2 125   3   1   1   8   1   3   1   0]\n [  0   2 153   1   1   2   1   0   0   1]\n [  1   3   9 157   1   3   0   4   1   1]\n [  0   0   2   0 117   2   0   0   3   0]\n [  0   1   3   0   3 113   2   0   1   0]\n [  0   1   2   0   1   1 123   0   0   4]\n [  0   4   4   0   0   1   1  99   1   4]\n [  0   0   0   0   3   4   2   1 126   0]\n [  0   0   4   0   0   0   2   1   1 136]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.307, Train Acc: 48.44%, Val Loss:  3.194, Val Acc: 47.86%, Time: 17.00s *\nEpoch: 2\nIter: 200, Train Loss:  2.057, Train Acc: 65.62%, Val Loss:  2.037, Val Acc: 66.95%, Time: 1.73s *\nIter: 300, Train Loss:  1.632, Train Acc: 76.56%, Val Loss:  1.496, Val Acc: 77.71%, Time: 17.77s *\nEpoch: 3\nIter: 400, Train Loss:  1.098, Train Acc: 82.81%, Val Loss:  1.254, Val Acc: 81.48%, Time: 2.04s *\nIter: 500, Train Loss: 0.6928, Train Acc: 90.62%, Val Loss:   1.06, Val Acc: 83.97%, Time: 17.99s *\nEpoch: 4\nIter: 600, Train Loss: 0.4555, Train Acc: 93.75%, Val Loss: 0.9203, Val Acc: 87.39%, Time: 2.30s *\nIter: 700, Train Loss: 0.4297, Train Acc: 96.88%, Val Loss: 0.9228, Val Acc: 86.61%, Time: 18.33s \nEpoch: 5\nIter: 800, Train Loss: 0.1152, Train Acc: 100.00%, Val Loss: 0.8104, Val Acc: 88.46%, Time: 2.58s *\nIter: 900, Train Loss: 0.2466, Train Acc: 95.31%, Val Loss:  0.843, Val Acc: 87.68%, Time: 18.46s \nEpoch: 6\nIter: 1000, Train Loss: 0.3005, Train Acc: 95.31%, Val Loss: 0.7876, Val Acc: 88.53%, Time: 2.91s *\nIter: 1100, Train Loss: 0.06318, Train Acc: 100.00%, Val Loss: 0.7598, Val Acc: 89.74%, Time: 18.95s *\nEpoch: 7\nIter: 1200, Train Loss: 0.3406, Train Acc: 96.88%, Val Loss: 0.7247, Val Acc: 90.03%, Time: 3.14s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1300, Train Loss: 0.1643, Train Acc: 95.31%, Val Loss: 0.8759, Val Acc: 88.25%, Time: 19.07s \nEpoch: 8\nIter: 1400, Train Loss: 0.06039, Train Acc: 100.00%, Val Loss: 0.8258, Val Acc: 89.17%, Time: 3.54s \nIter: 1500, Train Loss: 0.07989, Train Acc: 100.00%, Val Loss: 0.7071, Val Acc: 90.88%, Time: 19.47s *\nEpoch: 9\nIter: 1600, Train Loss: 0.09829, Train Acc: 98.44%, Val Loss: 0.8009, Val Acc: 88.96%, Time: 3.77s \nIter: 1700, Train Loss: 0.07384, Train Acc: 98.44%, Val Loss:  0.832, Val Acc: 89.32%, Time: 19.69s \nEpoch: 10\nIter: 1800, Train Loss: 0.1256, Train Acc: 98.44%, Val Loss: 0.7834, Val Acc: 90.17%, Time: 4.01s \nIter: 1900, Train Loss: 0.02012, Train Acc: 100.00%, Val Loss: 0.7055, Val Acc: 91.45%, Time: 19.84s *\nEpoch: 11\nIter: 2000, Train Loss: 0.01693, Train Acc: 100.00%, Val Loss: 0.7961, Val Acc: 90.81%, Time: 4.34s \nIter: 2100, Train Loss: 0.01219, Train Acc: 100.00%, Val Loss: 0.7675, Val Acc: 90.53%, Time: 20.18s \nEpoch: 12\nIter: 2200, Train Loss: 0.02657, Train Acc: 100.00%, Val Loss:  0.743, Val Acc: 90.60%, Time: 4.64s \nIter: 2300, Train Loss: 0.07031, Train Acc: 98.44%, Val Loss: 0.7418, Val Acc: 91.03%, Time: 20.52s \n2301:1900  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.84      0.96      0.90       131\n                   Filter       0.85      0.93      0.89       176\n    Compute Derived Value       0.88      0.87      0.87       156\n            Find Extremum       0.93      0.93      0.93       168\n                     Sort       0.96      0.93      0.94       108\n          Determine Range       0.93      0.88      0.91       121\nCharacterize Distribution       0.96      0.89      0.92       122\n           Find Anomalies       0.95      0.86      0.90       141\n                  Cluster       0.91      0.97      0.94       141\n                Correlate       0.96      0.89      0.92       140\n\n                micro avg       0.91      0.91      0.91      1404\n                macro avg       0.92      0.91      0.91      1404\n             weighted avg       0.91      0.91      0.91      1404\n\nConfusion Matrix...\n[[126   3   1   0   0   1   0   0   0   0]\n [  4 163   1   1   0   2   0   2   1   2]\n [  9   3 135   2   1   1   1   0   3   1]\n [  2   3   1 157   2   0   0   1   1   1]\n [  1   1   0   2 100   1   1   0   2   0]\n [  4   3   2   0   1 107   1   1   2   0]\n [  2   1   4   2   0   2 109   1   1   0]\n [  0  13   3   2   0   0   0 121   1   1]\n [  1   1   0   2   0   0   0   0 137   0]\n [  1   1   6   0   0   1   2   2   3 124]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.081, Train Acc: 46.88%, Val Loss:  3.084, Val Acc: 47.90%, Time: 16.93s *\nEpoch: 2\nIter: 200, Train Loss:  2.178, Train Acc: 65.62%, Val Loss:  1.902, Val Acc: 70.14%, Time: 1.75s *\nIter: 300, Train Loss:  1.418, Train Acc: 76.56%, Val Loss:   1.51, Val Acc: 76.12%, Time: 17.54s *\nEpoch: 3\nIter: 400, Train Loss: 0.8848, Train Acc: 87.50%, Val Loss:  1.298, Val Acc: 79.47%, Time: 1.96s *\nIter: 500, Train Loss: 0.7615, Train Acc: 85.94%, Val Loss:  1.095, Val Acc: 82.82%, Time: 17.97s *\nEpoch: 4\nIter: 600, Train Loss:  1.015, Train Acc: 85.94%, Val Loss:  1.001, Val Acc: 84.68%, Time: 2.30s *\nIter: 700, Train Loss:  0.646, Train Acc: 90.62%, Val Loss: 0.9495, Val Acc: 85.53%, Time: 18.12s *\nEpoch: 5\nIter: 800, Train Loss: 0.2221, Train Acc: 95.31%, Val Loss: 0.8514, Val Acc: 88.10%, Time: 2.59s *\nIter: 900, Train Loss: 0.3493, Train Acc: 93.75%, Val Loss: 0.9425, Val Acc: 86.96%, Time: 18.44s \nEpoch: 6\nIter: 1000, Train Loss: 0.2412, Train Acc: 98.44%, Val Loss: 0.8296, Val Acc: 88.03%, Time: 2.91s \nIter: 1100, Train Loss: 0.1488, Train Acc: 98.44%, Val Loss: 0.7365, Val Acc: 89.88%, Time: 18.83s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1849, Train Acc: 98.44%, Val Loss: 0.7896, Val Acc: 89.24%, Time: 3.18s \nIter: 1300, Train Loss: 0.2664, Train Acc: 96.88%, Val Loss: 0.9183, Val Acc: 87.74%, Time: 19.05s \nEpoch: 8\nIter: 1400, Train Loss: 0.0547, Train Acc: 100.00%, Val Loss: 0.7764, Val Acc: 90.24%, Time: 3.46s *\nIter: 1500, Train Loss: 0.04441, Train Acc: 100.00%, Val Loss: 0.7669, Val Acc: 90.24%, Time: 19.15s *\nEpoch: 9\nIter: 1600, Train Loss: 0.08857, Train Acc: 98.44%, Val Loss: 0.7672, Val Acc: 90.38%, Time: 3.76s *\nIter: 1700, Train Loss: 0.1396, Train Acc: 96.88%, Val Loss: 0.8347, Val Acc: 89.67%, Time: 19.62s \nEpoch: 10\nIter: 1800, Train Loss: 0.05846, Train Acc: 100.00%, Val Loss: 0.7996, Val Acc: 90.02%, Time: 4.02s \nIter: 1900, Train Loss: 0.1224, Train Acc: 96.88%, Val Loss: 0.7478, Val Acc: 91.38%, Time: 19.80s *\nEpoch: 11\nIter: 2000, Train Loss: 0.01732, Train Acc: 100.00%, Val Loss: 0.7747, Val Acc: 91.30%, Time: 4.29s \nIter: 2100, Train Loss: 0.01737, Train Acc: 100.00%, Val Loss: 0.8075, Val Acc: 90.95%, Time: 20.14s \nEpoch: 12\nIter: 2200, Train Loss: 0.03176, Train Acc: 100.00%, Val Loss:  0.827, Val Acc: 90.52%, Time: 4.55s \nIter: 2300, Train Loss: 0.01162, Train Acc: 100.00%, Val Loss: 0.6994, Val Acc: 91.09%, Time: 20.40s \n2301:1900  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.95      0.90      0.92       137\n                   Filter       0.88      0.79      0.83       120\n    Compute Derived Value       0.88      0.91      0.90       169\n            Find Extremum       0.93      0.94      0.93       168\n                     Sort       0.91      0.92      0.92       115\n          Determine Range       0.85      0.87      0.86       132\nCharacterize Distribution       0.93      0.92      0.93       139\n           Find Anomalies       0.88      0.94      0.91       143\n                  Cluster       0.95      0.94      0.94       127\n                Correlate       0.90      0.91      0.91       153\n\n                micro avg       0.91      0.91      0.91      1403\n                macro avg       0.91      0.90      0.91      1403\n             weighted avg       0.91      0.91      0.91      1403\n\nConfusion Matrix...\n[[123   0   8   1   1   1   1   1   0   1]\n [  0  95   4   3   0   8   0   7   0   3]\n [  2   1 154   2   1   4   1   3   0   1]\n [  1   1   2 158   2   4   0   0   0   0]\n [  1   0   0   5 106   0   0   0   3   0]\n [  2   8   0   0   2 115   1   0   1   3]\n [  0   1   3   1   2   1 128   1   0   2]\n [  0   1   2   0   0   0   2 135   0   3]\n [  0   0   0   0   0   1   2   3 119   2]\n [  1   1   2   0   2   1   2   3   2 139]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.827, Train Acc: 59.38%, Val Loss:  3.119, Val Acc: 49.25%, Time: 16.95s *\nEpoch: 2\nIter: 200, Train Loss:  1.981, Train Acc: 67.19%, Val Loss:  1.876, Val Acc: 71.85%, Time: 1.71s *\nIter: 300, Train Loss:  1.421, Train Acc: 76.56%, Val Loss:  1.369, Val Acc: 78.97%, Time: 17.57s *\nEpoch: 3\nIter: 400, Train Loss: 0.9813, Train Acc: 89.06%, Val Loss:   1.22, Val Acc: 80.68%, Time: 2.07s *\nIter: 500, Train Loss: 0.6701, Train Acc: 92.19%, Val Loss:  1.009, Val Acc: 84.39%, Time: 18.06s *\nEpoch: 4\nIter: 600, Train Loss: 0.7346, Train Acc: 90.62%, Val Loss:   0.96, Val Acc: 85.82%, Time: 2.27s *\nIter: 700, Train Loss: 0.6028, Train Acc: 92.19%, Val Loss: 0.8894, Val Acc: 87.38%, Time: 18.11s *\nEpoch: 5\nIter: 800, Train Loss: 0.5365, Train Acc: 93.75%, Val Loss: 0.7968, Val Acc: 87.74%, Time: 2.61s *\nIter: 900, Train Loss: 0.3684, Train Acc: 93.75%, Val Loss: 0.7367, Val Acc: 89.31%, Time: 18.52s *\nEpoch: 6\nIter: 1000, Train Loss: 0.1073, Train Acc: 100.00%, Val Loss: 0.7847, Val Acc: 88.38%, Time: 2.84s \nIter: 1100, Train Loss:  0.213, Train Acc: 98.44%, Val Loss: 0.7326, Val Acc: 89.45%, Time: 18.72s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2045, Train Acc: 95.31%, Val Loss: 0.7005, Val Acc: 90.31%, Time: 3.18s *\nIter: 1300, Train Loss: 0.4368, Train Acc: 92.19%, Val Loss: 0.7177, Val Acc: 90.24%, Time: 19.12s \nEpoch: 8\nIter: 1400, Train Loss: 0.02223, Train Acc: 100.00%, Val Loss: 0.7232, Val Acc: 90.81%, Time: 3.41s *\nIter: 1500, Train Loss: 0.1267, Train Acc: 98.44%, Val Loss: 0.7658, Val Acc: 89.88%, Time: 19.35s \n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 9\nIter: 1600, Train Loss: 0.02884, Train Acc: 100.00%, Val Loss: 0.7646, Val Acc: 89.74%, Time: 3.75s \nIter: 1700, Train Loss: 0.06581, Train Acc: 100.00%, Val Loss: 0.7813, Val Acc: 89.67%, Time: 19.67s \nEpoch: 10\nIter: 1800, Train Loss: 0.08723, Train Acc: 100.00%, Val Loss: 0.7578, Val Acc: 90.09%, Time: 3.99s \n1801:1400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.88      0.92      0.90       114\n                   Filter       0.92      0.80      0.86       146\n    Compute Derived Value       0.87      0.88      0.87       163\n            Find Extremum       0.94      0.94      0.94       159\n                     Sort       0.94      0.91      0.92       127\n          Determine Range       0.83      0.92      0.87       143\nCharacterize Distribution       0.93      0.90      0.91       140\n           Find Anomalies       0.84      0.91      0.88       125\n                  Cluster       0.95      0.94      0.94       140\n                Correlate       0.96      0.92      0.94       146\n\n                micro avg       0.90      0.90      0.90      1403\n                macro avg       0.90      0.90      0.90      1403\n             weighted avg       0.91      0.90      0.90      1403\n\nConfusion Matrix...\n[[105   1   2   0   1   0   4   1   0   0]\n [  1 117   4   1   0  11   1  11   0   0]\n [  6   1 143   2   3   5   2   1   0   0]\n [  0   1   3 150   0   1   0   3   1   0]\n [  3   0   0   3 116   2   0   0   3   0]\n [  3   4   3   0   0 131   0   1   1   0]\n [  1   0   3   1   1   6 126   0   1   1]\n [  0   2   4   1   0   0   1 114   1   2]\n [  0   1   0   0   1   2   0   2 131   3]\n [  1   0   2   2   2   0   2   2   0 135]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.763, Train Acc: 51.56%, Val Loss:  3.024, Val Acc: 52.53%, Time: 16.81s *\nEpoch: 2\nIter: 200, Train Loss:  1.757, Train Acc: 73.44%, Val Loss:  1.847, Val Acc: 72.20%, Time: 1.77s *\nIter: 300, Train Loss: 0.9736, Train Acc: 85.94%, Val Loss:  1.393, Val Acc: 78.97%, Time: 17.58s *\nEpoch: 3\nIter: 400, Train Loss: 0.7621, Train Acc: 87.50%, Val Loss:  1.267, Val Acc: 80.61%, Time: 2.00s *\nIter: 500, Train Loss: 0.9794, Train Acc: 87.50%, Val Loss:  1.081, Val Acc: 83.04%, Time: 17.81s *\nEpoch: 4\nIter: 600, Train Loss: 0.5271, Train Acc: 93.75%, Val Loss:  1.134, Val Acc: 83.11%, Time: 2.30s *\nIter: 700, Train Loss: 0.7625, Train Acc: 87.50%, Val Loss: 0.9517, Val Acc: 85.82%, Time: 18.19s *\nEpoch: 5\nIter: 800, Train Loss: 0.5739, Train Acc: 90.62%, Val Loss: 0.8722, Val Acc: 86.81%, Time: 2.55s *\nIter: 900, Train Loss: 0.3086, Train Acc: 96.88%, Val Loss: 0.8645, Val Acc: 87.53%, Time: 18.40s *\nEpoch: 6\nIter: 1000, Train Loss: 0.1264, Train Acc: 98.44%, Val Loss: 0.8188, Val Acc: 87.74%, Time: 2.91s *\nIter: 1100, Train Loss: 0.1908, Train Acc: 98.44%, Val Loss: 0.7852, Val Acc: 88.45%, Time: 18.87s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1469, Train Acc: 100.00%, Val Loss: 0.8265, Val Acc: 87.74%, Time: 3.15s \nIter: 1300, Train Loss: 0.1047, Train Acc: 98.44%, Val Loss: 0.8002, Val Acc: 88.81%, Time: 19.03s *\nEpoch: 8\nIter: 1400, Train Loss: 0.1621, Train Acc: 98.44%, Val Loss: 0.8586, Val Acc: 88.45%, Time: 3.48s \nIter: 1500, Train Loss: 0.09152, Train Acc: 100.00%, Val Loss: 0.7759, Val Acc: 88.38%, Time: 19.32s \nEpoch: 9\nIter: 1600, Train Loss: 0.2183, Train Acc: 96.88%, Val Loss:  0.905, Val Acc: 88.03%, Time: 3.81s \nIter: 1700, Train Loss: 0.08041, Train Acc: 98.44%, Val Loss: 0.9987, Val Acc: 88.17%, Time: 19.83s \n1701:1300  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.82      0.93      0.87       150\n                   Filter       0.84      0.84      0.84       160\n    Compute Derived Value       0.84      0.86      0.85       146\n            Find Extremum       0.96      0.80      0.87       173\n                     Sort       0.86      0.97      0.92       111\n          Determine Range       0.95      0.82      0.88       129\nCharacterize Distribution       0.86      0.90      0.88       126\n           Find Anomalies       0.85      0.91      0.88       141\n                  Cluster       0.93      0.90      0.91       124\n                Correlate       0.94      0.91      0.92       143\n\n                micro avg       0.88      0.88      0.88      1403\n                macro avg       0.88      0.88      0.88      1403\n             weighted avg       0.88      0.88      0.88      1403\n\nConfusion Matrix...\n[[140   0   4   0   1   0   1   2   1   1]\n [  4 135   5   1   3   2   4   5   0   1]\n [  8   3 125   3   0   2   1   4   0   0]\n [ 10   5   5 138   7   0   2   4   1   1]\n [  1   1   0   0 108   0   1   0   0   0]\n [  4   5   5   1   3 106   2   1   2   0]\n [  2   2   1   0   1   2 113   1   2   2]\n [  1  10   0   0   0   0   0 128   0   2]\n [  1   0   0   0   1   0   6   2 112   2]\n [  0   0   3   1   1   0   1   4   3 130]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.133, Train Acc: 57.81%, Val Loss:  3.061, Val Acc: 51.89%, Time: 16.89s *\nEpoch: 2\nIter: 200, Train Loss:  1.565, Train Acc: 75.00%, Val Loss:  1.931, Val Acc: 70.63%, Time: 1.74s *\nIter: 300, Train Loss:  1.594, Train Acc: 75.00%, Val Loss:  1.472, Val Acc: 76.76%, Time: 17.61s *\nEpoch: 3\nIter: 400, Train Loss:  1.478, Train Acc: 78.12%, Val Loss:  1.254, Val Acc: 80.19%, Time: 1.99s *\nIter: 500, Train Loss: 0.9293, Train Acc: 85.94%, Val Loss: 0.9985, Val Acc: 85.10%, Time: 17.87s *\nEpoch: 4\nIter: 600, Train Loss: 0.4951, Train Acc: 93.75%, Val Loss: 0.9998, Val Acc: 84.89%, Time: 2.29s \nIter: 700, Train Loss: 0.5033, Train Acc: 90.62%, Val Loss: 0.8099, Val Acc: 87.60%, Time: 18.18s *\nEpoch: 5\nIter: 800, Train Loss: 0.4278, Train Acc: 95.31%, Val Loss: 0.7694, Val Acc: 88.74%, Time: 2.54s *\nIter: 900, Train Loss: 0.3696, Train Acc: 95.31%, Val Loss: 0.8112, Val Acc: 88.24%, Time: 18.38s \nEpoch: 6\nIter: 1000, Train Loss: 0.4669, Train Acc: 87.50%, Val Loss: 0.7848, Val Acc: 88.74%, Time: 2.87s \nIter: 1100, Train Loss: 0.2649, Train Acc: 93.75%, Val Loss: 0.7312, Val Acc: 89.81%, Time: 18.77s *\nEpoch: 7\nIter: 1200, Train Loss: 0.08773, Train Acc: 98.44%, Val Loss: 0.7816, Val Acc: 88.95%, Time: 3.20s \nIter: 1300, Train Loss: 0.05916, Train Acc: 100.00%, Val Loss: 0.7132, Val Acc: 89.59%, Time: 19.04s \nEpoch: 8\nIter: 1400, Train Loss: 0.1492, Train Acc: 98.44%, Val Loss:  0.725, Val Acc: 90.45%, Time: 3.44s *\nIter: 1500, Train Loss:   0.17, Train Acc: 98.44%, Val Loss: 0.7666, Val Acc: 89.24%, Time: 19.35s \nEpoch: 9\nIter: 1600, Train Loss: 0.0526, Train Acc: 100.00%, Val Loss: 0.6657, Val Acc: 90.88%, Time: 3.72s *\nIter: 1700, Train Loss: 0.1732, Train Acc: 96.88%, Val Loss: 0.7714, Val Acc: 90.52%, Time: 19.50s \nEpoch: 10\nIter: 1800, Train Loss: 0.03188, Train Acc: 100.00%, Val Loss: 0.7016, Val Acc: 90.73%, Time: 3.97s \nIter: 1900, Train Loss: 0.04394, Train Acc: 100.00%, Val Loss: 0.7272, Val Acc: 91.38%, Time: 19.86s *\nEpoch: 11\nIter: 2000, Train Loss: 0.04068, Train Acc: 100.00%, Val Loss: 0.6967, Val Acc: 90.31%, Time: 4.28s \nIter: 2100, Train Loss: 0.09219, Train Acc: 98.44%, Val Loss: 0.6609, Val Acc: 91.38%, Time: 20.00s \nEpoch: 12\nIter: 2200, Train Loss: 0.0167, Train Acc: 100.00%, Val Loss: 0.7108, Val Acc: 90.45%, Time: 4.59s \nIter: 2300, Train Loss: 0.01696, Train Acc: 100.00%, Val Loss: 0.7744, Val Acc: 90.81%, Time: 20.64s \n2301:1900  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.92      0.95      0.93       130\n                   Filter       0.80      0.89      0.84       129\n    Compute Derived Value       0.90      0.90      0.90       148\n            Find Extremum       0.95      0.93      0.94       172\n                     Sort       0.94      0.92      0.93       130\n          Determine Range       0.89      0.88      0.89       132\nCharacterize Distribution       0.95      0.89      0.92       136\n           Find Anomalies       0.92      0.85      0.89       131\n                  Cluster       0.93      0.94      0.94       137\n                Correlate       0.90      0.93      0.91       158\n\n                micro avg       0.91      0.91      0.91      1403\n                macro avg       0.91      0.91      0.91      1403\n             weighted avg       0.91      0.91      0.91      1403\n\nConfusion Matrix...\n[[123   0   3   1   1   0   0   1   1   0]\n [  2 115   1   2   1   4   0   2   2   0]\n [  3   2 133   2   1   2   1   1   0   3]\n [  2   6   1 160   1   1   0   1   0   0]\n [  0   0   2   1 119   3   0   0   4   1]\n [  3   5   3   1   1 116   2   0   0   1]\n [  1   1   1   2   2   1 121   0   1   6]\n [  0  11   2   0   1   1   2 112   0   2]\n [  0   1   1   0   0   1   0   1 129   4]\n [  0   3   1   0   0   1   1   4   1 147]]\nFold:  10\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.953, Train Acc: 53.12%, Val Loss:   3.12, Val Acc: 48.33%, Time: 16.99s *\nEpoch: 2\nIter: 200, Train Loss:  1.797, Train Acc: 73.44%, Val Loss:  1.909, Val Acc: 70.21%, Time: 1.71s *\nIter: 300, Train Loss:  1.261, Train Acc: 81.25%, Val Loss:  1.477, Val Acc: 77.55%, Time: 17.55s *\nEpoch: 3\nIter: 400, Train Loss:  1.129, Train Acc: 81.25%, Val Loss:  1.156, Val Acc: 83.11%, Time: 1.97s *\nIter: 500, Train Loss: 0.8164, Train Acc: 85.94%, Val Loss:  1.055, Val Acc: 83.82%, Time: 17.79s *\nEpoch: 4\nIter: 600, Train Loss: 0.7664, Train Acc: 87.50%, Val Loss: 0.9462, Val Acc: 85.53%, Time: 2.28s *\nIter: 700, Train Loss: 0.7214, Train Acc: 87.50%, Val Loss: 0.8696, Val Acc: 87.10%, Time: 18.22s *\nEpoch: 5\nIter: 800, Train Loss:  0.312, Train Acc: 96.88%, Val Loss: 0.8128, Val Acc: 87.17%, Time: 2.59s *\nIter: 900, Train Loss: 0.2164, Train Acc: 95.31%, Val Loss: 0.8498, Val Acc: 87.38%, Time: 18.45s *\nEpoch: 6\nIter: 1000, Train Loss: 0.1279, Train Acc: 98.44%, Val Loss: 0.7597, Val Acc: 88.52%, Time: 2.88s *\nIter: 1100, Train Loss: 0.3876, Train Acc: 95.31%, Val Loss: 0.7831, Val Acc: 88.52%, Time: 18.76s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1583, Train Acc: 96.88%, Val Loss: 0.6679, Val Acc: 90.38%, Time: 3.13s *\nIter: 1300, Train Loss: 0.3175, Train Acc: 96.88%, Val Loss:  0.712, Val Acc: 90.02%, Time: 18.86s \nEpoch: 8\nIter: 1400, Train Loss: 0.08867, Train Acc: 98.44%, Val Loss: 0.6777, Val Acc: 90.59%, Time: 3.44s *\nIter: 1500, Train Loss: 0.03866, Train Acc: 100.00%, Val Loss:  0.762, Val Acc: 90.02%, Time: 19.38s \nEpoch: 9\nIter: 1600, Train Loss: 0.06554, Train Acc: 98.44%, Val Loss: 0.7004, Val Acc: 90.31%, Time: 3.75s \nIter: 1700, Train Loss: 0.07015, Train Acc: 98.44%, Val Loss: 0.8537, Val Acc: 89.59%, Time: 19.61s \nEpoch: 10\nIter: 1800, Train Loss: 0.05326, Train Acc: 100.00%, Val Loss: 0.8151, Val Acc: 90.09%, Time: 4.03s \n1801:1400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.93      0.86      0.89       139\n                   Filter       0.90      0.85      0.88       149\n    Compute Derived Value       0.79      0.88      0.83       139\n            Find Extremum       0.89      0.94      0.91       142\n                     Sort       0.92      0.92      0.92       137\n          Determine Range       0.91      0.85      0.88       139\nCharacterize Distribution       0.98      0.83      0.90       143\n           Find Anomalies       0.88      0.95      0.92       132\n                  Cluster       0.89      0.95      0.92       114\n                Correlate       0.93      0.96      0.94       169\n\n                micro avg       0.90      0.90      0.90      1403\n                macro avg       0.90      0.90      0.90      1403\n             weighted avg       0.90      0.90      0.90      1403\n\nConfusion Matrix...\n[[119   3  14   0   1   0   1   0   1   0]\n [  0 127   3   3   0   4   0   9   3   0]\n [  3   6 123   2   1   2   0   1   1   0]\n [  0   0   1 134   1   1   0   0   3   2]\n [  2   1   1   4 126   1   0   0   2   0]\n [  0   3   5   5   4 118   0   1   2   1]\n [  4   0   4   2   1   4 119   4   0   5]\n [  0   1   1   0   0   0   1 126   0   3]\n [  0   0   0   1   2   0   0   1 108   2]\n [  0   0   4   0   1   0   0   1   1 162]]\n[0.8945868945868946, 0.9024216524216524, 0.8760683760683761, 0.9123931623931624, 0.9109686609686609, 0.9066286528866714, 0.9037776193870278, 0.880256593014968, 0.9087669280114041, 0.8995010691375623]\n0.8995369608876379, 0.011852283347468472, 0.012493403617228414, 0.00014047662054867844\nexpert\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.716, Train Acc: 59.38%, Val Loss:  3.254, Val Acc: 47.68%, Time: 17.28s *\nEpoch: 2\nIter: 200, Train Loss:   1.55, Train Acc: 79.69%, Val Loss:  2.767, Val Acc: 53.02%, Time: 2.30s *\nIter: 300, Train Loss:  1.789, Train Acc: 73.44%, Val Loss:   2.13, Val Acc: 65.63%, Time: 18.36s *\nEpoch: 3\nIter: 400, Train Loss: 0.7327, Train Acc: 89.06%, Val Loss:  1.847, Val Acc: 70.72%, Time: 3.00s *\nIter: 500, Train Loss: 0.7178, Train Acc: 90.62%, Val Loss:   2.11, Val Acc: 68.11%, Time: 19.09s \nEpoch: 4\nIter: 600, Train Loss: 0.6731, Train Acc: 92.19%, Val Loss:  2.076, Val Acc: 69.96%, Time: 3.79s \nIter: 700, Train Loss: 0.6619, Train Acc: 89.06%, Val Loss:  2.115, Val Acc: 70.53%, Time: 19.81s \nEpoch: 5\nIter: 800, Train Loss: 0.2863, Train Acc: 96.88%, Val Loss:  2.129, Val Acc: 72.06%, Time: 4.52s *\nIter: 900, Train Loss:  0.623, Train Acc: 95.31%, Val Loss:  2.308, Val Acc: 69.76%, Time: 20.54s \nEpoch: 6\nIter: 1000, Train Loss: 0.2874, Train Acc: 95.31%, Val Loss:  2.485, Val Acc: 67.47%, Time: 5.23s \nIter: 1100, Train Loss:  0.143, Train Acc: 98.44%, Val Loss:  2.337, Val Acc: 70.78%, Time: 21.22s \nEpoch: 7\nIter: 1200, Train Loss: 0.1555, Train Acc: 98.44%, Val Loss:  2.202, Val Acc: 71.99%, Time: 5.90s \n1201:800  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.61      0.57       135\n                   Filter       0.65      0.57      0.61       183\n    Compute Derived Value       0.56      0.88      0.68       162\n            Find Extremum       0.88      0.77      0.82       215\n                     Sort       0.78      0.70      0.74       135\n          Determine Range       0.63      0.71      0.67       130\nCharacterize Distribution       0.77      0.80      0.79       135\n           Find Anomalies       0.80      0.75      0.78       137\n                  Cluster       0.93      0.75      0.83       159\n                Correlate       0.79      0.63      0.70       180\n\n                micro avg       0.72      0.72      0.72      1571\n                macro avg       0.73      0.72      0.72      1571\n             weighted avg       0.74      0.72      0.72      1571\n\nConfusion Matrix...\n[[ 83   0  42   0   1   6   1   1   0   1]\n [ 32 105  27   0   0  10   4   0   1   4]\n [  3   1 143   0   0   6   5   3   0   1]\n [  4  18   4 166  12  10   0   1   0   0]\n [ 12   2   1   9  95   8   4   0   1   3]\n [  4   9   5   6   1  92   5   7   1   0]\n [  0   1   9   0   2   0 108   0   3  12]\n [  1  14   9   0   0   0   1 103   3   6]\n [  4   5   1   8  10   1   3   3 120   4]\n [ 12   6  16   0   1  12   9  10   0 114]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.887, Train Acc: 56.25%, Val Loss:  3.325, Val Acc: 43.79%, Time: 16.78s *\nIter: 200, Train Loss:  1.848, Train Acc: 70.31%, Val Loss:   2.35, Val Acc: 63.10%, Time: 32.37s *\nEpoch: 2\nIter: 300, Train Loss:  1.165, Train Acc: 81.25%, Val Loss:  2.201, Val Acc: 65.37%, Time: 15.36s *\nIter: 400, Train Loss: 0.6732, Train Acc: 92.19%, Val Loss:  2.156, Val Acc: 66.09%, Time: 30.97s *\nEpoch: 3\nIter: 500, Train Loss: 0.8996, Train Acc: 87.50%, Val Loss:  2.007, Val Acc: 67.36%, Time: 14.80s *\nIter: 600, Train Loss:  1.347, Train Acc: 85.94%, Val Loss:  2.106, Val Acc: 67.09%, Time: 30.42s \nEpoch: 4\nIter: 700, Train Loss: 0.5951, Train Acc: 95.31%, Val Loss:  2.154, Val Acc: 69.90%, Time: 14.33s *\nIter: 800, Train Loss: 0.3107, Train Acc: 93.75%, Val Loss:  2.282, Val Acc: 69.17%, Time: 29.92s \nEpoch: 5\nIter: 900, Train Loss: 0.5614, Train Acc: 92.19%, Val Loss:  2.104, Val Acc: 70.08%, Time: 13.74s *\nIter: 1000, Train Loss: 0.1998, Train Acc: 98.44%, Val Loss:  2.244, Val Acc: 69.99%, Time: 29.29s \nEpoch: 6\nIter: 1100, Train Loss: 0.1946, Train Acc: 96.88%, Val Loss:  2.655, Val Acc: 69.99%, Time: 13.50s \nIter: 1200, Train Loss: 0.4978, Train Acc: 95.31%, Val Loss:  2.465, Val Acc: 69.99%, Time: 28.93s \nEpoch: 7\nIter: 1300, Train Loss: 0.1177, Train Acc: 98.44%, Val Loss:  2.828, Val Acc: 68.00%, Time: 12.89s \n1301:900  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.15      0.08      0.10       116\n                   Filter       0.53      0.82      0.64       113\n    Compute Derived Value       0.39      0.76      0.51       125\n            Find Extremum       0.82      0.66      0.73       119\n                     Sort       0.95      0.98      0.96       127\n          Determine Range       0.84      0.28      0.42        97\nCharacterize Distribution       0.98      0.83      0.90        96\n           Find Anomalies       0.87      0.76      0.81        95\n                  Cluster       0.96      0.82      0.88       107\n                Correlate       0.75      0.74      0.75       108\n\n                micro avg       0.68      0.68      0.68      1103\n                macro avg       0.72      0.67      0.67      1103\n             weighted avg       0.71      0.68      0.67      1103\n\nConfusion Matrix...\n[[  9  34  59   7   2   0   0   0   0   5]\n [  5  93   9   4   0   1   0   0   1   0]\n [  3  12  95   2   0   0   0   1   0  12]\n [  3   1  26  79   4   1   1   2   0   2]\n [  1   0   0   0 124   0   0   0   2   0]\n [ 36   5  25   1   0  27   1   0   1   1]\n [  0   1  12   0   0   2  80   0   0   1]\n [  0  23   0   0   0   0   0  72   0   0]\n [  2   1   0   2   1   1   0   7  88   5]\n [  0   7  19   1   0   0   0   1   0  80]]\nFold:  3\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.744, Train Acc: 59.38%, Val Loss:  3.534, Val Acc: 36.69%, Time: 16.59s *\nIter: 200, Train Loss:   2.03, Train Acc: 66.67%, Val Loss:  3.013, Val Acc: 52.12%, Time: 32.29s *\nEpoch: 2\nIter: 300, Train Loss:  1.206, Train Acc: 81.25%, Val Loss:   2.75, Val Acc: 58.43%, Time: 15.72s *\nIter: 400, Train Loss:  1.335, Train Acc: 87.50%, Val Loss:  2.865, Val Acc: 57.55%, Time: 31.48s \nEpoch: 3\nIter: 500, Train Loss: 0.7322, Train Acc: 89.06%, Val Loss:  3.081, Val Acc: 56.75%, Time: 15.80s \nIter: 600, Train Loss:  0.353, Train Acc: 95.83%, Val Loss:  3.086, Val Acc: 58.59%, Time: 31.68s *\nEpoch: 4\nIter: 700, Train Loss:  0.215, Train Acc: 96.88%, Val Loss:  2.968, Val Acc: 60.91%, Time: 15.77s *\nIter: 800, Train Loss: 0.3956, Train Acc: 95.83%, Val Loss:  3.129, Val Acc: 60.51%, Time: 31.55s \nEpoch: 5\nIter: 900, Train Loss: 0.2294, Train Acc: 98.44%, Val Loss:  3.287, Val Acc: 60.67%, Time: 15.90s \nIter: 1000, Train Loss: 0.4971, Train Acc: 93.75%, Val Loss:  3.382, Val Acc: 58.51%, Time: 31.69s \nEpoch: 6\nIter: 1100, Train Loss: 0.3879, Train Acc: 92.19%, Val Loss:  3.318, Val Acc: 59.79%, Time: 15.74s \n1101:700  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.63      0.42      0.50       126\n                   Filter       0.55      0.43      0.49       134\n    Compute Derived Value       0.51      0.62      0.56       131\n            Find Extremum       0.45      0.78      0.57       125\n                     Sort       0.90      0.76      0.82       124\n          Determine Range       0.78      0.71      0.75       132\nCharacterize Distribution       0.52      0.74      0.61       126\n           Find Anomalies       0.53      0.59      0.56       113\n                  Cluster       0.70      0.43      0.53       124\n                Correlate       0.60      0.42      0.49       116\n\n                micro avg       0.59      0.59      0.59      1251\n                macro avg       0.62      0.59      0.59      1251\n             weighted avg       0.62      0.59      0.59      1251\n\nConfusion Matrix...\n[[53 12 33  1  0  0 25  0  1  1]\n [ 3 58 25  9  0  6  4 26  0  3]\n [11  1 81 23  0  1 13  0  0  1]\n [15  0  1 98  1  6  3  1  0  0]\n [ 0  1  9 15 94  1  1  1  2  0]\n [ 0  3  1 22  2 94  8  1  1  0]\n [ 1  0  6 16  2  4 93  3  0  1]\n [ 0 15  3  9  1  0  7 67  1 10]\n [ 0  9  1  5  2  7 16 14 53 17]\n [ 1  6  0 18  2  1  8 13 18 49]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.793, Train Acc: 53.12%, Val Loss:  3.471, Val Acc: 44.66%, Time: 16.74s *\nIter: 200, Train Loss:  1.934, Train Acc: 67.65%, Val Loss:  2.377, Val Acc: 65.38%, Time: 32.47s *\nEpoch: 2\nIter: 300, Train Loss:  1.072, Train Acc: 84.38%, Val Loss:  2.197, Val Acc: 69.09%, Time: 15.57s *\nIter: 400, Train Loss:  1.046, Train Acc: 85.29%, Val Loss:  2.008, Val Acc: 72.65%, Time: 31.28s *\nEpoch: 3\nIter: 500, Train Loss: 0.7871, Train Acc: 84.38%, Val Loss:  2.122, Val Acc: 72.73%, Time: 15.77s *\nIter: 600, Train Loss:  1.036, Train Acc: 88.24%, Val Loss:  2.255, Val Acc: 69.49%, Time: 31.55s \nEpoch: 4\nIter: 700, Train Loss: 0.4876, Train Acc: 95.31%, Val Loss:  2.217, Val Acc: 71.46%, Time: 15.75s \nIter: 800, Train Loss: 0.4356, Train Acc: 94.12%, Val Loss:  2.212, Val Acc: 72.41%, Time: 31.52s \nEpoch: 5\nIter: 900, Train Loss: 0.4381, Train Acc: 95.31%, Val Loss:  2.328, Val Acc: 70.12%, Time: 15.76s \n901:500  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.66      0.62      0.64       102\n                   Filter       0.86      0.46      0.60       144\n    Compute Derived Value       0.54      0.65      0.59       122\n            Find Extremum       0.82      0.66      0.73       124\n                     Sort       0.76      0.86      0.81       111\n          Determine Range       0.58      0.71      0.64       140\nCharacterize Distribution       0.75      0.74      0.75       121\n           Find Anomalies       0.68      0.84      0.75       125\n                  Cluster       0.70      0.87      0.77       118\n                Correlate       0.81      0.64      0.71       158\n\n                micro avg       0.70      0.70      0.70      1265\n                macro avg       0.71      0.71      0.70      1265\n             weighted avg       0.72      0.70      0.70      1265\n\nConfusion Matrix...\n[[ 63   1  28   1   0   8   1   0   0   0]\n [ 18  66  12   4   5  21   1  16   1   0]\n [  2   3  79   5   0  17   0   8   1   7]\n [  3   0  11  82  11   3   2   3   6   3]\n [  1   1   0   2  96   6   0   1   3   1]\n [  3   1  12   5   6 100   4   5   1   3]\n [  3   1   5   0   2  13  90   1   5   1]\n [  2   4   0   1   0   0   5 105   3   5]\n [  0   0   0   0   7   0   3   1 103   4]\n [  0   0   0   0   0   3  14  15  25 101]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.087, Train Acc: 48.44%, Val Loss:  3.403, Val Acc: 42.09%, Time: 17.25s *\nEpoch: 2\nIter: 200, Train Loss:  1.718, Train Acc: 73.44%, Val Loss:  2.648, Val Acc: 57.39%, Time: 2.73s *\nIter: 300, Train Loss:  1.108, Train Acc: 84.38%, Val Loss:   2.66, Val Acc: 60.75%, Time: 18.99s *\nEpoch: 3\nIter: 400, Train Loss: 0.5897, Train Acc: 92.19%, Val Loss:  2.569, Val Acc: 62.38%, Time: 3.77s *\nIter: 500, Train Loss: 0.6148, Train Acc: 90.62%, Val Loss:  2.563, Val Acc: 63.94%, Time: 19.93s *\nEpoch: 4\nIter: 600, Train Loss: 0.3295, Train Acc: 92.19%, Val Loss:   2.41, Val Acc: 67.13%, Time: 4.76s *\nIter: 700, Train Loss: 0.4071, Train Acc: 93.75%, Val Loss:  2.573, Val Acc: 67.01%, Time: 20.92s \nEpoch: 5\nIter: 800, Train Loss: 0.2295, Train Acc: 96.88%, Val Loss:  2.757, Val Acc: 66.72%, Time: 5.80s \nIter: 900, Train Loss: 0.2616, Train Acc: 95.31%, Val Loss:  2.729, Val Acc: 66.84%, Time: 21.96s \nEpoch: 6\nIter: 1000, Train Loss: 0.4235, Train Acc: 92.19%, Val Loss:  2.873, Val Acc: 63.07%, Time: 6.81s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.69      0.57      0.63       185\n                   Filter       0.43      0.37      0.39       171\n    Compute Derived Value       0.53      0.43      0.47       248\n            Find Extremum       0.63      0.87      0.73       185\n                     Sort       0.70      0.88      0.78       117\n          Determine Range       0.67      0.59      0.63       207\nCharacterize Distribution       0.54      0.93      0.68       117\n           Find Anomalies       0.71      0.65      0.68       165\n                  Cluster       0.73      0.81      0.77       138\n                Correlate       0.81      0.54      0.65       192\n\n                micro avg       0.63      0.63      0.63      1725\n                macro avg       0.64      0.66      0.64      1725\n             weighted avg       0.64      0.63      0.63      1725\n\nConfusion Matrix...\n[[106  28  16   5  11  13   2   2   2   0]\n [ 38  63  23  15   0  10   5   8   6   3]\n [  5  16 107  49   5  29  24   3  10   0]\n [  1   4  17 161   1   0   1   0   0   0]\n [  0   1   0  11 103   0   0   0   1   1]\n [  2  11  10   6  21 123  25   2   7   0]\n [  1   0   0   0   2   4 109   0   1   0]\n [  0  18   2   8   0   2  11 107   4  13]\n [  0   2   2   2   4   1   8   0 112   7]\n [  0   5  26   0   1   1  17  29  10 103]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.203, Train Acc: 51.56%, Val Loss:  3.233, Val Acc: 46.05%, Time: 17.12s *\nEpoch: 2\nIter: 200, Train Loss:  2.066, Train Acc: 68.75%, Val Loss:  2.309, Val Acc: 64.45%, Time: 2.34s *\nIter: 300, Train Loss:  1.239, Train Acc: 79.69%, Val Loss:  2.128, Val Acc: 66.98%, Time: 18.43s *\nEpoch: 3\nIter: 400, Train Loss: 0.9304, Train Acc: 85.94%, Val Loss:  2.046, Val Acc: 68.12%, Time: 3.06s *\nIter: 500, Train Loss:  1.213, Train Acc: 84.38%, Val Loss:  2.073, Val Acc: 69.26%, Time: 19.04s *\nEpoch: 4\nIter: 600, Train Loss: 0.4859, Train Acc: 92.19%, Val Loss:   2.07, Val Acc: 72.11%, Time: 3.80s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 700, Train Loss:  0.532, Train Acc: 93.75%, Val Loss:  2.206, Val Acc: 70.21%, Time: 19.74s \nEpoch: 5\nIter: 800, Train Loss: 0.2405, Train Acc: 98.44%, Val Loss:  2.397, Val Acc: 67.55%, Time: 4.50s \nIter: 900, Train Loss: 0.2693, Train Acc: 93.75%, Val Loss:  2.325, Val Acc: 69.32%, Time: 20.62s \nEpoch: 6\nIter: 1000, Train Loss: 0.3223, Train Acc: 96.88%, Val Loss:  2.423, Val Acc: 71.47%, Time: 5.19s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.59      0.52       105\n                   Filter       0.64      0.80      0.71       111\n    Compute Derived Value       0.71      0.69      0.70       202\n            Find Extremum       0.79      0.70      0.74       321\n                     Sort       0.95      0.76      0.84       115\n          Determine Range       0.47      0.52      0.49       125\nCharacterize Distribution       0.73      0.68      0.70       177\n           Find Anomalies       0.77      0.62      0.68       125\n                  Cluster       0.85      0.80      0.82       124\n                Correlate       0.69      0.85      0.76       176\n\n                micro avg       0.70      0.70      0.70      1581\n                macro avg       0.71      0.70      0.70      1581\n             weighted avg       0.72      0.70      0.71      1581\n\nConfusion Matrix...\n[[ 62   3  20   1   0   3   2  11   2   1]\n [  7  89   1   1   0   0   0   3   5   5]\n [ 32   3 139   8   0   5   5   0   0  10]\n [ 17  19  21 224   0  37   1   1   0   1]\n [  1   0   0  15  87   2   0   0   8   2]\n [ 11  17   3  21   5  65   0   0   1   2]\n [  3   5   1   0   0  12 120   4   0  32]\n [  0   1   3  15   0   3  11  77   2  13]\n [  1   0   1   0   0   9  14   0  99   0]\n [  0   1   8   0   0   3  11   4   0 149]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.063, Train Acc: 48.44%, Val Loss:  3.141, Val Acc: 52.00%, Time: 17.30s *\nEpoch: 2\nIter: 200, Train Loss:  1.859, Train Acc: 70.31%, Val Loss:  2.112, Val Acc: 64.88%, Time: 2.96s *\nIter: 300, Train Loss:  1.407, Train Acc: 76.56%, Val Loss:  1.919, Val Acc: 68.61%, Time: 19.27s *\nEpoch: 3\nIter: 400, Train Loss: 0.7985, Train Acc: 89.06%, Val Loss:  1.961, Val Acc: 69.23%, Time: 4.08s *\nIter: 500, Train Loss:  0.644, Train Acc: 92.19%, Val Loss:  1.906, Val Acc: 70.07%, Time: 20.26s *\nEpoch: 4\nIter: 600, Train Loss: 0.4237, Train Acc: 92.19%, Val Loss:  1.788, Val Acc: 74.25%, Time: 5.27s *\nIter: 700, Train Loss: 0.4138, Train Acc: 92.19%, Val Loss:  1.833, Val Acc: 72.84%, Time: 21.49s \nEpoch: 5\nIter: 800, Train Loss:  0.182, Train Acc: 96.88%, Val Loss:  2.038, Val Acc: 72.11%, Time: 6.45s \nIter: 900, Train Loss: 0.4112, Train Acc: 92.19%, Val Loss:   2.15, Val Acc: 71.09%, Time: 22.59s \nEpoch: 6\nIter: 1000, Train Loss: 0.2552, Train Acc: 95.31%, Val Loss:  2.154, Val Acc: 72.50%, Time: 7.60s \n1001:600  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.96      0.70       229\n                   Filter       0.62      0.61      0.62       227\n    Compute Derived Value       0.54      0.56      0.55       172\n            Find Extremum       0.83      0.70      0.76       205\n                     Sort       0.79      0.56      0.66       123\n          Determine Range       0.71      0.73      0.72       126\nCharacterize Distribution       0.90      0.85      0.88       158\n           Find Anomalies       0.93      0.54      0.68       202\n                  Cluster       0.72      0.74      0.73       121\n                Correlate       0.95      0.87      0.91       208\n\n                micro avg       0.72      0.72      0.72      1771\n                macro avg       0.75      0.71      0.72      1771\n             weighted avg       0.75      0.72      0.72      1771\n\nConfusion Matrix...\n[[220   1   8   0   0   0   0   0   0   0]\n [ 38 139   9  10   0  21   0   4   6   0]\n [ 48  18  96   0   0   1   6   0   0   3]\n [ 14  11  23 143  12   0   0   2   0   0]\n [ 20   0  10  17  69   5   0   0   2   0]\n [  9   1  14   3   0  92   5   0   1   1]\n [  1   0   6   0   5   5 135   0   5   1]\n [ 19  39   4   0   0   5   1 109  21   4]\n [ 28   2   0   0   0   0   0   1  90   0]\n [  3  12   8   0   1   0   3   1   0 180]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.837, Train Acc: 53.12%, Val Loss:  3.051, Val Acc: 51.44%, Time: 16.83s *\nIter: 200, Train Loss: 0.9292, Train Acc: 87.50%, Val Loss:   2.12, Val Acc: 68.82%, Time: 32.71s *\nEpoch: 2\nIter: 300, Train Loss:  1.161, Train Acc: 84.38%, Val Loss:  1.942, Val Acc: 71.71%, Time: 15.83s *\nIter: 400, Train Loss: 0.7117, Train Acc: 93.75%, Val Loss:  1.878, Val Acc: 73.97%, Time: 31.60s *\nEpoch: 3\nIter: 500, Train Loss: 0.9086, Train Acc: 84.38%, Val Loss:  1.976, Val Acc: 71.63%, Time: 15.84s \nIter: 600, Train Loss: 0.1705, Train Acc: 100.00%, Val Loss:  2.162, Val Acc: 70.69%, Time: 31.57s \nEpoch: 4\nIter: 700, Train Loss: 0.4534, Train Acc: 93.75%, Val Loss:  2.257, Val Acc: 71.71%, Time: 15.84s \nIter: 800, Train Loss: 0.2899, Train Acc: 100.00%, Val Loss:  2.247, Val Acc: 72.49%, Time: 31.63s \nEpoch: 5\n801:400  No optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.78      0.74      0.76       119\n                   Filter       0.74      0.83      0.78       129\n    Compute Derived Value       0.72      0.35      0.48       124\n            Find Extremum       0.82      0.86      0.84       124\n                     Sort       0.85      0.76      0.80       119\n          Determine Range       0.59      0.64      0.62       119\nCharacterize Distribution       0.80      0.95      0.87       129\n           Find Anomalies       0.83      0.57      0.68       150\n                  Cluster       0.70      0.72      0.71       140\n                Correlate       0.55      0.83      0.66       130\n\n                micro avg       0.72      0.72      0.72      1283\n                macro avg       0.74      0.73      0.72      1283\n             weighted avg       0.74      0.72      0.72      1283\n\nConfusion Matrix...\n[[ 88   1   8   2   0   1   1   0   1  17]\n [  0 107   2   4   2  11   0   2   1   0]\n [ 11  11  44   1   6  12   8  12   0  19]\n [  1  10   0 107   3   0   0   1   0   2]\n [  2   0   0   0  90   0   1   0  25   1]\n [  7   4   3  12   1  76   9   0   2   5]\n [  1   0   0   0   0   1 122   0   1   4]\n [  0  10   4   3   2  17   2  86   1  25]\n [  3   0   0   1   1   9   5   3 101  17]\n [  0   1   0   1   1   1   5   0  13 108]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: add/_83 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_680_add\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: add/_83 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_680_add\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: add/_83 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_680_add\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-9-07130408a9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest_acc_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-3-39a028c39de0>\u001b[0m in \u001b[0;36mtrain_split_data\u001b[0;34m(model, train_data, test_data, split_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-6-e2a195a368b4>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, x_val, y_val, categories)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0macc_val\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;31m# \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-6-e2a195a368b4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sess, model, x_pad, y_pad, loss1, acc1, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbatch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfeed_dict1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlossTmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: add/_83 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_680_add\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'perturloss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 31, in __init__\n    perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: perturloss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](perturloss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: add/_83 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_680_add\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"]}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}