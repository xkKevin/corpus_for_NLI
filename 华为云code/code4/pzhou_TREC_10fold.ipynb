{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-text-classification1/data/train_5500.label.txt\"\ntestDataPath = \"s3://corpus-text-classification1/data/TREC_10.label.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    train_data = []\n    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n        for line in f.readlines():\n            word = line.strip().split()\n            label = word[0].split(\":\")[0]\n            content = word[1:]\n            train_data.append([content,label])\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data)\n\n\ndef loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, cat_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\nnum_classes = len(categories)\n\ncat_to_id = {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\nprint(len(embedding),embedding_dim,vocab_size)\n\ntestData = readfile(testDataPath)\ntrainData = readfile(trainDataPath)\n\nprint(len(testData),len(trainData))\ntrainData = np.r_[trainData,testData]\nnp.random.shuffle(trainData)\nlen(trainData)\n\nseq_length = 37", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000\n500 5452\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def train_10_fold(train_data, categories):\n    \n    tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, cat_to_id, num_classes, seq_length)\n    print(len(tx),len(tx[0]),len(tx[1]))\n    \n    fold_id = 0\n    test_acc = []\n    \n    kf = KFold(n_splits=10)\n    for train_i, test_i in kf.split(tx):\n        fold_id += 1\n        print(\"Fold: \", fold_id)\n        test_acc.append(classifier.train(\n            X_train=tx[train_i],\n            y_train=ty[train_i],\n            X_eval=tx[test_i],\n            y_eval=ty[test_i],\n            categories=categories,\n            epochs=80\n        ))\n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class Classifier:\n\n    def __init__(self, model, input_length, output_length):\n        self.model = model\n        self.input_length = input_length\n        self.output_length = output_length\n\n    def compile(self, batch_size=32):\n        self._ds_x = tf.placeholder(tf.float32, [None, self.input_length])\n        self._ds_y = tf.placeholder(tf.float32, [None, self.output_length])\n\n        ds = tf.data.Dataset.from_tensor_slices((self._ds_x, self._ds_y))\n        ds = ds.batch(batch_size)\n\n        self._ds_it = ds.make_initializable_iterator()\n        self._input, self._labels = self._ds_it.get_next()\n\n        self._features = self.model(self._input)\n        self._output = _create_dense_layer(self._features, self.output_length)\n\n        self._create_acc_computations()\n        self._create_backpropagation()\n\n    def _create_acc_computations(self):\n        self._predictions = tf.argmax(self._output, 1)\n        labels = tf.argmax(self._labels, 1)\n        self._accuracy = tf.reduce_mean(\n            tf.cast(tf.equal(self._predictions, labels), 'float32'))\n\n    def _create_backpropagation(self):\n        losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self._output,\n            labels=self._labels)\n        self._loss = tf.reduce_mean(losses)\n\n        optimizer = tf.train.AdamOptimizer(0.005)  # learning_rate = 0.001\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n        grads_and_vars = optimizer.compute_gradients(self._loss)\n\n        self._train_op = optimizer.apply_gradients(\n            grads_and_vars, global_step=global_step)\n\n    def summary(self):\n        print('input:', self._input.shape)\n        self.model.summary()\n        print('output:', self._output.shape)\n\n    def train(self, X_train, y_train, X_eval, y_eval, categories, epochs=20, require_improve=5):\n        \n        session = tf.Session()\n        session.run(tf.global_variables_initializer())\n        session.run(tf.local_variables_initializer())\n        \n        best_vac_acc = 0.0\n        last_improved = 0\n        \n        for e in range(epochs):\n            start_time = time.time()\n            loss, acc = self._train(X_train, y_train, session)\n            duration = time.time() - start_time\n\n            val_loss, val_acc = self._eval(X_eval, y_eval, session)\n            \n            if val_acc > best_vac_acc:\n                best_vac_acc = val_acc\n                last_improved = e\n                improved_str = '*'\n            else:\n                improved_str = ''\n            \n            output = 'Epoch: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n            print(output.format(e + 1, loss, acc, val_loss, val_acc, duration, improved_str))\n            \n            if e - last_improved > require_improve:\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                y_test_cls = np.argmax(y_eval, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.argmax(self.predict(X_eval, session), 1)\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                \n                # evaluate\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                print(\"Confusion Matrix...\")\n                print(metrics.confusion_matrix(y_test_cls, y_test_pred_cls))\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                break\n        # endfor\n        session.close()\n        return accuracy_score\n\n    def _train(self, X_train, y_train, session):\n        import numpy as np\n\n        session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_train,\n                self._ds_y: y_train\n            })\n        loss, acc, = [], []\n        while True:\n            try:\n                _, vloss, vacc = session.run(\n                    fetches=[self._train_op, self._loss, self._accuracy])\n\n                loss.append(vloss)\n                acc.append(vacc)\n            except tf.errors.OutOfRangeError:\n                break\n        # endwhile\n\n        loss, acc = np.mean(loss), np.mean(acc)\n        return loss, acc\n\n    def _eval(self, X_val, y_val, session):\n        session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_val,\n                self._ds_y: y_val\n            })\n\n        loss, acc, = 0, 0\n        while True:\n            try:\n                l, vloss, vacc = session.run(\n                    fetches=[self._labels, self._loss, self._accuracy])\n\n                loss += vloss * len(l)\n                acc += vacc * len(l)\n            except tf.errors.OutOfRangeError:\n                break\n\n        return loss / len(X_val), acc / len(X_val)\n\n    def predict(self, X, session):\n        \n\n        session.run(self._ds_it.initializer,\n                         feed_dict={\n                             self._ds_x: X,\n                             self._ds_y: np.empty((len(X), self.output_length))\n                         }\n                         )\n\n        pred = list()\n        while True:\n            try:\n                ppred = session.run(tf.nn.softmax(self._output))\n\n                pred.extend(map(lambda l: l.tolist(), ppred))\n            except tf.errors.OutOfRangeError:\n                break\n\n        return pred\n\ndef _create_dense_layer(x, output_length):\n    '''Creates a dense layer\n    '''\n    input_size = x.shape[1].value\n    W = tf.Variable(\n        initial_value=tf.truncated_normal(\n            shape=[input_size, output_length],\n            stddev=0.1))\n    b = tf.Variable(\n        initial_value=tf.truncated_normal(\n            shape=[output_length]))\n\n    dense = tf.nn.xw_plus_b(x, W, b)\n\n    return dense", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class ZhouBLSTMCNNModel:\n    '''\n    Implementation proposal of: https://arxiv.org/pdf/1611.06639v1.pdf\n    '''\n    def __init__(self,\n        embedding,\n        em_drop_rate = 0.5,\n        lstm_units   = 6,  # 300\n        lstm_drop_rate = 0.2,\n        conv_size    = (2, 2), # (3, 3)\n        conv_filters = 3,  # 100\n        pool_size    = (2, 2),\n        pool_drop_rate = 0.4):\n        '''Constructor.\n        # Parameters:\n        embedding: Numpy array representing the embedding.\n        em_drop_rate: Drop rate after the embedding layer.\n        lstm_units: Size of the internal states of the LSTM cells.\n        lstm_drop_rate: Drop rate after the lstm layer.\n        conv_size: Size of the convolutions.\n        conv_filters: Number of convolutions filters.\n        pool_size: Size for the max pooling layer.\n        pool_drop_rate: Drop rate of the max pooling layer.\n        '''\n        self._embedding      = embedding\n        self._em_drop_rate   = em_drop_rate\n        self._lstm_units     = lstm_units\n        self._lstm_drop_rate = lstm_drop_rate\n        self._conv_size      = conv_size\n        self._conv_filters   = conv_filters\n        self._pool_size      = pool_size\n        self._pool_drop_rate = pool_drop_rate\n\n    def __call__(self, input):\n        self._embedding_tf = self._create_embedding_layer(\n            self._em_drop_rate, self._embedding, input)\n\n        self._sequences_tf = self._create_blstm_layer(\n            self._lstm_units,\n            self._lstm_drop_rate,\n            self._embedding_tf)\n\n        self._convolution_tf = self._create_convolutional_layer(\n            self._conv_size,\n            self._conv_filters,\n            self._sequences_tf)\n        self._pooling_tf = self._create_maxpooling_layer(\n            self._pool_size,\n            self._pool_drop_rate,\n            self._convolution_tf)\n\n        self._flatten_tf = self._create_flatten_layer(self._pooling_tf)\n\n        return self._flatten_tf\n\n    def summary(self):\n        print(\"embedding: \" + str(self._embedding_tf.shape))\n        print(\"lstm: \" + str(self._sequences_tf.shape))\n        print(\"conv: \" + str(self._convolution_tf.shape))\n        print(\"pooling: \" + str(self._pooling_tf.shape))\n        print(\"flatten: \" + str(self._flatten_tf.shape))\n\n    def _create_embedding_layer(self, em_drop_rate, embedding, input_x):\n        embedding = tf.Variable(initial_value=embedding)\n\n        embedded_chars = tf.nn.embedding_lookup(\n            embedding, tf.cast(input_x, 'int32'))\n\n        return tf.nn.dropout(embedded_chars, em_drop_rate)\n\n    def _create_blstm_layer(self, lstm_units, lstm_drop_rate, embedding):\n        lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_units)\n        sequence = tf.unstack(embedding, axis=1)\n        hs, _, _ = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell,\n            sequence,\n            dtype=tf.float32)\n        \n        hs = tf.stack(\n            values=hs,\n            axis=1)\n        ss = tf.reduce_sum(\n            tf.reshape(hs, shape=[-1, hs.shape[1], 2, lstm_units]),\n            axis=2\n        )\n\n        return tf.nn.dropout(ss, lstm_drop_rate)\n\n    def _create_convolutional_layer(self,\n        conv_size, num_filters, tensor):\n        \n        print(str(tensor.shape))\n\n        filter_heigth = conv_size[0]\n        filter_width  = conv_size[1]\n\n        filter_shape = [filter_heigth, filter_width,\n            1, num_filters]\n\n        W = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=filter_shape,\n                stddev=0.1))\n        b = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=[num_filters]))\n\n        tensor_expanded = tf.expand_dims(tensor, -1)\n        conv = tf.nn.conv2d(\n            input=tensor_expanded,\n            filter=W,\n            strides=[1,1,1,1],\n            padding='VALID')\n\n        bias = tf.nn.bias_add(conv, b)\n        c = tf.nn.relu(bias)\n\n        return c\n\n    def _create_maxpooling_layer(self, size, pool_drop_rate, conv):\n        pooled = tf.nn.max_pool3d(\n            input=tf.expand_dims(conv, -1),\n            ksize=[1, size[0], size[1], conv.shape[3], 1],\n            strides=[1, size[0], size[1], conv.shape[3], 1],\n            padding='VALID')\n        \n        return tf.nn.dropout(pooled, pool_drop_rate)\n\n    def _create_flatten_layer(self, tensor):\n        return tf.reshape(tensor, [-1, tensor.shape[1] * tensor.shape[2]])", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_vector = embedding.astype('float32')\nmodel = ZhouBLSTMCNNModel(embedding=word_vector)\n\nclassifier = Classifier(\n    model=model,\n    input_length=seq_length,\n    output_length=num_classes)\n\nclassifier.compile(batch_size=32)\nclassifier.summary()", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "(?, 37, 6)\ninput: (?, 37)\nembedding: (?, 37, 100)\nlstm: (?, 37, 6)\nconv: (?, 36, 5, 3)\npooling: (?, 18, 2, 1, 1)\nflatten: (?, 36)\noutput: (?, 6)\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_10_fold(trainData, categories)", "execution_count": null, "outputs": [{"output_type": "stream", "text": "5952 37 37\nFold:  1\nEpoch: 1, Train Loss:  1.724, Train Acc: 21.33%, Val Loss:   1.67, Val Acc: 21.98%, Time: 24.10s *\nEpoch: 2, Train Loss:  1.651, Train Acc: 24.06%, Val Loss:  1.553, Val Acc: 28.36%, Time: 20.53s *\nEpoch: 3, Train Loss:  1.483, Train Acc: 34.49%, Val Loss:  1.396, Val Acc: 40.94%, Time: 20.59s *\nEpoch: 4, Train Loss:  1.357, Train Acc: 41.82%, Val Loss:  1.351, Val Acc: 43.79%, Time: 20.70s *\nEpoch: 5, Train Loss:   1.25, Train Acc: 48.20%, Val Loss:  1.295, Val Acc: 47.32%, Time: 20.78s *\nEpoch: 6, Train Loss:  1.164, Train Acc: 52.47%, Val Loss:  1.275, Val Acc: 51.51%, Time: 20.53s *\nEpoch: 7, Train Loss:  1.061, Train Acc: 57.42%, Val Loss:   1.26, Val Acc: 53.19%, Time: 20.40s *\nEpoch: 8, Train Loss: 0.9988, Train Acc: 60.11%, Val Loss:  1.296, Val Acc: 53.52%, Time: 20.41s *\nEpoch: 9, Train Loss:  0.968, Train Acc: 61.93%, Val Loss:  1.226, Val Acc: 57.38%, Time: 20.69s *\nEpoch: 10, Train Loss: 0.9302, Train Acc: 63.31%, Val Loss:  1.286, Val Acc: 56.21%, Time: 20.59s \nEpoch: 11, Train Loss: 0.9198, Train Acc: 63.73%, Val Loss:   1.36, Val Acc: 55.37%, Time: 20.72s \nEpoch: 12, Train Loss: 0.8757, Train Acc: 65.40%, Val Loss:  1.226, Val Acc: 57.21%, Time: 20.55s \nEpoch: 13, Train Loss:  0.853, Train Acc: 66.28%, Val Loss:  1.276, Val Acc: 58.56%, Time: 20.51s *\nEpoch: 14, Train Loss: 0.8402, Train Acc: 66.82%, Val Loss:  1.242, Val Acc: 60.23%, Time: 20.44s *\nEpoch: 15, Train Loss: 0.8171, Train Acc: 67.58%, Val Loss:  1.389, Val Acc: 57.89%, Time: 20.41s \nEpoch: 16, Train Loss: 0.7977, Train Acc: 69.09%, Val Loss:  1.373, Val Acc: 58.05%, Time: 20.43s \nEpoch: 17, Train Loss: 0.7926, Train Acc: 68.34%, Val Loss:  1.466, Val Acc: 55.03%, Time: 20.42s \nEpoch: 18, Train Loss: 0.7657, Train Acc: 69.31%, Val Loss:  1.495, Val Acc: 59.06%, Time: 20.51s \nEpoch: 19, Train Loss: 0.7621, Train Acc: 70.05%, Val Loss:  1.323, Val Acc: 62.58%, Time: 20.55s *\nEpoch: 20, Train Loss: 0.7365, Train Acc: 70.69%, Val Loss:  1.441, Val Acc: 56.54%, Time: 20.61s \nEpoch: 21, Train Loss: 0.7201, Train Acc: 70.85%, Val Loss:  1.373, Val Acc: 60.40%, Time: 20.66s \nEpoch: 22, Train Loss: 0.7031, Train Acc: 71.88%, Val Loss:  1.432, Val Acc: 58.39%, Time: 20.44s \nEpoch: 23, Train Loss: 0.7067, Train Acc: 72.12%, Val Loss:  1.574, Val Acc: 59.73%, Time: 20.60s \nEpoch: 24, Train Loss: 0.6998, Train Acc: 72.07%, Val Loss:  1.526, Val Acc: 58.72%, Time: 20.74s \nEpoch: 25, Train Loss: 0.7124, Train Acc: 70.89%, Val Loss:  1.466, Val Acc: 58.89%, Time: 20.91s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         7\n        DESC       0.77      0.64      0.70       135\n        ENTY       0.38      0.28      0.33       134\n         HUM       0.48      0.79      0.59       135\n         LOC       0.67      0.61      0.64        82\n         NUM       0.93      0.80      0.86       103\n\n   micro avg       0.61      0.61      0.61       596\n   macro avg       0.54      0.52      0.52       596\nweighted avg       0.62      0.61      0.60       596\n\nConfusion Matrix...\n[[  0   0   3   1   3   0]\n [  0  86  17  18  11   3]\n [  0  13  38  77   5   1]\n [  0   2  25 106   2   0]\n [  0   6   9  15  50   2]\n [  0   4   7   6   4  82]]\nFold:  2\n", "name": "stdout"}, {"output_type": "stream", "text": "/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n", "name": "stderr"}, {"output_type": "stream", "text": "Epoch: 1, Train Loss:  1.746, Train Acc: 20.65%, Val Loss:  1.699, Val Acc: 20.13%, Time: 23.14s *\nEpoch: 2, Train Loss:  1.631, Train Acc: 25.40%, Val Loss:  1.527, Val Acc: 35.07%, Time: 20.64s *\nEpoch: 3, Train Loss:  1.372, Train Acc: 42.00%, Val Loss:  1.376, Val Acc: 45.13%, Time: 20.81s *\nEpoch: 4, Train Loss:  1.227, Train Acc: 49.13%, Val Loss:   1.29, Val Acc: 45.47%, Time: 20.53s *\nEpoch: 5, Train Loss:  1.135, Train Acc: 53.68%, Val Loss:  1.229, Val Acc: 50.50%, Time: 20.59s *\nEpoch: 6, Train Loss:  1.072, Train Acc: 57.68%, Val Loss:  1.249, Val Acc: 54.19%, Time: 20.43s *\nEpoch: 7, Train Loss:  1.009, Train Acc: 61.25%, Val Loss:  1.243, Val Acc: 53.69%, Time: 20.53s \nEpoch: 8, Train Loss: 0.9535, Train Acc: 63.64%, Val Loss:  1.282, Val Acc: 56.71%, Time: 20.47s *\nEpoch: 9, Train Loss: 0.9173, Train Acc: 65.12%, Val Loss:  1.229, Val Acc: 56.38%, Time: 20.62s \nEpoch: 10, Train Loss: 0.8569, Train Acc: 67.64%, Val Loss:  1.268, Val Acc: 58.72%, Time: 20.56s *\nEpoch: 11, Train Loss: 0.8189, Train Acc: 68.46%, Val Loss:  1.308, Val Acc: 56.54%, Time: 20.60s \nEpoch: 12, Train Loss: 0.8012, Train Acc: 69.60%, Val Loss:  1.519, Val Acc: 57.72%, Time: 20.50s \nEpoch: 13, Train Loss: 0.7605, Train Acc: 70.62%, Val Loss:  1.498, Val Acc: 57.38%, Time: 20.56s \nEpoch: 14, Train Loss: 0.7554, Train Acc: 70.94%, Val Loss:  1.334, Val Acc: 59.56%, Time: 20.36s *\nEpoch: 15, Train Loss: 0.7359, Train Acc: 71.72%, Val Loss:  1.321, Val Acc: 59.23%, Time: 20.67s \nEpoch: 16, Train Loss:  0.699, Train Acc: 72.35%, Val Loss:  1.477, Val Acc: 57.38%, Time: 20.59s \nEpoch: 17, Train Loss: 0.6859, Train Acc: 73.58%, Val Loss:  1.444, Val Acc: 58.22%, Time: 20.63s \nEpoch: 18, Train Loss: 0.6953, Train Acc: 72.62%, Val Loss:  1.438, Val Acc: 57.21%, Time: 20.61s \nEpoch: 19, Train Loss: 0.6578, Train Acc: 73.96%, Val Loss:  1.433, Val Acc: 59.73%, Time: 20.77s *\nEpoch: 20, Train Loss: 0.6548, Train Acc: 73.41%, Val Loss:   1.72, Val Acc: 56.54%, Time: 20.43s \nEpoch: 21, Train Loss: 0.6593, Train Acc: 74.40%, Val Loss:  1.602, Val Acc: 58.89%, Time: 20.90s \nEpoch: 22, Train Loss: 0.6436, Train Acc: 74.19%, Val Loss:  1.525, Val Acc: 62.25%, Time: 20.52s *\nEpoch: 23, Train Loss: 0.6479, Train Acc: 74.57%, Val Loss:  1.578, Val Acc: 58.89%, Time: 20.60s \nEpoch: 24, Train Loss: 0.6433, Train Acc: 75.20%, Val Loss:  1.719, Val Acc: 56.71%, Time: 20.75s \nEpoch: 25, Train Loss: 0.6197, Train Acc: 75.97%, Val Loss:  1.801, Val Acc: 57.72%, Time: 20.63s \nEpoch: 26, Train Loss: 0.6021, Train Acc: 76.65%, Val Loss:  1.877, Val Acc: 61.58%, Time: 20.59s \nEpoch: 27, Train Loss: 0.6002, Train Acc: 79.32%, Val Loss:  1.765, Val Acc: 61.91%, Time: 18.92s \nEpoch: 28, Train Loss: 0.5769, Train Acc: 79.71%, Val Loss:  1.741, Val Acc: 62.92%, Time: 20.54s *\nEpoch: 29, Train Loss: 0.5737, Train Acc: 81.24%, Val Loss:  1.667, Val Acc: 65.44%, Time: 20.42s *\nEpoch: 30, Train Loss: 0.5488, Train Acc: 81.96%, Val Loss:  1.909, Val Acc: 65.27%, Time: 20.66s \nEpoch: 31, Train Loss:  0.546, Train Acc: 82.43%, Val Loss:  1.883, Val Acc: 65.60%, Time: 20.62s *\nEpoch: 32, Train Loss: 0.5392, Train Acc: 83.53%, Val Loss:  1.879, Val Acc: 63.59%, Time: 20.48s \nEpoch: 33, Train Loss: 0.5244, Train Acc: 83.73%, Val Loss:  1.892, Val Acc: 65.27%, Time: 20.44s \nEpoch: 34, Train Loss: 0.5029, Train Acc: 84.49%, Val Loss:  1.849, Val Acc: 65.10%, Time: 20.59s \nEpoch: 35, Train Loss: 0.5002, Train Acc: 84.29%, Val Loss:  2.225, Val Acc: 63.76%, Time: 20.46s \nEpoch: 36, Train Loss: 0.5097, Train Acc: 84.13%, Val Loss:  1.956, Val Acc: 65.44%, Time: 20.53s \nEpoch: 37, Train Loss: 0.4865, Train Acc: 85.66%, Val Loss:   2.01, Val Acc: 67.79%, Time: 20.57s *\nEpoch: 38, Train Loss: 0.4941, Train Acc: 85.53%, Val Loss:  1.918, Val Acc: 64.60%, Time: 20.61s \nEpoch: 39, Train Loss: 0.4828, Train Acc: 85.27%, Val Loss:   1.95, Val Acc: 68.12%, Time: 20.59s *\nEpoch: 40, Train Loss: 0.4921, Train Acc: 85.47%, Val Loss:  1.828, Val Acc: 66.95%, Time: 20.79s \nEpoch: 41, Train Loss: 0.4837, Train Acc: 86.17%, Val Loss:  1.894, Val Acc: 66.28%, Time: 20.53s \nEpoch: 42, Train Loss: 0.4746, Train Acc: 86.22%, Val Loss:  1.983, Val Acc: 66.78%, Time: 20.52s \nEpoch: 43, Train Loss: 0.4648, Train Acc: 86.35%, Val Loss:  2.088, Val Acc: 66.95%, Time: 20.40s \nEpoch: 44, Train Loss:  0.457, Train Acc: 86.74%, Val Loss:  2.426, Val Acc: 67.95%, Time: 20.72s \nEpoch: 45, Train Loss: 0.4564, Train Acc: 86.73%, Val Loss:  1.829, Val Acc: 67.28%, Time: 20.65s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         9\n        DESC       0.53      0.74      0.62       122\n        ENTY       0.67      0.62      0.64       126\n         HUM       0.70      0.67      0.68       129\n         LOC       0.71      0.68      0.69        98\n         NUM       0.93      0.77      0.84       112\n\n   micro avg       0.68      0.68      0.68       596\n   macro avg       0.59      0.58      0.58       596\nweighted avg       0.69      0.68      0.68       596\n\nConfusion Matrix...\n[[ 0  8  1  0  0  0]\n [ 0 90 15  9  6  2]\n [ 0 31 78 11  3  3]\n [ 0 18  9 86 16  0]\n [ 0 16  3 11 67  1]\n [ 0  7 10  6  3 86]]\nFold:  3\nEpoch: 1, Train Loss:  1.754, Train Acc: 20.89%, Val Loss:  1.666, Val Acc: 22.35%, Time: 24.25s *\nEpoch: 2, Train Loss:  1.672, Train Acc: 22.34%, Val Loss:  1.678, Val Acc: 23.19%, Time: 20.66s *\nEpoch: 3, Train Loss:   1.63, Train Acc: 26.05%, Val Loss:  1.581, Val Acc: 28.57%, Time: 20.41s *\nEpoch: 4, Train Loss:  1.468, Train Acc: 39.00%, Val Loss:   1.49, Val Acc: 37.65%, Time: 20.50s *\nEpoch: 5, Train Loss:  1.302, Train Acc: 47.73%, Val Loss:  1.349, Val Acc: 43.87%, Time: 20.66s *\nEpoch: 6, Train Loss:  1.202, Train Acc: 51.26%, Val Loss:  1.389, Val Acc: 45.21%, Time: 20.60s *\nEpoch: 7, Train Loss:  1.151, Train Acc: 53.02%, Val Loss:  1.369, Val Acc: 46.55%, Time: 20.85s *\nEpoch: 8, Train Loss:   1.11, Train Acc: 55.40%, Val Loss:  1.312, Val Acc: 46.22%, Time: 20.68s \nEpoch: 9, Train Loss:  1.073, Train Acc: 56.63%, Val Loss:  1.364, Val Acc: 48.91%, Time: 20.66s *\nEpoch: 10, Train Loss:  1.039, Train Acc: 58.16%, Val Loss:  1.328, Val Acc: 49.08%, Time: 20.55s *\nEpoch: 11, Train Loss:  1.022, Train Acc: 58.66%, Val Loss:  1.447, Val Acc: 45.88%, Time: 20.53s \nEpoch: 12, Train Loss: 0.9875, Train Acc: 60.27%, Val Loss:   1.47, Val Acc: 47.06%, Time: 20.56s \nEpoch: 13, Train Loss: 0.9553, Train Acc: 62.47%, Val Loss:  1.411, Val Acc: 51.93%, Time: 20.58s *\nEpoch: 14, Train Loss: 0.9481, Train Acc: 62.93%, Val Loss:  1.458, Val Acc: 51.93%, Time: 20.72s *\nEpoch: 15, Train Loss: 0.9227, Train Acc: 64.59%, Val Loss:   1.48, Val Acc: 50.42%, Time: 20.59s \nEpoch: 16, Train Loss: 0.9131, Train Acc: 64.67%, Val Loss:  1.503, Val Acc: 50.25%, Time: 20.57s \nEpoch: 17, Train Loss: 0.8851, Train Acc: 65.39%, Val Loss:  1.435, Val Acc: 49.08%, Time: 20.62s \nEpoch: 18, Train Loss:  0.872, Train Acc: 66.30%, Val Loss:  1.482, Val Acc: 52.61%, Time: 20.48s *\nEpoch: 19, Train Loss: 0.8623, Train Acc: 67.72%, Val Loss:  1.746, Val Acc: 51.09%, Time: 20.54s \nEpoch: 20, Train Loss: 0.8482, Train Acc: 67.56%, Val Loss:  1.612, Val Acc: 53.78%, Time: 20.71s *\nEpoch: 21, Train Loss: 0.8279, Train Acc: 68.41%, Val Loss:   1.53, Val Acc: 50.25%, Time: 20.53s \nEpoch: 22, Train Loss: 0.8124, Train Acc: 69.19%, Val Loss:  1.588, Val Acc: 50.76%, Time: 20.27s \nEpoch: 23, Train Loss: 0.8046, Train Acc: 69.33%, Val Loss:  1.794, Val Acc: 51.26%, Time: 20.56s \nEpoch: 24, Train Loss:    0.8, Train Acc: 69.93%, Val Loss:  1.583, Val Acc: 52.94%, Time: 20.58s \nEpoch: 25, Train Loss: 0.7974, Train Acc: 69.75%, Val Loss:  1.528, Val Acc: 52.44%, Time: 20.66s \nEpoch: 26, Train Loss:   0.79, Train Acc: 70.96%, Val Loss:  2.043, Val Acc: 50.42%, Time: 20.59s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         9\n        DESC       0.51      0.61      0.56       141\n        ENTY       0.78      0.45      0.57       134\n         HUM       0.35      0.79      0.48       107\n         LOC       0.40      0.04      0.07       100\n         NUM       0.85      0.79      0.82       104\n\n   micro avg       0.53      0.53      0.53       595\n   macro avg       0.48      0.45      0.42       595\nweighted avg       0.58      0.53      0.50       595\n\nConfusion Matrix...\n[[ 0  2  0  6  1  0]\n [ 0 86  9 37  1  8]\n [ 0 43 60 25  4  2]\n [ 0 13  5 85  0  4]\n [ 0 15  1 80  4  0]\n [ 0  9  2 11  0 82]]\nFold:  4\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 1, Train Loss:  1.712, Train Acc: 21.31%, Val Loss:  1.681, Val Acc: 22.86%, Time: 24.36s *\nEpoch: 2, Train Loss:   1.67, Train Acc: 21.90%, Val Loss:  1.663, Val Acc: 23.53%, Time: 20.61s *\nEpoch: 3, Train Loss:  1.615, Train Acc: 27.18%, Val Loss:  1.576, Val Acc: 32.61%, Time: 20.77s *\nEpoch: 4, Train Loss:  1.445, Train Acc: 38.39%, Val Loss:  1.462, Val Acc: 38.15%, Time: 20.67s *\nEpoch: 5, Train Loss:  1.298, Train Acc: 44.69%, Val Loss:  1.334, Val Acc: 41.51%, Time: 20.78s *\nEpoch: 6, Train Loss:  1.166, Train Acc: 54.98%, Val Loss:  1.335, Val Acc: 48.91%, Time: 20.65s *\nEpoch: 7, Train Loss:  1.081, Train Acc: 59.10%, Val Loss:  1.309, Val Acc: 53.61%, Time: 20.87s *\nEpoch: 8, Train Loss: 0.9883, Train Acc: 62.86%, Val Loss:  1.214, Val Acc: 56.47%, Time: 20.61s *\nEpoch: 9, Train Loss: 0.9501, Train Acc: 64.68%, Val Loss:  1.407, Val Acc: 57.65%, Time: 20.56s *\nEpoch: 10, Train Loss: 0.9111, Train Acc: 67.19%, Val Loss:  1.472, Val Acc: 56.64%, Time: 20.50s \nEpoch: 11, Train Loss: 0.8783, Train Acc: 69.00%, Val Loss:  1.199, Val Acc: 59.66%, Time: 20.48s *\nEpoch: 12, Train Loss: 0.8234, Train Acc: 70.82%, Val Loss:  1.224, Val Acc: 61.85%, Time: 20.61s *\nEpoch: 13, Train Loss: 0.7863, Train Acc: 73.48%, Val Loss:  1.403, Val Acc: 60.00%, Time: 20.50s \nEpoch: 14, Train Loss: 0.7694, Train Acc: 73.82%, Val Loss:  1.292, Val Acc: 61.01%, Time: 20.65s \nEpoch: 15, Train Loss: 0.7298, Train Acc: 76.00%, Val Loss:  1.302, Val Acc: 61.51%, Time: 20.41s \nEpoch: 16, Train Loss: 0.7453, Train Acc: 75.90%, Val Loss:  1.338, Val Acc: 63.19%, Time: 21.06s *\nEpoch: 17, Train Loss: 0.7034, Train Acc: 77.52%, Val Loss:  1.417, Val Acc: 62.52%, Time: 20.72s \nEpoch: 18, Train Loss:  0.687, Train Acc: 77.98%, Val Loss:  1.424, Val Acc: 64.20%, Time: 20.55s *\nEpoch: 19, Train Loss: 0.6938, Train Acc: 77.50%, Val Loss:   1.45, Val Acc: 62.52%, Time: 20.67s \nEpoch: 20, Train Loss: 0.6571, Train Acc: 79.59%, Val Loss:  1.528, Val Acc: 62.02%, Time: 20.75s \nEpoch: 21, Train Loss:  0.649, Train Acc: 79.30%, Val Loss:  1.641, Val Acc: 64.54%, Time: 20.75s *\nEpoch: 22, Train Loss: 0.6346, Train Acc: 80.59%, Val Loss:  1.514, Val Acc: 64.54%, Time: 20.79s *\nEpoch: 23, Train Loss: 0.6316, Train Acc: 81.14%, Val Loss:  1.503, Val Acc: 63.53%, Time: 20.56s \nEpoch: 24, Train Loss: 0.5893, Train Acc: 81.42%, Val Loss:  1.569, Val Acc: 63.53%, Time: 20.53s \nEpoch: 25, Train Loss: 0.5884, Train Acc: 82.23%, Val Loss:   1.48, Val Acc: 62.69%, Time: 20.61s \nEpoch: 26, Train Loss: 0.5841, Train Acc: 82.08%, Val Loss:   1.67, Val Acc: 62.35%, Time: 20.63s \nEpoch: 27, Train Loss: 0.6112, Train Acc: 82.22%, Val Loss:  1.706, Val Acc: 61.85%, Time: 20.57s \nEpoch: 28, Train Loss: 0.5425, Train Acc: 83.46%, Val Loss:  1.661, Val Acc: 65.38%, Time: 20.70s *\nEpoch: 29, Train Loss:  0.567, Train Acc: 82.63%, Val Loss:  1.813, Val Acc: 62.52%, Time: 20.58s \nEpoch: 30, Train Loss: 0.5645, Train Acc: 83.38%, Val Loss:  1.495, Val Acc: 64.71%, Time: 18.98s \nEpoch: 31, Train Loss: 0.5324, Train Acc: 84.21%, Val Loss:   1.81, Val Acc: 61.18%, Time: 20.52s \nEpoch: 32, Train Loss: 0.5681, Train Acc: 83.74%, Val Loss:  1.788, Val Acc: 63.87%, Time: 20.84s \nEpoch: 33, Train Loss: 0.5378, Train Acc: 84.02%, Val Loss:  1.853, Val Acc: 66.05%, Time: 20.51s *\nEpoch: 34, Train Loss: 0.5286, Train Acc: 83.79%, Val Loss:  1.911, Val Acc: 64.03%, Time: 20.63s \nEpoch: 35, Train Loss: 0.5345, Train Acc: 83.73%, Val Loss:  1.809, Val Acc: 65.88%, Time: 20.65s \nEpoch: 36, Train Loss: 0.5283, Train Acc: 83.74%, Val Loss:  1.697, Val Acc: 64.71%, Time: 20.62s \nEpoch: 37, Train Loss: 0.5032, Train Acc: 84.51%, Val Loss:  1.802, Val Acc: 63.53%, Time: 20.85s \nEpoch: 38, Train Loss: 0.5013, Train Acc: 84.93%, Val Loss:  1.666, Val Acc: 65.55%, Time: 20.58s \nEpoch: 39, Train Loss: 0.4855, Train Acc: 85.27%, Val Loss:  2.112, Val Acc: 65.88%, Time: 20.62s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00        10\n        DESC       0.50      0.67      0.57       119\n        ENTY       0.70      0.61      0.66       135\n         HUM       0.76      0.68      0.72       147\n         LOC       0.85      0.74      0.80        86\n         NUM       0.72      0.82      0.77        98\n\n   micro avg       0.68      0.68      0.68       595\n   macro avg       0.59      0.59      0.58       595\nweighted avg       0.69      0.68      0.68       595\n\nConfusion Matrix...\n[[  0   6   0   0   0   4]\n [  0  80  14   9   7   9]\n [  0  30  83  16   3   3]\n [  0  21  17 100   0   9]\n [  0  11   2   3  64   6]\n [  0  12   2   3   1  80]]\nFold:  5\nEpoch: 1, Train Loss:   1.77, Train Acc: 18.89%, Val Loss:  1.674, Val Acc: 23.70%, Time: 23.94s *\nEpoch: 2, Train Loss:   1.68, Train Acc: 21.48%, Val Loss:  1.653, Val Acc: 23.03%, Time: 20.66s \nEpoch: 3, Train Loss:  1.609, Train Acc: 28.18%, Val Loss:  1.539, Val Acc: 33.11%, Time: 20.94s *\nEpoch: 4, Train Loss:  1.478, Train Acc: 35.37%, Val Loss:  1.445, Val Acc: 34.45%, Time: 20.61s *\nEpoch: 5, Train Loss:   1.37, Train Acc: 41.96%, Val Loss:  1.449, Val Acc: 40.50%, Time: 20.64s *\nEpoch: 6, Train Loss:  1.247, Train Acc: 48.70%, Val Loss:  1.367, Val Acc: 44.54%, Time: 20.86s *\nEpoch: 7, Train Loss:  1.164, Train Acc: 53.40%, Val Loss:   1.32, Val Acc: 44.87%, Time: 20.66s *\nEpoch: 8, Train Loss:  1.098, Train Acc: 56.50%, Val Loss:  1.303, Val Acc: 49.92%, Time: 20.87s *\nEpoch: 9, Train Loss:  1.063, Train Acc: 59.30%, Val Loss:  1.251, Val Acc: 51.26%, Time: 20.73s *\nEpoch: 10, Train Loss: 0.9933, Train Acc: 61.60%, Val Loss:  1.382, Val Acc: 52.77%, Time: 20.69s *\nEpoch: 11, Train Loss: 0.9441, Train Acc: 63.89%, Val Loss:  1.299, Val Acc: 52.61%, Time: 20.91s \nEpoch: 12, Train Loss: 0.9039, Train Acc: 66.36%, Val Loss:  1.259, Val Acc: 53.61%, Time: 20.54s *\nEpoch: 13, Train Loss: 0.8606, Train Acc: 68.25%, Val Loss:  1.282, Val Acc: 55.13%, Time: 20.63s *\nEpoch: 14, Train Loss: 0.8494, Train Acc: 69.01%, Val Loss:  1.386, Val Acc: 58.32%, Time: 20.81s *\nEpoch: 15, Train Loss: 0.8064, Train Acc: 71.24%, Val Loss:  1.536, Val Acc: 56.64%, Time: 20.57s \nEpoch: 16, Train Loss: 0.7797, Train Acc: 73.60%, Val Loss:  1.333, Val Acc: 58.82%, Time: 20.45s *\nEpoch: 17, Train Loss: 0.7485, Train Acc: 75.01%, Val Loss:  1.629, Val Acc: 60.00%, Time: 20.82s *\nEpoch: 18, Train Loss: 0.7114, Train Acc: 77.21%, Val Loss:  1.313, Val Acc: 62.86%, Time: 20.53s *\nEpoch: 19, Train Loss: 0.6733, Train Acc: 79.00%, Val Loss:  1.359, Val Acc: 62.52%, Time: 20.56s \nEpoch: 20, Train Loss:  0.684, Train Acc: 78.60%, Val Loss:  1.496, Val Acc: 63.53%, Time: 20.53s *\nEpoch: 21, Train Loss: 0.6588, Train Acc: 80.04%, Val Loss:  1.461, Val Acc: 62.69%, Time: 20.71s \nEpoch: 22, Train Loss: 0.6495, Train Acc: 79.84%, Val Loss:  1.814, Val Acc: 61.34%, Time: 20.48s \nEpoch: 23, Train Loss: 0.6182, Train Acc: 80.82%, Val Loss:  1.568, Val Acc: 62.52%, Time: 20.78s \nEpoch: 24, Train Loss: 0.5932, Train Acc: 82.32%, Val Loss:  1.689, Val Acc: 65.21%, Time: 20.56s *\nEpoch: 25, Train Loss: 0.6074, Train Acc: 81.76%, Val Loss:  1.818, Val Acc: 62.02%, Time: 20.67s \nEpoch: 26, Train Loss: 0.5912, Train Acc: 82.33%, Val Loss:  1.495, Val Acc: 67.06%, Time: 20.58s *\nEpoch: 27, Train Loss: 0.5744, Train Acc: 83.17%, Val Loss:  1.642, Val Acc: 66.72%, Time: 20.80s \nEpoch: 28, Train Loss:  0.585, Train Acc: 82.30%, Val Loss:  2.049, Val Acc: 65.71%, Time: 20.53s \nEpoch: 29, Train Loss: 0.5571, Train Acc: 83.62%, Val Loss:   1.86, Val Acc: 66.05%, Time: 20.58s \nEpoch: 30, Train Loss: 0.5643, Train Acc: 83.22%, Val Loss:  2.174, Val Acc: 65.55%, Time: 20.36s \nEpoch: 31, Train Loss: 0.5607, Train Acc: 83.72%, Val Loss:  2.161, Val Acc: 62.86%, Time: 20.75s \nEpoch: 32, Train Loss: 0.5206, Train Acc: 84.08%, Val Loss:  1.826, Val Acc: 69.08%, Time: 20.84s *\nEpoch: 33, Train Loss: 0.5264, Train Acc: 84.87%, Val Loss:  2.051, Val Acc: 63.36%, Time: 20.53s \nEpoch: 34, Train Loss: 0.5219, Train Acc: 85.16%, Val Loss:  1.908, Val Acc: 66.55%, Time: 20.70s \nEpoch: 35, Train Loss: 0.5215, Train Acc: 84.86%, Val Loss:  1.922, Val Acc: 66.05%, Time: 20.73s \nEpoch: 36, Train Loss: 0.5091, Train Acc: 84.91%, Val Loss:  1.937, Val Acc: 65.38%, Time: 20.64s \n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 37, Train Loss: 0.5334, Train Acc: 84.67%, Val Loss:  2.107, Val Acc: 65.04%, Time: 20.68s \nEpoch: 38, Train Loss: 0.5145, Train Acc: 84.69%, Val Loss:  2.019, Val Acc: 67.56%, Time: 20.84s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         7\n        DESC       0.52      0.68      0.59       127\n        ENTY       0.70      0.56      0.62       151\n         HUM       0.72      0.64      0.68       118\n         LOC       0.59      0.66      0.63       100\n         NUM       0.83      0.83      0.83        92\n\n   micro avg       0.65      0.65      0.65       595\n   macro avg       0.56      0.56      0.56       595\nweighted avg       0.66      0.65      0.65       595\n\nConfusion Matrix...\n[[ 0  7  0  0  0  0]\n [ 0 86 12  5 19  5]\n [ 0 28 85 22 12  4]\n [ 0 19 15 76  6  2]\n [ 0 21  7  1 66  5]\n [ 0  5  2  1  8 76]]\nFold:  6\nEpoch: 1, Train Loss:  1.771, Train Acc: 22.15%, Val Loss:  1.738, Val Acc: 20.84%, Time: 24.20s *\nEpoch: 2, Train Loss:  1.683, Train Acc: 21.42%, Val Loss:  1.703, Val Acc: 22.18%, Time: 20.64s *\nEpoch: 3, Train Loss:   1.64, Train Acc: 25.16%, Val Loss:  1.604, Val Acc: 27.39%, Time: 20.96s *\nEpoch: 4, Train Loss:   1.49, Train Acc: 33.73%, Val Loss:  1.482, Val Acc: 34.45%, Time: 20.55s *\nEpoch: 5, Train Loss:  1.367, Train Acc: 44.08%, Val Loss:  1.428, Val Acc: 41.85%, Time: 20.68s *\nEpoch: 6, Train Loss:  1.186, Train Acc: 53.71%, Val Loss:  1.313, Val Acc: 46.39%, Time: 20.53s *\nEpoch: 7, Train Loss:  1.069, Train Acc: 57.67%, Val Loss:  1.269, Val Acc: 49.75%, Time: 20.72s *\nEpoch: 8, Train Loss: 0.9882, Train Acc: 61.53%, Val Loss:  1.273, Val Acc: 52.27%, Time: 20.64s *\nEpoch: 9, Train Loss: 0.9172, Train Acc: 65.29%, Val Loss:  1.357, Val Acc: 54.62%, Time: 20.65s *\nEpoch: 10, Train Loss: 0.8468, Train Acc: 68.88%, Val Loss:  1.307, Val Acc: 54.29%, Time: 20.70s \nEpoch: 11, Train Loss: 0.8165, Train Acc: 71.81%, Val Loss:  1.329, Val Acc: 58.15%, Time: 20.52s *\nEpoch: 12, Train Loss: 0.7784, Train Acc: 73.35%, Val Loss:  1.429, Val Acc: 57.48%, Time: 20.66s \nEpoch: 13, Train Loss: 0.7597, Train Acc: 74.72%, Val Loss:  1.483, Val Acc: 57.82%, Time: 20.65s \nEpoch: 14, Train Loss: 0.7124, Train Acc: 76.80%, Val Loss:   1.52, Val Acc: 61.51%, Time: 20.68s *\nEpoch: 15, Train Loss: 0.6765, Train Acc: 78.38%, Val Loss:  1.578, Val Acc: 60.00%, Time: 20.64s \nEpoch: 16, Train Loss: 0.6625, Train Acc: 79.08%, Val Loss:  1.555, Val Acc: 60.34%, Time: 20.48s \nEpoch: 17, Train Loss: 0.6361, Train Acc: 79.75%, Val Loss:  1.472, Val Acc: 62.02%, Time: 20.63s *\nEpoch: 18, Train Loss: 0.6192, Train Acc: 81.47%, Val Loss:  1.413, Val Acc: 61.68%, Time: 20.45s \nEpoch: 19, Train Loss: 0.6109, Train Acc: 81.27%, Val Loss:  1.759, Val Acc: 62.02%, Time: 20.83s \nEpoch: 20, Train Loss: 0.5965, Train Acc: 82.12%, Val Loss:  1.609, Val Acc: 64.03%, Time: 20.70s *\nEpoch: 21, Train Loss: 0.5728, Train Acc: 82.48%, Val Loss:   1.63, Val Acc: 61.68%, Time: 20.42s \nEpoch: 22, Train Loss:  0.585, Train Acc: 81.90%, Val Loss:  1.726, Val Acc: 62.86%, Time: 20.60s \nEpoch: 23, Train Loss: 0.5751, Train Acc: 82.52%, Val Loss:   1.72, Val Acc: 63.36%, Time: 20.85s \nEpoch: 24, Train Loss: 0.5306, Train Acc: 84.21%, Val Loss:  2.022, Val Acc: 59.83%, Time: 20.81s \nEpoch: 25, Train Loss: 0.5572, Train Acc: 83.01%, Val Loss:  2.026, Val Acc: 62.02%, Time: 20.52s \nEpoch: 26, Train Loss: 0.5256, Train Acc: 84.17%, Val Loss:  1.881, Val Acc: 60.00%, Time: 22.33s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00        14\n        DESC       0.48      0.49      0.48       119\n        ENTY       0.59      0.54      0.56       125\n         HUM       0.68      0.60      0.63       131\n         LOC       0.75      0.55      0.63       108\n         NUM       0.45      0.77      0.57        98\n\n   micro avg       0.57      0.57      0.57       595\n   macro avg       0.49      0.49      0.48       595\nweighted avg       0.58      0.57      0.56       595\n\nConfusion Matrix...\n[[ 0  4  0  0  0 10]\n [ 0 58 14  4  9 34]\n [ 0 21 67 23  6  8]\n [ 0  9 16 78  2 26]\n [ 0 19 12  6 59 12]\n [ 0 11  5  4  3 75]]\nFold:  7\nEpoch: 1, Train Loss:   1.77, Train Acc: 20.27%, Val Loss:  1.704, Val Acc: 21.85%, Time: 24.36s *\nEpoch: 2, Train Loss:  1.686, Train Acc: 20.66%, Val Loss:  1.686, Val Acc: 22.02%, Time: 20.77s *\nEpoch: 3, Train Loss:  1.684, Train Acc: 20.66%, Val Loss:  1.665, Val Acc: 24.20%, Time: 20.57s *\nEpoch: 4, Train Loss:  1.637, Train Acc: 27.50%, Val Loss:  1.616, Val Acc: 23.36%, Time: 20.72s \nEpoch: 5, Train Loss:  1.541, Train Acc: 33.53%, Val Loss:  1.506, Val Acc: 35.29%, Time: 20.85s *\nEpoch: 6, Train Loss:  1.458, Train Acc: 39.57%, Val Loss:  1.466, Val Acc: 38.32%, Time: 20.57s *\nEpoch: 7, Train Loss:  1.383, Train Acc: 42.54%, Val Loss:  1.455, Val Acc: 41.51%, Time: 20.50s *\nEpoch: 8, Train Loss:  1.304, Train Acc: 47.00%, Val Loss:  1.478, Val Acc: 39.50%, Time: 20.72s \nEpoch: 9, Train Loss:  1.216, Train Acc: 51.48%, Val Loss:  1.361, Val Acc: 44.87%, Time: 20.46s *\nEpoch: 10, Train Loss:   1.13, Train Acc: 55.03%, Val Loss:   1.36, Val Acc: 44.20%, Time: 20.77s \nEpoch: 11, Train Loss:  1.083, Train Acc: 56.24%, Val Loss:  1.279, Val Acc: 52.27%, Time: 23.79s *\nEpoch: 12, Train Loss:  1.044, Train Acc: 58.20%, Val Loss:  1.404, Val Acc: 44.71%, Time: 27.39s \nEpoch: 13, Train Loss: 0.9991, Train Acc: 59.96%, Val Loss:  1.383, Val Acc: 46.05%, Time: 27.65s \nEpoch: 14, Train Loss:  0.976, Train Acc: 60.81%, Val Loss:  1.394, Val Acc: 47.39%, Time: 27.61s \nEpoch: 15, Train Loss: 0.9621, Train Acc: 61.61%, Val Loss:  1.307, Val Acc: 49.24%, Time: 27.37s \nEpoch: 16, Train Loss: 0.9404, Train Acc: 61.95%, Val Loss:   1.36, Val Acc: 49.92%, Time: 27.01s \nEpoch: 17, Train Loss: 0.9101, Train Acc: 64.15%, Val Loss:  1.296, Val Acc: 52.27%, Time: 27.75s *\nEpoch: 18, Train Loss: 0.8985, Train Acc: 66.60%, Val Loss:  1.487, Val Acc: 50.08%, Time: 27.44s \nEpoch: 19, Train Loss:  0.871, Train Acc: 69.21%, Val Loss:  1.315, Val Acc: 52.10%, Time: 27.61s \nEpoch: 20, Train Loss: 0.8336, Train Acc: 70.14%, Val Loss:  1.457, Val Acc: 56.13%, Time: 27.40s *\nEpoch: 21, Train Loss: 0.8126, Train Acc: 72.91%, Val Loss:  1.377, Val Acc: 54.79%, Time: 27.88s \nEpoch: 22, Train Loss: 0.7931, Train Acc: 74.14%, Val Loss:  1.293, Val Acc: 57.31%, Time: 27.57s *\nEpoch: 23, Train Loss: 0.7707, Train Acc: 74.90%, Val Loss:  1.354, Val Acc: 58.66%, Time: 27.24s *\nEpoch: 24, Train Loss:   0.78, Train Acc: 74.83%, Val Loss:  1.426, Val Acc: 59.16%, Time: 27.37s *\nEpoch: 25, Train Loss: 0.7388, Train Acc: 76.67%, Val Loss:  1.386, Val Acc: 59.16%, Time: 27.41s \nEpoch: 26, Train Loss: 0.6993, Train Acc: 77.88%, Val Loss:  1.383, Val Acc: 60.34%, Time: 27.68s *\nEpoch: 27, Train Loss: 0.7008, Train Acc: 77.43%, Val Loss:  1.428, Val Acc: 56.30%, Time: 27.46s \nEpoch: 28, Train Loss: 0.6781, Train Acc: 78.41%, Val Loss:  1.378, Val Acc: 60.50%, Time: 27.44s *\nEpoch: 29, Train Loss:   0.67, Train Acc: 79.08%, Val Loss:  1.493, Val Acc: 62.02%, Time: 27.67s *\nEpoch: 30, Train Loss: 0.6643, Train Acc: 79.80%, Val Loss:  1.354, Val Acc: 59.83%, Time: 27.47s \nEpoch: 31, Train Loss: 0.6343, Train Acc: 80.49%, Val Loss:  1.435, Val Acc: 61.68%, Time: 27.64s \nEpoch: 32, Train Loss: 0.6355, Train Acc: 80.81%, Val Loss:  1.449, Val Acc: 57.82%, Time: 27.31s \nEpoch: 33, Train Loss: 0.5929, Train Acc: 81.95%, Val Loss:  1.531, Val Acc: 61.18%, Time: 27.44s \nEpoch: 34, Train Loss: 0.6038, Train Acc: 81.84%, Val Loss:  1.456, Val Acc: 64.20%, Time: 27.83s *\nEpoch: 35, Train Loss: 0.6001, Train Acc: 81.10%, Val Loss:  1.464, Val Acc: 63.03%, Time: 27.80s \nEpoch: 36, Train Loss: 0.5799, Train Acc: 82.05%, Val Loss:  1.408, Val Acc: 60.67%, Time: 27.50s \nEpoch: 37, Train Loss: 0.5749, Train Acc: 82.95%, Val Loss:  1.455, Val Acc: 61.34%, Time: 27.64s \nEpoch: 38, Train Loss: 0.5783, Train Acc: 82.37%, Val Loss:  1.506, Val Acc: 58.49%, Time: 27.94s \nEpoch: 39, Train Loss: 0.5428, Train Acc: 83.95%, Val Loss:  1.491, Val Acc: 64.37%, Time: 25.80s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 40, Train Loss: 0.5595, Train Acc: 82.84%, Val Loss:  1.641, Val Acc: 60.67%, Time: 27.35s \nEpoch: 41, Train Loss: 0.5582, Train Acc: 83.25%, Val Loss:  1.559, Val Acc: 64.20%, Time: 27.27s \nEpoch: 42, Train Loss: 0.5558, Train Acc: 83.53%, Val Loss:  1.484, Val Acc: 64.87%, Time: 27.48s *\nEpoch: 43, Train Loss: 0.5359, Train Acc: 84.01%, Val Loss:  1.584, Val Acc: 62.86%, Time: 27.47s \nEpoch: 44, Train Loss: 0.5306, Train Acc: 84.29%, Val Loss:  1.686, Val Acc: 59.33%, Time: 27.46s \nEpoch: 45, Train Loss: 0.5062, Train Acc: 84.85%, Val Loss:  1.646, Val Acc: 62.18%, Time: 27.64s \nEpoch: 46, Train Loss: 0.5024, Train Acc: 84.87%, Val Loss:  1.604, Val Acc: 64.87%, Time: 27.31s \nEpoch: 47, Train Loss: 0.5245, Train Acc: 84.78%, Val Loss:  1.524, Val Acc: 65.04%, Time: 27.93s *\nEpoch: 48, Train Loss: 0.5175, Train Acc: 84.65%, Val Loss:  1.804, Val Acc: 63.87%, Time: 27.62s \nEpoch: 49, Train Loss: 0.5182, Train Acc: 85.09%, Val Loss:  1.597, Val Acc: 66.22%, Time: 27.84s *\nEpoch: 50, Train Loss:  0.511, Train Acc: 84.56%, Val Loss:  1.742, Val Acc: 63.36%, Time: 27.59s \nEpoch: 51, Train Loss: 0.4945, Train Acc: 85.83%, Val Loss:  1.717, Val Acc: 62.69%, Time: 27.67s \nEpoch: 52, Train Loss:  0.479, Train Acc: 85.52%, Val Loss:  1.881, Val Acc: 62.86%, Time: 27.62s \nEpoch: 53, Train Loss: 0.4756, Train Acc: 86.06%, Val Loss:  1.908, Val Acc: 60.17%, Time: 27.53s \nEpoch: 54, Train Loss: 0.4902, Train Acc: 85.83%, Val Loss:  1.419, Val Acc: 64.20%, Time: 27.76s \nEpoch: 55, Train Loss: 0.4875, Train Acc: 84.74%, Val Loss:   1.77, Val Acc: 61.18%, Time: 27.45s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         9\n        DESC       0.69      0.51      0.59       139\n        ENTY       0.68      0.53      0.60       133\n         HUM       0.54      0.74      0.62       123\n         LOC       0.59      0.69      0.64        91\n         NUM       0.73      0.81      0.77       100\n\n   micro avg       0.63      0.63      0.63       595\n   macro avg       0.54      0.55      0.54       595\nweighted avg       0.64      0.63      0.63       595\n\nConfusion Matrix...\n[[ 0  3  0  2  4  0]\n [ 0 71 19 22 24  3]\n [ 0 19 71 32  8  3]\n [ 0  2 14 91  2 14]\n [ 0  6  1 11 63 10]\n [ 0  2  0 11  6 81]]\nFold:  8\nEpoch: 1, Train Loss:  1.705, Train Acc: 21.52%, Val Loss:  1.681, Val Acc: 22.52%, Time: 31.68s *\nEpoch: 2, Train Loss:  1.591, Train Acc: 28.56%, Val Loss:  1.556, Val Acc: 31.76%, Time: 27.50s *\nEpoch: 3, Train Loss:  1.432, Train Acc: 38.32%, Val Loss:  1.495, Val Acc: 35.13%, Time: 27.13s *\nEpoch: 4, Train Loss:  1.312, Train Acc: 42.68%, Val Loss:  1.435, Val Acc: 39.50%, Time: 27.58s *\nEpoch: 5, Train Loss:  1.232, Train Acc: 46.99%, Val Loss:  1.504, Val Acc: 43.53%, Time: 27.33s *\nEpoch: 6, Train Loss:  1.168, Train Acc: 51.32%, Val Loss:  1.404, Val Acc: 44.71%, Time: 27.89s *\nEpoch: 7, Train Loss:  1.105, Train Acc: 54.14%, Val Loss:  1.538, Val Acc: 45.71%, Time: 25.86s *\nEpoch: 8, Train Loss:  1.063, Train Acc: 56.24%, Val Loss:   1.46, Val Acc: 47.23%, Time: 27.48s *\nEpoch: 9, Train Loss: 0.9999, Train Acc: 58.48%, Val Loss:  1.541, Val Acc: 49.41%, Time: 27.37s *\nEpoch: 10, Train Loss: 0.9305, Train Acc: 64.78%, Val Loss:  1.532, Val Acc: 55.80%, Time: 27.57s *\nEpoch: 11, Train Loss:   0.89, Train Acc: 68.25%, Val Loss:  1.453, Val Acc: 56.81%, Time: 27.43s *\nEpoch: 12, Train Loss: 0.8433, Train Acc: 70.53%, Val Loss:  1.643, Val Acc: 59.50%, Time: 27.50s *\nEpoch: 13, Train Loss: 0.8021, Train Acc: 72.40%, Val Loss:  1.746, Val Acc: 63.53%, Time: 27.75s *\nEpoch: 14, Train Loss: 0.7614, Train Acc: 73.86%, Val Loss:  1.828, Val Acc: 58.32%, Time: 27.34s \nEpoch: 15, Train Loss: 0.7508, Train Acc: 75.50%, Val Loss:  1.636, Val Acc: 61.34%, Time: 27.69s \nEpoch: 16, Train Loss: 0.7138, Train Acc: 75.93%, Val Loss:  1.681, Val Acc: 60.34%, Time: 27.27s \nEpoch: 17, Train Loss: 0.7011, Train Acc: 76.55%, Val Loss:  1.851, Val Acc: 59.66%, Time: 27.49s \nEpoch: 18, Train Loss: 0.6813, Train Acc: 78.37%, Val Loss:  1.801, Val Acc: 61.01%, Time: 27.36s \nEpoch: 19, Train Loss: 0.6479, Train Acc: 79.09%, Val Loss:  1.812, Val Acc: 61.34%, Time: 27.90s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00         9\n        DESC       0.54      0.65      0.59       124\n        ENTY       0.68      0.65      0.67       141\n         HUM       0.74      0.68      0.71       117\n         LOC       0.72      0.55      0.62        99\n         NUM       0.65      0.79      0.72       105\n\n   micro avg       0.66      0.66      0.66       595\n   macro avg       0.56      0.55      0.55       595\nweighted avg       0.65      0.66      0.65       595\n\nConfusion Matrix...\n[[ 0  1  2  0  0  6]\n [ 0 81 13  3  7 20]\n [ 0 18 92 15  9  7]\n [ 0 13 18 80  3  3]\n [ 0 27  7  3 54  8]\n [ 0 10  3  7  2 83]]\nFold:  9\nEpoch: 1, Train Loss:  1.784, Train Acc: 21.29%, Val Loss:  1.713, Val Acc: 20.84%, Time: 30.87s *\nEpoch: 2, Train Loss:  1.687, Train Acc: 21.37%, Val Loss:  1.687, Val Acc: 23.87%, Time: 27.29s *\nEpoch: 3, Train Loss:  1.679, Train Acc: 22.07%, Val Loss:  1.673, Val Acc: 19.66%, Time: 27.24s \nEpoch: 4, Train Loss:  1.647, Train Acc: 25.01%, Val Loss:  1.575, Val Acc: 32.10%, Time: 28.25s *\nEpoch: 5, Train Loss:  1.456, Train Acc: 40.18%, Val Loss:  1.388, Val Acc: 42.18%, Time: 27.45s *\nEpoch: 6, Train Loss:  1.307, Train Acc: 46.01%, Val Loss:  1.337, Val Acc: 43.36%, Time: 27.61s *\nEpoch: 7, Train Loss:  1.209, Train Acc: 51.62%, Val Loss:  1.333, Val Acc: 43.70%, Time: 27.76s *\nEpoch: 8, Train Loss:  1.144, Train Acc: 53.55%, Val Loss:  1.383, Val Acc: 44.87%, Time: 27.91s *\nEpoch: 9, Train Loss:  1.092, Train Acc: 56.30%, Val Loss:  1.367, Val Acc: 46.55%, Time: 27.74s *\nEpoch: 10, Train Loss:  1.052, Train Acc: 58.54%, Val Loss:  1.346, Val Acc: 47.39%, Time: 27.46s *\nEpoch: 11, Train Loss:  1.025, Train Acc: 59.05%, Val Loss:  1.431, Val Acc: 46.72%, Time: 27.84s \nEpoch: 12, Train Loss: 0.9964, Train Acc: 60.45%, Val Loss:  1.499, Val Acc: 47.39%, Time: 27.53s \nEpoch: 13, Train Loss:  0.974, Train Acc: 60.75%, Val Loss:  1.387, Val Acc: 48.07%, Time: 27.76s *\nEpoch: 14, Train Loss: 0.9238, Train Acc: 64.06%, Val Loss:  1.412, Val Acc: 53.28%, Time: 27.61s *\nEpoch: 15, Train Loss: 0.9231, Train Acc: 65.52%, Val Loss:   1.52, Val Acc: 55.29%, Time: 27.75s *\nEpoch: 16, Train Loss: 0.9106, Train Acc: 68.22%, Val Loss:  1.543, Val Acc: 55.80%, Time: 25.88s *\nEpoch: 17, Train Loss: 0.8719, Train Acc: 70.30%, Val Loss:  1.454, Val Acc: 56.81%, Time: 26.95s *\nEpoch: 18, Train Loss: 0.8574, Train Acc: 71.95%, Val Loss:  1.412, Val Acc: 55.63%, Time: 27.33s \nEpoch: 19, Train Loss: 0.8635, Train Acc: 71.26%, Val Loss:  1.605, Val Acc: 56.64%, Time: 27.56s \nEpoch: 20, Train Loss: 0.8331, Train Acc: 73.64%, Val Loss:  1.503, Val Acc: 58.82%, Time: 27.42s *\nEpoch: 21, Train Loss: 0.7969, Train Acc: 74.10%, Val Loss:  1.521, Val Acc: 56.13%, Time: 27.67s \nEpoch: 22, Train Loss: 0.7742, Train Acc: 75.75%, Val Loss:  1.641, Val Acc: 55.29%, Time: 26.65s \nEpoch: 23, Train Loss: 0.7537, Train Acc: 75.61%, Val Loss:  1.609, Val Acc: 58.32%, Time: 26.91s \nEpoch: 24, Train Loss: 0.7335, Train Acc: 77.16%, Val Loss:  1.756, Val Acc: 57.31%, Time: 27.94s \nEpoch: 25, Train Loss: 0.7415, Train Acc: 77.26%, Val Loss:  1.678, Val Acc: 56.97%, Time: 27.25s \nEpoch: 26, Train Loss: 0.7279, Train Acc: 78.05%, Val Loss:  1.677, Val Acc: 57.82%, Time: 27.91s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.00      0.00      0.00        13\n        DESC       0.71      0.48      0.57       135\n        ENTY       0.47      0.50      0.48       119\n         HUM       0.75      0.58      0.65       149\n         LOC       0.79      0.58      0.67        72\n         NUM       0.43      0.82      0.56       107\n\n   micro avg       0.57      0.57      0.57       595\n   macro avg       0.52      0.49      0.49       595\nweighted avg       0.61      0.57      0.57       595\n\nConfusion Matrix...\n[[ 0  2  1  0  0 10]\n [ 0 65 15  5  4 46]\n [ 0  3 60 10  5 41]\n [ 0 10 32 86  2 19]\n [ 0  4 12 11 42  3]\n [ 0  8  9  2  0 88]]\nFold:  10\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 1, Train Loss:  1.693, Train Acc: 20.82%, Val Loss:  1.652, Val Acc: 22.35%, Time: 30.96s *\nEpoch: 2, Train Loss:  1.667, Train Acc: 21.70%, Val Loss:  1.628, Val Acc: 25.04%, Time: 27.24s *\nEpoch: 3, Train Loss:  1.606, Train Acc: 28.83%, Val Loss:  1.521, Val Acc: 35.29%, Time: 27.71s *\nEpoch: 4, Train Loss:  1.458, Train Acc: 36.75%, Val Loss:  1.425, Val Acc: 39.33%, Time: 27.77s *\nEpoch: 5, Train Loss:  1.289, Train Acc: 46.19%, Val Loss:  1.351, Val Acc: 44.20%, Time: 27.75s *\nEpoch: 6, Train Loss:  1.149, Train Acc: 54.20%, Val Loss:  1.273, Val Acc: 46.39%, Time: 28.06s *\nEpoch: 7, Train Loss:  1.061, Train Acc: 57.74%, Val Loss:  1.263, Val Acc: 50.76%, Time: 27.45s *\nEpoch: 8, Train Loss: 0.9961, Train Acc: 61.14%, Val Loss:  1.277, Val Acc: 53.45%, Time: 27.87s *\nEpoch: 9, Train Loss: 0.9458, Train Acc: 63.12%, Val Loss:  1.295, Val Acc: 54.29%, Time: 27.27s *\nEpoch: 10, Train Loss: 0.9106, Train Acc: 65.40%, Val Loss:  1.228, Val Acc: 55.46%, Time: 27.39s *\nEpoch: 11, Train Loss: 0.8852, Train Acc: 66.62%, Val Loss:  1.464, Val Acc: 55.46%, Time: 27.96s *\nEpoch: 12, Train Loss: 0.8715, Train Acc: 67.52%, Val Loss:  1.337, Val Acc: 52.94%, Time: 27.90s \nEpoch: 13, Train Loss: 0.8242, Train Acc: 69.32%, Val Loss:  1.341, Val Acc: 55.97%, Time: 27.51s *\nEpoch: 14, Train Loss: 0.7674, Train Acc: 72.89%, Val Loss:  1.236, Val Acc: 58.82%, Time: 27.48s *\nEpoch: 15, Train Loss:  0.788, Train Acc: 71.95%, Val Loss:  1.442, Val Acc: 57.14%, Time: 27.47s \nEpoch: 16, Train Loss: 0.7394, Train Acc: 74.77%, Val Loss:  1.465, Val Acc: 58.99%, Time: 27.69s *\nEpoch: 17, Train Loss: 0.7283, Train Acc: 76.04%, Val Loss:  1.524, Val Acc: 60.50%, Time: 27.79s *\nEpoch: 18, Train Loss:  0.708, Train Acc: 77.45%, Val Loss:  1.395, Val Acc: 63.03%, Time: 27.79s *\nEpoch: 19, Train Loss: 0.6955, Train Acc: 78.17%, Val Loss:   1.51, Val Acc: 62.02%, Time: 27.55s \nEpoch: 20, Train Loss:  0.684, Train Acc: 77.90%, Val Loss:  1.493, Val Acc: 61.01%, Time: 27.47s \nEpoch: 21, Train Loss: 0.6516, Train Acc: 80.00%, Val Loss:  1.485, Val Acc: 61.34%, Time: 27.60s \nEpoch: 22, Train Loss: 0.6452, Train Acc: 80.01%, Val Loss:  1.604, Val Acc: 62.86%, Time: 27.36s \nEpoch: 23, Train Loss:  0.641, Train Acc: 80.27%, Val Loss:  1.723, Val Acc: 63.36%, Time: 27.98s *\nEpoch: 24, Train Loss: 0.6054, Train Acc: 81.09%, Val Loss:  1.606, Val Acc: 63.03%, Time: 27.85s \nEpoch: 25, Train Loss: 0.5967, Train Acc: 81.66%, Val Loss:  1.588, Val Acc: 63.03%, Time: 27.64s \nEpoch: 26, Train Loss: 0.5773, Train Acc: 81.77%, Val Loss:  1.594, Val Acc: 61.34%, Time: 27.73s \n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "raw", "source": "classifier.train(\n    X_train=X_train,\n    y_train=y_train,\n    X_eval=X_eval,\n    y_eval=y_eval,\n    epochs=30\n)"}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}