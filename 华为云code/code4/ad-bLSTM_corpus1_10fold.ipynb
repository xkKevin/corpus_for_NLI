{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import shuffle\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_5_for_train.txt\"\ntestDataPath = \"s3://corpus-2/dataset/corpus_5_new.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_info = {\n    \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\n\ndef dataset_split(info):\n    if info:\n        [num, pi] = info\n        train_data = [[] for i in range(num)]\n        test_data = [[] for i in range(num)]\n        \n        with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                train_data[index].append([content,label])\n                \n        with open(testDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                test_data[index].append([content,label])\n\n        for i in range(num):\n            # np.random.shuffle(train_data[i])\n            train_data[i], test_data[i] = shuffle(train_data[i], test_data[i])\n            train_data[i] = np.asarray(train_data[i])\n            test_data[i] = np.asarray(test_data[i])\n\n        train_data, test_data = shuffle(train_data, test_data)\n        return train_data, test_data\n    \n    \n    train_data = []\n    test_data = []\n    \n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n            \n    with open(testDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            test_data.append([content,label])\n            \n    train_data, test_data = shuffle(train_data, test_data)\n    return np.asarray(train_data), np.asarray(test_data)\n\n\ndef mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(model, train_data, test_data, split_type):\n    \n    print(split_type)\n    \n    test_acc = []\n    fold_id = 0\n    \n    if split_type != \"random\":\n        tx = []\n        ty = []\n        for ti in train_data:\n            x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            tx.append(x_train)\n            ty.append(y_train)\n\n        tx = np.asarray(tx)\n        ty = np.asarray(ty)\n\n        te_x = []\n        te_y = []\n        for ti in test_data:\n            x_test, y_test = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            te_x.append(x_test)\n            te_y.append(y_test)\n\n        te_x = np.asarray(te_x)\n        te_y = np.asarray(te_y)\n        \n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_x, train_y = mergeData(tx[train_i],ty[train_i])\n            test_x, test_y = mergeData(te_x[test_i],te_y[test_i])\n            test_acc.append(model_train(model, train_x, train_y, test_x, test_y, categories))\n        \n    else:\n        tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n        te_x, te_y = process_file(test_data[:,0], test_data[:,1], word_to_id, num_classes, seq_length)\n        # print(len(tx),len(tx[0]),len(tx[1]))\n\n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            test_acc.append(model_train(model,tx[train_i], ty[train_i],te_x[test_i], te_y[test_i], categories))\n        \n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\nprint(len(embedding),embedding_dim,vocab_size)\n \nseq_length = 41  # seq_length = 37  TREC", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, x_val, y_val, categories):\n    \n    # save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n\n    print('Training and evaluating...')\n    \n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 400  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    print_per_batch = 100\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        start_time = time.time()\n        \n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.dropoutKeepProb] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, model, x_val, y_val, model.loss, model.acc, 64)\n                if acc_val > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_val\n                    last_improved = total_batch\n                    # saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n                \n                duration = time.time() - start_time\n                output = 'Iter: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n                print(output.format(total_batch, loss_train, acc_train, loss_val, acc_val, duration, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                test_data_len = len(x_val)\n                test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n                y_test_cls = np.argmax(y_val, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.zeros(shape=len(x_val), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n                for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n                    start_id = i * batch_size\n                    end_id = min((i + 1) * batch_size, test_data_len)\n                    feed_dict = {\n                        model.inputX: x_val[start_id:end_id],\n                        model.dropoutKeepProb: 1.0\n                    }\n                    y_test_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                # \u8bc4\u4f30\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                # \u6df7\u6dc6\u77e9\u9635\n                print(\"Confusion Matrix...\")\n                cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                print(cm)\n                \n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return accuracy_score", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            wordEmbedding = tf.Variable(initial_value=wordEmbedding)\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nlstm = AdversarailLSTM(embedding)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:171: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-7-a79079a13536>:172: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:128\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nsplit_info = {\n    # \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "expert\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.129, Train Acc: 53.12%, Val Loss:  3.234, Val Acc: 50.09%, Time: 31.01s *\nIter: 200, Train Loss:  2.312, Train Acc: 59.38%, Val Loss:  2.217, Val Acc: 64.58%, Time: 61.12s *\nEpoch: 2\nIter: 300, Train Loss:   1.22, Train Acc: 79.69%, Val Loss:  1.917, Val Acc: 70.84%, Time: 29.68s *\nIter: 400, Train Loss:  1.367, Train Acc: 76.56%, Val Loss:  1.744, Val Acc: 73.88%, Time: 59.87s *\nEpoch: 3\nIter: 500, Train Loss: 0.8094, Train Acc: 92.19%, Val Loss:  1.696, Val Acc: 75.31%, Time: 29.08s *\nIter: 600, Train Loss: 0.3819, Train Acc: 96.88%, Val Loss:  1.618, Val Acc: 75.13%, Time: 59.39s \nEpoch: 4\nIter: 700, Train Loss: 0.9812, Train Acc: 87.50%, Val Loss:  1.595, Val Acc: 74.78%, Time: 28.63s \nIter: 800, Train Loss: 0.7281, Train Acc: 89.06%, Val Loss:  1.679, Val Acc: 74.15%, Time: 58.99s \nEpoch: 5\nIter: 900, Train Loss: 0.6569, Train Acc: 85.94%, Val Loss:  1.783, Val Acc: 76.03%, Time: 27.93s *\nIter: 1000, Train Loss: 0.3363, Train Acc: 98.44%, Val Loss:  1.635, Val Acc: 74.60%, Time: 58.23s \nEpoch: 6\nIter: 1100, Train Loss: 0.3296, Train Acc: 95.31%, Val Loss:  1.848, Val Acc: 74.33%, Time: 27.34s \nIter: 1200, Train Loss: 0.2037, Train Acc: 100.00%, Val Loss:   1.66, Val Acc: 75.49%, Time: 57.74s \nEpoch: 7\nIter: 1300, Train Loss:  0.407, Train Acc: 95.31%, Val Loss:  1.888, Val Acc: 73.43%, Time: 26.86s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.60      0.44      0.51       126\n                   Filter       0.60      0.72      0.65       115\n    Compute Derived Value       0.67      0.87      0.75       120\n            Find Extremum       0.93      0.78      0.84       112\n                     Sort       1.00      0.81      0.89       120\n          Determine Range       0.67      0.66      0.67        95\nCharacterize Distribution       0.91      0.71      0.80       111\n           Find Anomalies       0.86      0.82      0.84       107\n                  Cluster       0.58      0.71      0.64       103\n                Correlate       0.74      0.88      0.80       109\n\n                micro avg       0.74      0.74      0.74      1118\n                macro avg       0.75      0.74      0.74      1118\n             weighted avg       0.76      0.74      0.74      1118\n\nConfusion Matrix...\n[[ 56  20  35   0   0   4   0   0   6   5]\n [ 10  83   1   4   0  15   0   0   2   0]\n [  5   5 104   1   0   3   0   1   1   0]\n [  4   4   1  87   0   1   0   7   3   5]\n [  0   4   0   0  97   0   0   0  19   0]\n [ 14   7   5   0   0  63   2   0   4   0]\n [  1  13   1   0   0   3  79   1  10   3]\n [  0   3   0   1   0   4   5  88   5   1]\n [  1   0   4   0   0   0   1   4  73  20]\n [  3   0   5   1   0   1   0   1   2  96]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.931, Train Acc: 60.94%, Val Loss:  3.379, Val Acc: 46.00%, Time: 31.67s *\nEpoch: 2\nIter: 200, Train Loss:  2.152, Train Acc: 75.00%, Val Loss:  2.549, Val Acc: 56.99%, Time: 3.93s *\nIter: 300, Train Loss:  1.469, Train Acc: 73.44%, Val Loss:  2.182, Val Acc: 63.43%, Time: 34.83s *\nEpoch: 3\nIter: 400, Train Loss:  1.044, Train Acc: 87.50%, Val Loss:   2.12, Val Acc: 66.47%, Time: 5.96s *\nIter: 500, Train Loss:  1.006, Train Acc: 85.94%, Val Loss:  2.021, Val Acc: 68.26%, Time: 36.88s *\nEpoch: 4\nIter: 600, Train Loss: 0.9251, Train Acc: 85.94%, Val Loss:  2.169, Val Acc: 67.40%, Time: 8.05s \nIter: 700, Train Loss: 0.4544, Train Acc: 90.62%, Val Loss:  1.973, Val Acc: 70.27%, Time: 39.04s *\nEpoch: 5\nIter: 800, Train Loss: 0.7652, Train Acc: 89.06%, Val Loss:  2.174, Val Acc: 69.64%, Time: 10.02s \nIter: 900, Train Loss: 0.3923, Train Acc: 95.31%, Val Loss:  2.123, Val Acc: 70.10%, Time: 41.05s \nEpoch: 6\nIter: 1000, Train Loss: 0.7132, Train Acc: 93.75%, Val Loss:  2.137, Val Acc: 70.39%, Time: 12.05s *\nIter: 1100, Train Loss: 0.1784, Train Acc: 98.44%, Val Loss:  2.244, Val Acc: 70.04%, Time: 43.10s \nEpoch: 7\nIter: 1200, Train Loss: 0.1909, Train Acc: 96.88%, Val Loss:  2.255, Val Acc: 70.56%, Time: 14.14s *\nIter: 1300, Train Loss: 0.5371, Train Acc: 93.75%, Val Loss:  2.314, Val Acc: 69.52%, Time: 45.25s \nEpoch: 8\nIter: 1400, Train Loss: 0.1822, Train Acc: 98.44%, Val Loss:  2.462, Val Acc: 69.29%, Time: 16.16s \nIter: 1500, Train Loss: 0.1142, Train Acc: 98.44%, Val Loss:  2.181, Val Acc: 70.39%, Time: 47.27s \nEpoch: 9\nIter: 1600, Train Loss: 0.1187, Train Acc: 98.44%, Val Loss:  2.482, Val Acc: 69.06%, Time: 18.17s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.49      0.52       199\n                   Filter       0.49      0.29      0.37       170\n    Compute Derived Value       0.64      0.58      0.61       255\n            Find Extremum       0.82      0.91      0.87       181\n                     Sort       0.81      0.73      0.77       118\n          Determine Range       0.61      0.85      0.71       203\nCharacterize Distribution       0.72      0.89      0.80       110\n           Find Anomalies       0.64      0.93      0.76       169\n                  Cluster       0.86      0.82      0.84       148\n                Correlate       0.83      0.54      0.66       186\n\n                micro avg       0.69      0.69      0.69      1739\n                macro avg       0.70      0.70      0.69      1739\n             weighted avg       0.69      0.69      0.68      1739\n\nConfusion Matrix...\n[[ 97   8  45   2   9  25   5   3   4   1]\n [ 47  50   9   4   0  38   8  12   0   2]\n [ 21   2 148  23   1  30   6  12   6   6]\n [  1   7   4 165   0   2   0   2   0   0]\n [  0   4   3   2  86  11   1   9   2   0]\n [  2  11   2   0   3 172   8   2   3   0]\n [  3   0   2   0   3   2  98   0   2   0]\n [  0   5   3   3   0   0   0 157   0   1]\n [  0   6   0   1   1   0   1   7 122  10]\n [  6   9  14   0   3   1   9  40   3 101]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.927, Train Acc: 56.25%, Val Loss:  3.336, Val Acc: 43.63%, Time: 31.23s *\nIter: 200, Train Loss:  1.709, Train Acc: 75.00%, Val Loss:  2.602, Val Acc: 59.46%, Time: 61.45s *\nEpoch: 2\nIter: 300, Train Loss:   1.37, Train Acc: 79.69%, Val Loss:   2.26, Val Acc: 63.63%, Time: 30.49s *\nIter: 400, Train Loss: 0.08893, Train Acc: 100.00%, Val Loss:  2.531, Val Acc: 62.24%, Time: 60.75s \nEpoch: 3\nIter: 500, Train Loss: 0.7242, Train Acc: 85.94%, Val Loss:  2.511, Val Acc: 63.94%, Time: 30.61s *\nIter: 600, Train Loss: 0.3699, Train Acc: 100.00%, Val Loss:  2.656, Val Acc: 64.86%, Time: 60.88s *\nEpoch: 4\nIter: 700, Train Loss: 0.8925, Train Acc: 90.62%, Val Loss:  2.587, Val Acc: 65.25%, Time: 30.52s *\nIter: 800, Train Loss: 0.7397, Train Acc: 75.00%, Val Loss:   2.48, Val Acc: 66.64%, Time: 60.84s *\nEpoch: 5\nIter: 900, Train Loss: 0.5097, Train Acc: 90.62%, Val Loss:  2.864, Val Acc: 65.48%, Time: 30.60s \nIter: 1000, Train Loss: 0.05428, Train Acc: 100.00%, Val Loss:  2.928, Val Acc: 63.55%, Time: 60.81s \nEpoch: 6\nIter: 1100, Train Loss: 0.3203, Train Acc: 93.75%, Val Loss:  3.093, Val Acc: 64.17%, Time: 30.56s \nIter: 1200, Train Loss: 0.04318, Train Acc: 100.00%, Val Loss:  3.271, Val Acc: 63.63%, Time: 60.67s \nEpoch: 7\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.86      0.42      0.57       121\n                   Filter       0.60      0.59      0.60       144\n    Compute Derived Value       0.44      0.57      0.50       124\n            Find Extremum       0.46      0.94      0.61       126\n                     Sort       0.81      0.86      0.83       122\n          Determine Range       0.79      0.62      0.69       126\nCharacterize Distribution       0.74      0.69      0.71       128\n           Find Anomalies       0.72      0.64      0.68       142\n                  Cluster       0.73      0.57      0.64       141\n                Correlate       0.63      0.47      0.54       121\n\n                micro avg       0.64      0.64      0.64      1295\n                macro avg       0.68      0.64      0.64      1295\n             weighted avg       0.68      0.64      0.64      1295\n\nConfusion Matrix...\n[[ 51   4  40   5   7   0  12   1   1   0]\n [  1  85  24  15   0   8   0   5   6   0]\n [  1   0  71  34   9   0   2   5   2   0]\n [  0   3   2 118   2   0   0   1   0   0]\n [  2   0   5   0 105   0   3   0   7   0]\n [  0   4   3  33   2  78   3   1   2   0]\n [  0   0   5  15   2   1  88   7   8   2]\n [  0   8   8  14   0   0   6  91   1  14]\n [  4  21   0   2   0  12   5   0  80  17]\n [  0  16   4  23   3   0   0  15   3  57]]\nFold:  4\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.705, Train Acc: 67.19%, Val Loss:  2.966, Val Acc: 55.86%, Time: 32.02s *\nEpoch: 2\nIter: 200, Train Loss:  1.448, Train Acc: 84.38%, Val Loss:  2.353, Val Acc: 61.99%, Time: 6.08s *\nIter: 300, Train Loss:  0.856, Train Acc: 89.06%, Val Loss:  2.052, Val Acc: 66.92%, Time: 37.33s *\nEpoch: 3\nIter: 400, Train Loss: 0.9231, Train Acc: 87.50%, Val Loss:  2.174, Val Acc: 67.35%, Time: 9.79s *\nIter: 500, Train Loss:  0.776, Train Acc: 85.94%, Val Loss:  2.177, Val Acc: 68.17%, Time: 41.09s *\nEpoch: 4\nIter: 600, Train Loss: 0.7682, Train Acc: 89.06%, Val Loss:  2.432, Val Acc: 66.87%, Time: 13.66s \nIter: 700, Train Loss: 0.2672, Train Acc: 95.31%, Val Loss:  2.475, Val Acc: 68.21%, Time: 44.96s *\nEpoch: 5\nIter: 800, Train Loss: 0.3571, Train Acc: 93.75%, Val Loss:   2.66, Val Acc: 67.40%, Time: 17.40s \nIter: 900, Train Loss: 0.4479, Train Acc: 90.62%, Val Loss:  2.526, Val Acc: 69.84%, Time: 48.62s *\nEpoch: 6\nIter: 1000, Train Loss: 0.3402, Train Acc: 98.44%, Val Loss:  2.953, Val Acc: 67.64%, Time: 21.21s \nIter: 1100, Train Loss: 0.2879, Train Acc: 96.88%, Val Loss:  2.841, Val Acc: 68.41%, Time: 52.52s \nEpoch: 7\nIter: 1200, Train Loss: 0.09992, Train Acc: 100.00%, Val Loss:   3.03, Val Acc: 67.88%, Time: 25.01s \nIter: 1300, Train Loss: 0.06283, Train Acc: 100.00%, Val Loss:  2.769, Val Acc: 70.42%, Time: 56.20s *\nEpoch: 8\nIter: 1400, Train Loss: 0.09657, Train Acc: 100.00%, Val Loss:  2.777, Val Acc: 67.40%, Time: 28.81s \nEpoch: 9\nIter: 1500, Train Loss: 0.1389, Train Acc: 98.44%, Val Loss:   3.01, Val Acc: 70.13%, Time: 3.52s \nIter: 1600, Train Loss: 0.1486, Train Acc: 96.88%, Val Loss:  3.442, Val Acc: 67.40%, Time: 34.94s \nEpoch: 10\nIter: 1700, Train Loss: 0.06996, Train Acc: 100.00%, Val Loss:  3.391, Val Acc: 68.69%, Time: 7.23s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.57      0.70      0.62       227\n                   Filter       0.66      0.45      0.53       283\n    Compute Derived Value       0.55      0.71      0.62       205\n            Find Extremum       0.92      0.71      0.80       296\n                     Sort       0.73      0.79      0.76       145\n          Determine Range       0.61      0.79      0.69       138\nCharacterize Distribution       0.90      0.76      0.83       172\n           Find Anomalies       0.71      0.60      0.65       217\n                  Cluster       0.65      0.73      0.69       143\n                Correlate       0.70      0.79      0.74       263\n\n                micro avg       0.69      0.69      0.69      2089\n                macro avg       0.70      0.70      0.69      2089\n             weighted avg       0.71      0.69      0.69      2089\n\nConfusion Matrix...\n[[158   2   5   1   0   0   0  26   0  35]\n [ 28 126  47   8   1  29   0  15  18  11]\n [ 23  15 145   0   0   4   0   2   0  16]\n [ 23   6   2 211  26  18   1   8   1   0]\n [ 20   0   2   1 115   5   1   0   0   1]\n [  8   0   6   5   1 109   4   0   5   0]\n [  0   0  10   1   1   5 131   0  10  14]\n [ 11  22  16   0   0   5   1 130  21  11]\n [  7   5   7   1  12   3   0   1 105   2]\n [  1  15  23   1   1   2   7   2   2 209]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.753, Train Acc: 57.81%, Val Loss:  3.336, Val Acc: 48.41%, Time: 31.16s *\nIter: 200, Train Loss:  1.518, Train Acc: 76.56%, Val Loss:  2.338, Val Acc: 62.90%, Time: 61.45s *\nEpoch: 2\nIter: 300, Train Loss: 0.9972, Train Acc: 89.06%, Val Loss:  2.206, Val Acc: 68.10%, Time: 30.26s *\nIter: 400, Train Loss: 0.8077, Train Acc: 89.06%, Val Loss:  2.139, Val Acc: 67.29%, Time: 60.68s \nEpoch: 3\nIter: 500, Train Loss:  1.049, Train Acc: 89.06%, Val Loss:  2.152, Val Acc: 66.31%, Time: 29.95s \nIter: 600, Train Loss: 0.6992, Train Acc: 92.19%, Val Loss:  2.404, Val Acc: 66.80%, Time: 60.40s \nEpoch: 4\nIter: 700, Train Loss: 0.5151, Train Acc: 89.06%, Val Loss:  2.646, Val Acc: 66.88%, Time: 29.66s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.39      0.95      0.56       111\n                   Filter       0.45      0.32      0.38       133\n    Compute Derived Value       0.67      0.38      0.48       126\n            Find Extremum       0.86      0.66      0.75       123\n                     Sort       0.84      0.92      0.88       111\n          Determine Range       0.47      0.40      0.43       136\nCharacterize Distribution       0.87      0.66      0.75       120\n           Find Anomalies       0.88      0.68      0.76       127\n                  Cluster       0.82      0.92      0.86       118\n                Correlate       0.78      0.90      0.83       124\n\n                micro avg       0.66      0.66      0.66      1229\n                macro avg       0.70      0.68      0.67      1229\n             weighted avg       0.70      0.66      0.66      1229\n\nConfusion Matrix...\n[[105   1   0   1   0   3   1   0   0   0]\n [ 51  43   0   0   1  34   0   4   0   0]\n [ 44  20  48   1   0   1   2   1   2   7]\n [ 14   7   4  81   3   1   0   2   6   5]\n [  0   0   0   1 102   6   0   0   1   1]\n [ 34   4  10   9   9  54   0   2   7   7]\n [ 13   1   2   1   2  12  79   1   6   3]\n [  3  19   0   0   4   4   6  86   0   5]\n [  1   0   0   0   0   1   3   2 108   3]\n [  2   1   8   0   0   0   0   0   2 111]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.351, Train Acc: 45.31%, Val Loss:  3.546, Val Acc: 39.69%, Time: 31.63s *\nEpoch: 2\nIter: 200, Train Loss:  1.656, Train Acc: 70.31%, Val Loss:  3.013, Val Acc: 53.37%, Time: 3.23s *\nIter: 300, Train Loss:  1.472, Train Acc: 85.94%, Val Loss:  3.059, Val Acc: 57.56%, Time: 33.98s *\nEpoch: 3\nIter: 400, Train Loss:   0.57, Train Acc: 92.19%, Val Loss:  2.974, Val Acc: 58.81%, Time: 4.70s *\nIter: 500, Train Loss:  1.078, Train Acc: 82.81%, Val Loss:  2.981, Val Acc: 59.94%, Time: 35.56s *\nEpoch: 4\nIter: 600, Train Loss: 0.6345, Train Acc: 87.50%, Val Loss:   3.41, Val Acc: 58.88%, Time: 6.11s \nIter: 700, Train Loss: 0.4736, Train Acc: 92.19%, Val Loss:  3.417, Val Acc: 59.44%, Time: 36.82s \nEpoch: 5\nIter: 800, Train Loss:  0.434, Train Acc: 93.75%, Val Loss:  3.551, Val Acc: 59.56%, Time: 7.60s \nIter: 900, Train Loss: 0.3407, Train Acc: 96.88%, Val Loss:  3.675, Val Acc: 57.31%, Time: 38.40s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.40      0.45      0.42       107\n                   Filter       0.37      0.42      0.39       113\n    Compute Derived Value       0.72      0.45      0.56       207\n            Find Extremum       0.68      0.65      0.66       323\n                     Sort       0.86      0.72      0.79       122\n          Determine Range       0.32      0.48      0.38       122\nCharacterize Distribution       0.72      0.45      0.55       179\n           Find Anomalies       0.48      0.23      0.32       124\n                  Cluster       0.70      0.68      0.69       120\n                Correlate       0.50      0.93      0.65       183\n\n                micro avg       0.57      0.57      0.57      1600\n                macro avg       0.58      0.55      0.54      1600\n             weighted avg       0.60      0.57      0.56      1600\n\nConfusion Matrix...\n[[ 48   6   7  18   4   4   0   6   0  14]\n [  3  47   1   1   3  20   0  20   8  10]\n [ 20  13  94  31   0  17   0   2   2  28]\n [ 20  16  14 210   1  53   2   1   2   4]\n [  0   1   1  11  88   8   0   0  10   3]\n [  6  27   1  21   2  59   1   0   0   5]\n [ 14   0  11   1   0  12  80   0   0  61]\n [  7  16   1  18   0   1   7  29  11  34]\n [  0   1   1   0   4  11  13   1  81   8]\n [  2   1   0   0   0   0   8   1   1 170]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.697, Train Acc: 62.50%, Val Loss:  3.401, Val Acc: 39.82%, Time: 31.16s *\nIter: 200, Train Loss:   1.43, Train Acc: 82.61%, Val Loss:  2.235, Val Acc: 61.13%, Time: 61.37s *\nEpoch: 2\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 300, Train Loss:  1.548, Train Acc: 71.88%, Val Loss:  2.124, Val Acc: 65.36%, Time: 30.51s *\nIter: 400, Train Loss: 0.7129, Train Acc: 89.13%, Val Loss:  1.855, Val Acc: 69.75%, Time: 60.89s *\nEpoch: 3\nIter: 500, Train Loss: 0.7279, Train Acc: 89.06%, Val Loss:  1.844, Val Acc: 70.95%, Time: 30.48s *\nIter: 600, Train Loss: 0.5064, Train Acc: 93.48%, Val Loss:  1.905, Val Acc: 71.35%, Time: 60.83s *\nEpoch: 4\nIter: 700, Train Loss: 0.3503, Train Acc: 96.88%, Val Loss:  1.775, Val Acc: 72.47%, Time: 30.38s *\nIter: 800, Train Loss: 0.5179, Train Acc: 89.13%, Val Loss:  1.803, Val Acc: 72.23%, Time: 60.66s \nEpoch: 5\nIter: 900, Train Loss: 0.1472, Train Acc: 100.00%, Val Loss:  1.919, Val Acc: 71.99%, Time: 30.42s \nIter: 1000, Train Loss: 0.2928, Train Acc: 97.83%, Val Loss:  1.708, Val Acc: 75.34%, Time: 60.77s *\nEpoch: 6\nIter: 1100, Train Loss: 0.2558, Train Acc: 96.88%, Val Loss:  1.919, Val Acc: 71.75%, Time: 30.50s \nIter: 1200, Train Loss:  0.228, Train Acc: 97.83%, Val Loss:  1.858, Val Acc: 72.94%, Time: 60.78s \nEpoch: 7\nIter: 1300, Train Loss: 0.2647, Train Acc: 96.88%, Val Loss:  2.115, Val Acc: 71.91%, Time: 30.51s \nIter: 1400, Train Loss: 0.1572, Train Acc: 97.83%, Val Loss:  2.069, Val Acc: 72.23%, Time: 60.93s \nEpoch: 8\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.62      0.74      0.68       127\n                   Filter       0.61      0.75      0.67       129\n    Compute Derived Value       0.58      0.64      0.61       131\n            Find Extremum       0.94      0.40      0.56       127\n                     Sort       0.93      0.69      0.79       119\n          Determine Range       0.77      0.69      0.73       125\nCharacterize Distribution       0.90      0.86      0.88       133\n           Find Anomalies       0.68      0.83      0.75       113\n                  Cluster       0.71      0.77      0.74       124\n                Correlate       0.75      0.86      0.80       125\n\n                micro avg       0.72      0.72      0.72      1253\n                macro avg       0.75      0.72      0.72      1253\n             weighted avg       0.75      0.72      0.72      1253\n\nConfusion Matrix...\n[[ 94   7  20   0   0   3   2   0   1   0]\n [  2  97   2   0   1   0   0  20   5   2]\n [ 37   3  84   0   0   1   1   0   2   3]\n [  7  15  19  51   2  21   1   3   5   3]\n [  7   6   1   3  82   0   2   9   9   0]\n [  0   4  14   0   3  86   4   1   8   5]\n [  3   1   4   0   0   1 114   0   4   6]\n [  0  13   1   0   0   0   0  94   0   5]\n [  1   4   0   0   0   0   2  10  95  12]\n [  0  10   0   0   0   0   0   2   5 108]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.555, Train Acc: 31.25%, Val Loss:  3.261, Val Acc: 44.64%, Time: 31.10s *\nIter: 200, Train Loss:  1.872, Train Acc: 69.39%, Val Loss:  2.414, Val Acc: 58.80%, Time: 61.32s *\nEpoch: 2\nIter: 300, Train Loss:  1.497, Train Acc: 81.25%, Val Loss:  2.116, Val Acc: 65.04%, Time: 30.44s *\nIter: 400, Train Loss:  1.174, Train Acc: 75.51%, Val Loss:  2.057, Val Acc: 66.96%, Time: 60.78s *\nEpoch: 3\nIter: 500, Train Loss: 0.9331, Train Acc: 84.38%, Val Loss:  2.037, Val Acc: 68.88%, Time: 30.43s *\nIter: 600, Train Loss: 0.5581, Train Acc: 91.84%, Val Loss:  1.801, Val Acc: 72.48%, Time: 60.75s *\nEpoch: 4\nIter: 700, Train Loss: 0.5178, Train Acc: 93.75%, Val Loss:  2.028, Val Acc: 70.56%, Time: 30.52s \nIter: 800, Train Loss: 0.6557, Train Acc: 93.88%, Val Loss:  2.149, Val Acc: 68.64%, Time: 60.95s \nEpoch: 5\nIter: 900, Train Loss: 0.3879, Train Acc: 92.19%, Val Loss:  2.164, Val Acc: 69.84%, Time: 30.47s \nIter: 1000, Train Loss: 0.3437, Train Acc: 93.88%, Val Loss:  2.056, Val Acc: 72.24%, Time: 60.97s \nEpoch: 6\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.57      0.56       120\n                   Filter       0.51      0.57      0.54       117\n    Compute Derived Value       0.43      0.54      0.48       123\n            Find Extremum       0.84      0.83      0.84       131\n                     Sort       0.87      0.99      0.92       124\n          Determine Range       0.62      0.57      0.60       126\nCharacterize Distribution       0.99      0.88      0.93       127\n           Find Anomalies       0.71      0.74      0.72       120\n                  Cluster       0.98      0.78      0.87       134\n                Correlate       0.84      0.68      0.75       128\n\n                micro avg       0.72      0.72      0.72      1250\n                macro avg       0.73      0.72      0.72      1250\n             weighted avg       0.74      0.72      0.73      1250\n\nConfusion Matrix...\n[[ 69  19  19   2   1   9   0   1   0   0]\n [ 21  67  12   2   3   0   1  11   0   0]\n [  6   3  67   2   2  28   0   6   0   9]\n [  3   1  15 109   1   2   0   0   0   0]\n [  0   0   0   0 123   0   0   1   0   0]\n [ 29   3  17   2   2  72   0   1   0   0]\n [  0   0   4   0   2   4 112   4   1   0]\n [  0  26   1   1   0   0   0  89   0   3]\n [  0   0   2  12   7   1   0   4 104   4]\n [  0  12  18   0   1   0   0   9   1  87]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.738, Train Acc: 59.38%, Val Loss:  3.345, Val Acc: 45.96%, Time: 31.08s *\nIter: 200, Train Loss:  1.888, Train Acc: 65.62%, Val Loss:  2.261, Val Acc: 65.95%, Time: 61.55s *\nEpoch: 2\nIter: 300, Train Loss:  1.405, Train Acc: 81.25%, Val Loss:  2.019, Val Acc: 69.86%, Time: 30.25s *\nIter: 400, Train Loss:  1.225, Train Acc: 81.25%, Val Loss:  1.995, Val Acc: 71.94%, Time: 60.85s *\nEpoch: 3\nIter: 500, Train Loss:  0.781, Train Acc: 87.50%, Val Loss:  1.968, Val Acc: 69.86%, Time: 30.02s \nIter: 600, Train Loss: 0.4712, Train Acc: 92.19%, Val Loss:  2.123, Val Acc: 70.77%, Time: 60.60s \nEpoch: 4\nIter: 700, Train Loss:  0.526, Train Acc: 92.19%, Val Loss:   2.08, Val Acc: 69.53%, Time: 29.55s \nIter: 800, Train Loss: 0.4404, Train Acc: 93.75%, Val Loss:  2.159, Val Acc: 70.86%, Time: 60.30s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.78      0.55      0.65       112\n                   Filter       0.64      0.87      0.74       113\n    Compute Derived Value       0.63      0.47      0.54       118\n            Find Extremum       0.94      0.63      0.75       116\n                     Sort       0.96      0.81      0.88       114\n          Determine Range       0.69      0.67      0.68       126\nCharacterize Distribution       0.77      0.89      0.82       126\n           Find Anomalies       0.70      0.49      0.58       130\n                  Cluster       0.79      0.91      0.85       123\n                Correlate       0.52      0.85      0.64       123\n\n                micro avg       0.71      0.71      0.71      1201\n                macro avg       0.74      0.71      0.71      1201\n             weighted avg       0.74      0.71      0.71      1201\n\nConfusion Matrix...\n[[ 62  10  25   0   0   4   0   0   0  11]\n [  0  98   0   1   0   3   6   4   1   0]\n [  3   9  56   0   0   5  10   4   0  31]\n [  1  14   1  73   0   9   0  15   0   3]\n [  1   0   2   0  92   0   1   0  12   6]\n [ 11  10   2   3   0  84  12   0   0   4]\n [  1   2   0   0   0   1 112   4   2   4]\n [  0  10   0   1   1  16   0  64   2  36]\n [  0   0   0   0   3   0   4   1 112   3]\n [  1   0   3   0   0   0   1   0  13 105]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.717, Train Acc: 60.94%, Val Loss:  3.284, Val Acc: 47.74%, Time: 31.20s *\nIter: 200, Train Loss:  1.648, Train Acc: 81.58%, Val Loss:  2.243, Val Acc: 65.27%, Time: 61.67s *\nEpoch: 2\nIter: 300, Train Loss: 0.8911, Train Acc: 87.50%, Val Loss:  2.017, Val Acc: 69.47%, Time: 30.49s *\nIter: 400, Train Loss:  1.388, Train Acc: 84.21%, Val Loss:  1.969, Val Acc: 69.63%, Time: 60.84s *\nEpoch: 3\nIter: 500, Train Loss: 0.6502, Train Acc: 90.62%, Val Loss:   1.84, Val Acc: 72.48%, Time: 30.50s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 600, Train Loss: 0.8623, Train Acc: 86.84%, Val Loss:  1.716, Val Acc: 74.62%, Time: 60.97s *\nEpoch: 4\nIter: 700, Train Loss:   0.48, Train Acc: 93.75%, Val Loss:  1.895, Val Acc: 73.83%, Time: 30.51s \nIter: 800, Train Loss: 0.5561, Train Acc: 92.11%, Val Loss:  1.742, Val Acc: 75.26%, Time: 60.95s *\nEpoch: 5\nIter: 900, Train Loss: 0.5725, Train Acc: 90.62%, Val Loss:  1.865, Val Acc: 74.94%, Time: 30.60s \nIter: 1000, Train Loss: 0.3329, Train Acc: 94.74%, Val Loss:  1.869, Val Acc: 74.54%, Time: 61.22s \nEpoch: 6\nIter: 1100, Train Loss: 0.4644, Train Acc: 93.75%, Val Loss:  2.054, Val Acc: 74.15%, Time: 30.50s \nIter: 1200, Train Loss: 0.2627, Train Acc: 97.37%, Val Loss:  2.014, Val Acc: 74.62%, Time: 61.15s \nEpoch: 7\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.56      0.82      0.67       114\n                   Filter       0.54      0.74      0.62       135\n    Compute Derived Value       0.76      0.59      0.66       126\n            Find Extremum       0.86      0.67      0.75       125\n                     Sort       0.91      0.89      0.90       111\n          Determine Range       0.66      0.54      0.59       125\nCharacterize Distribution       0.76      0.86      0.81       123\n           Find Anomalies       0.89      0.65      0.75       124\n                  Cluster       0.86      0.92      0.89       116\n                Correlate       0.87      0.80      0.83       162\n\n                micro avg       0.75      0.75      0.75      1261\n                macro avg       0.77      0.75      0.75      1261\n             weighted avg       0.77      0.75      0.75      1261\n\nConfusion Matrix...\n[[ 93   4   4   2   1   9   1   0   0   0]\n [ 18 100   0   5   2   0   1   8   0   1]\n [ 21  11  74   3   0  14   1   1   0   1]\n [  9  15   9  84   1   2   0   0   0   5]\n [  1   0   0   3  99   3   1   0   4   0]\n [ 13  42   1   0   1  67   0   0   1   0]\n [  1   9   1   0   1   5 106   0   0   0]\n [  5   5   9   1   0   0  11  81   2  10]\n [  2   0   0   0   4   0   0   0 107   3]\n [  2   0   0   0   0   1  19   1  10 129]]\n[0.738819320214669, 0.6877515813686026, 0.6362934362934363, 0.6888463379607468, 0.6647681041497152, 0.56625, 0.722266560255387, 0.7192, 0.7144046627810158, 0.7454401268834259]\n0.6884040129906999, 0.05173230402921703, 0.054530636446877216, 0.0026762312801713447\nbundle\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.302, Train Acc: 37.50%, Val Loss:  3.819, Val Acc: 34.45%, Time: 31.35s *\nEpoch: 2\nIter: 200, Train Loss:  1.972, Train Acc: 71.88%, Val Loss:  2.923, Val Acc: 52.47%, Time: 2.15s *\nIter: 300, Train Loss:  1.547, Train Acc: 78.12%, Val Loss:  2.612, Val Acc: 59.54%, Time: 32.82s *\nEpoch: 3\nIter: 400, Train Loss:   1.47, Train Acc: 76.56%, Val Loss:  2.279, Val Acc: 66.40%, Time: 2.71s *\nIter: 500, Train Loss: 0.7977, Train Acc: 87.50%, Val Loss:  2.418, Val Acc: 65.90%, Time: 33.30s \nEpoch: 4\nIter: 600, Train Loss: 0.6595, Train Acc: 92.19%, Val Loss:  2.359, Val Acc: 66.40%, Time: 3.33s *\nIter: 700, Train Loss: 0.4936, Train Acc: 95.31%, Val Loss:  2.496, Val Acc: 65.26%, Time: 33.93s \nEpoch: 5\nIter: 800, Train Loss: 0.2615, Train Acc: 96.88%, Val Loss:  2.596, Val Acc: 66.05%, Time: 3.87s \nIter: 900, Train Loss: 0.6171, Train Acc: 89.06%, Val Loss:  2.439, Val Acc: 67.69%, Time: 34.67s *\nEpoch: 6\nIter: 1000, Train Loss: 0.2027, Train Acc: 98.44%, Val Loss:  2.986, Val Acc: 62.26%, Time: 4.48s \nIter: 1100, Train Loss: 0.4595, Train Acc: 90.62%, Val Loss:  2.678, Val Acc: 68.48%, Time: 35.21s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1244, Train Acc: 98.44%, Val Loss:  2.763, Val Acc: 69.12%, Time: 5.09s *\nIter: 1300, Train Loss:  0.086, Train Acc: 100.00%, Val Loss:  3.133, Val Acc: 64.90%, Time: 35.74s \nEpoch: 8\nIter: 1400, Train Loss: 0.04626, Train Acc: 100.00%, Val Loss:  3.048, Val Acc: 66.48%, Time: 5.64s \nIter: 1500, Train Loss: 0.1567, Train Acc: 98.44%, Val Loss:  3.105, Val Acc: 68.98%, Time: 36.46s \nEpoch: 9\nIter: 1600, Train Loss: 0.1423, Train Acc: 98.44%, Val Loss:  3.068, Val Acc: 69.34%, Time: 6.21s *\nIter: 1700, Train Loss: 0.1106, Train Acc: 100.00%, Val Loss:  3.254, Val Acc: 67.48%, Time: 36.94s \nEpoch: 10\nIter: 1800, Train Loss: 0.05163, Train Acc: 100.00%, Val Loss:  3.367, Val Acc: 68.83%, Time: 6.81s \nIter: 1900, Train Loss: 0.03362, Train Acc: 100.00%, Val Loss:  3.356, Val Acc: 66.69%, Time: 37.53s \nEpoch: 11\nIter: 2000, Train Loss: 0.1481, Train Acc: 98.44%, Val Loss:  3.213, Val Acc: 67.41%, Time: 7.41s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.60      0.56       136\n                   Filter       0.54      0.71      0.61       168\n    Compute Derived Value       0.69      0.55      0.61       216\n            Find Extremum       0.97      0.61      0.75       248\n                     Sort       0.72      0.93      0.81        44\n          Determine Range       0.66      0.68      0.67       176\nCharacterize Distribution       0.82      0.72      0.77       136\n           Find Anomalies       0.54      0.70      0.61        66\n                  Cluster       0.55      0.55      0.55        65\n                Correlate       0.68      0.88      0.77       144\n\n                micro avg       0.67      0.67      0.67      1399\n                macro avg       0.67      0.69      0.67      1399\n             weighted avg       0.70      0.67      0.67      1399\n\nConfusion Matrix...\n[[ 81  15   9   5   3   0   0   1   3  19]\n [  2 120  12   0   1   4   0  21   5   3]\n [  7  55 119   0   0  24   3   5   3   0]\n [ 20  11  15 152   8  17   1   6  11   7]\n [  0   0   0   0  41   0   0   0   0   3]\n [ 24  14   8   0   2 119   0   5   3   1]\n [  5   2   5   0   1  17  98   0   3   5]\n [  0   6   3   0   0   0   0  46   0  11]\n [ 11   1   1   0   1   0   4   0  36  11]\n [  1   0   0   0   0   0  13   1   2 127]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.034, Train Acc: 46.88%, Val Loss:  3.146, Val Acc: 49.22%, Time: 31.41s *\nEpoch: 2\nIter: 200, Train Loss:  1.941, Train Acc: 73.44%, Val Loss:  2.677, Val Acc: 57.52%, Time: 2.15s *\nIter: 300, Train Loss:  1.059, Train Acc: 85.94%, Val Loss:  2.498, Val Acc: 61.70%, Time: 32.69s *\nEpoch: 3\nIter: 400, Train Loss:   1.25, Train Acc: 82.81%, Val Loss:  2.625, Val Acc: 61.49%, Time: 2.77s \nIter: 500, Train Loss: 0.9115, Train Acc: 87.50%, Val Loss:  2.594, Val Acc: 64.04%, Time: 33.32s *\nEpoch: 4\nIter: 600, Train Loss: 0.6649, Train Acc: 92.19%, Val Loss:  2.551, Val Acc: 65.53%, Time: 3.34s *\nIter: 700, Train Loss: 0.3425, Train Acc: 98.44%, Val Loss:  2.396, Val Acc: 68.44%, Time: 33.91s *\nEpoch: 5\nIter: 800, Train Loss: 0.3431, Train Acc: 96.88%, Val Loss:  2.551, Val Acc: 65.60%, Time: 3.89s \nIter: 900, Train Loss: 0.3999, Train Acc: 92.19%, Val Loss:  2.728, Val Acc: 66.38%, Time: 34.59s \nEpoch: 6\nIter: 1000, Train Loss: 0.2161, Train Acc: 98.44%, Val Loss:  2.872, Val Acc: 66.74%, Time: 4.49s \nIter: 1100, Train Loss: 0.3663, Train Acc: 96.88%, Val Loss:  2.935, Val Acc: 67.38%, Time: 35.13s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.77      0.77      0.77       142\n                   Filter       0.56      0.70      0.62       188\n    Compute Derived Value       0.53      0.73      0.62       127\n            Find Extremum       0.85      0.65      0.74       232\n                     Sort       0.83      0.68      0.75       132\n          Determine Range       0.36      0.58      0.44        64\nCharacterize Distribution       0.73      0.73      0.73       111\n           Find Anomalies       0.80      0.56      0.66       160\n                  Cluster       0.77      0.72      0.74       136\n                Correlate       0.59      0.59      0.59       118\n\n                micro avg       0.67      0.67      0.67      1410\n                macro avg       0.68      0.67      0.67      1410\n             weighted avg       0.71      0.67      0.68      1410\n\nConfusion Matrix...\n[[110   5  14   2   0   3   7   0   1   0]\n [ 12 132  27   0   0   5   3   3   2   4]\n [  6   1  93   0   4   0  12   3   2   6]\n [  3  23  26 151   2  24   1   1   1   0]\n [  2   9   0  13  90   4   0   0  13   1]\n [  1   5   1   9   4  37   1   6   0   0]\n [  0   6   1   0   0   5  81   0   1  17]\n [  3  47   0   1   1  11   1  89   1   6]\n [  0   2   1   0   5  10   2   3  98  15]\n [  5   5  12   1   2   5   3   6   9  70]]\nFold:  3\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.155, Train Acc: 53.12%, Val Loss:  3.463, Val Acc: 44.86%, Time: 31.43s *\nEpoch: 2\nIter: 200, Train Loss:  2.104, Train Acc: 65.62%, Val Loss:  2.568, Val Acc: 58.92%, Time: 2.14s *\nIter: 300, Train Loss:  1.127, Train Acc: 81.25%, Val Loss:  2.337, Val Acc: 62.77%, Time: 32.75s *\nEpoch: 3\nIter: 400, Train Loss: 0.7135, Train Acc: 87.50%, Val Loss:  2.411, Val Acc: 64.98%, Time: 2.72s *\nIter: 500, Train Loss: 0.8636, Train Acc: 89.06%, Val Loss:  2.239, Val Acc: 66.33%, Time: 33.50s *\nEpoch: 4\nIter: 600, Train Loss: 0.4336, Train Acc: 95.31%, Val Loss:  2.265, Val Acc: 68.62%, Time: 3.32s *\nIter: 700, Train Loss:  0.505, Train Acc: 92.19%, Val Loss:   2.23, Val Acc: 66.48%, Time: 34.08s \nEpoch: 5\nIter: 800, Train Loss: 0.4315, Train Acc: 93.75%, Val Loss:  2.656, Val Acc: 65.12%, Time: 3.90s \nIter: 900, Train Loss:  0.254, Train Acc: 95.31%, Val Loss:  2.577, Val Acc: 66.41%, Time: 34.70s \nEpoch: 6\nIter: 1000, Train Loss: 0.07908, Train Acc: 100.00%, Val Loss:   2.39, Val Acc: 68.47%, Time: 4.48s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.63      0.54      0.58       162\n                   Filter       0.54      0.59      0.57       151\n    Compute Derived Value       0.63      0.66      0.64       211\n            Find Extremum       0.74      0.83      0.78       159\n                     Sort       0.68      0.80      0.73        64\n          Determine Range       0.51      0.44      0.47       107\nCharacterize Distribution       0.75      0.87      0.80       123\n           Find Anomalies       0.89      0.76      0.82       168\n                  Cluster       0.80      0.54      0.65       144\n                Correlate       0.72      0.93      0.81       113\n\n                micro avg       0.69      0.69      0.69      1402\n                macro avg       0.69      0.70      0.69      1402\n             weighted avg       0.69      0.69      0.68      1402\n\nConfusion Matrix...\n[[ 88   8  41   8   5   9   1   0   1   1]\n [ 18  89  18  13   3   2   1   0   6   1]\n [ 25   5 139  13   0  15   6   1   1   6]\n [  1  14   0 132  11   0   0   0   0   1]\n [  3   0   4   2  51   0   1   0   2   1]\n [  2  33   9   5   1  47   4   2   2   2]\n [  0   2   5   1   2   4 107   0   2   0]\n [  0   3   4   3   0   3  12 128   3  12]\n [  0  10   1   2   2  13  11  11  78  16]\n [  3   0   1   0   0   0   0   2   2 105]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.183, Train Acc: 50.00%, Val Loss:  3.201, Val Acc: 49.51%, Time: 31.27s *\nEpoch: 2\nIter: 200, Train Loss:  1.694, Train Acc: 75.00%, Val Loss:  2.281, Val Acc: 62.27%, Time: 2.15s *\nIter: 300, Train Loss:  1.676, Train Acc: 70.31%, Val Loss:  1.999, Val Acc: 69.32%, Time: 32.75s *\nEpoch: 3\nIter: 400, Train Loss: 0.7307, Train Acc: 89.06%, Val Loss:  1.812, Val Acc: 73.41%, Time: 2.74s *\nIter: 500, Train Loss: 0.7618, Train Acc: 92.19%, Val Loss:  1.809, Val Acc: 73.84%, Time: 33.36s *\nEpoch: 4\nIter: 600, Train Loss: 0.7653, Train Acc: 93.75%, Val Loss:  1.773, Val Acc: 74.54%, Time: 3.35s *\nIter: 700, Train Loss: 0.7333, Train Acc: 90.62%, Val Loss:  1.897, Val Acc: 72.92%, Time: 34.03s \nEpoch: 5\nIter: 800, Train Loss: 0.4069, Train Acc: 93.75%, Val Loss:  1.699, Val Acc: 76.38%, Time: 3.96s *\nIter: 900, Train Loss: 0.4352, Train Acc: 92.19%, Val Loss:  1.912, Val Acc: 73.27%, Time: 34.60s \nEpoch: 6\nIter: 1000, Train Loss: 0.1917, Train Acc: 98.44%, Val Loss:  1.916, Val Acc: 74.61%, Time: 4.53s \nIter: 1100, Train Loss: 0.1287, Train Acc: 98.44%, Val Loss:    2.2, Val Acc: 72.07%, Time: 35.31s \nEpoch: 7\nIter: 1200, Train Loss:  0.216, Train Acc: 98.44%, Val Loss:   2.21, Val Acc: 72.64%, Time: 5.08s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.73      0.62        73\n                   Filter       0.75      0.56      0.64       200\n    Compute Derived Value       0.62      0.84      0.71        80\n            Find Extremum       0.77      0.57      0.65       171\n                     Sort       0.95      0.73      0.83       200\n          Determine Range       0.51      0.80      0.62       133\nCharacterize Distribution       0.75      0.75      0.75        97\n           Find Anomalies       0.71      0.75      0.73       122\n                  Cluster       0.85      0.83      0.84       225\n                Correlate       0.75      0.83      0.79       117\n\n                micro avg       0.73      0.73      0.73      1418\n                macro avg       0.72      0.74      0.72      1418\n             weighted avg       0.75      0.73      0.73      1418\n\nConfusion Matrix...\n[[ 53   1   7   0   0  10   0   0   1   1]\n [  3 112   3  18   0  28   6  24   3   3]\n [  0   0  67   0   0   1   0   1   6   5]\n [ 16   8   7  97   5  36   0   1   0   1]\n [  4   4   5  10 147  22   0   0   8   0]\n [  2  12   8   0   1 107   1   0   1   1]\n [  2   0   3   0   0   3  73   1   0  15]\n [  7   4   2   0   0   3   0  91  13   2]\n [  8   0   2   1   0   1  17   6 186   4]\n [  2   8   4   0   1   0   0   5   0  97]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.889, Train Acc: 62.50%, Val Loss:  3.199, Val Acc: 48.40%, Time: 31.25s *\nEpoch: 2\nIter: 200, Train Loss:  1.425, Train Acc: 78.12%, Val Loss:  2.157, Val Acc: 65.48%, Time: 2.09s *\nIter: 300, Train Loss:  1.381, Train Acc: 75.00%, Val Loss:  1.929, Val Acc: 70.49%, Time: 32.79s *\nEpoch: 3\nIter: 400, Train Loss:  1.026, Train Acc: 87.50%, Val Loss:  1.864, Val Acc: 72.31%, Time: 2.70s *\nIter: 500, Train Loss:  1.359, Train Acc: 81.25%, Val Loss:  1.776, Val Acc: 73.04%, Time: 33.29s *\nEpoch: 4\nIter: 600, Train Loss: 0.6334, Train Acc: 90.62%, Val Loss:  1.822, Val Acc: 73.11%, Time: 3.30s *\nIter: 700, Train Loss: 0.3761, Train Acc: 93.75%, Val Loss:  2.084, Val Acc: 71.15%, Time: 33.84s \nEpoch: 5\nIter: 800, Train Loss: 0.2346, Train Acc: 96.88%, Val Loss:  2.137, Val Acc: 72.38%, Time: 3.86s \nIter: 900, Train Loss: 0.3444, Train Acc: 90.62%, Val Loss:  1.983, Val Acc: 73.26%, Time: 34.42s *\nEpoch: 6\nIter: 1000, Train Loss: 0.3734, Train Acc: 93.75%, Val Loss:  2.111, Val Acc: 73.11%, Time: 4.44s \nIter: 1100, Train Loss: 0.1947, Train Acc: 95.31%, Val Loss:  2.293, Val Acc: 70.71%, Time: 35.00s \nEpoch: 7\nIter: 1200, Train Loss: 0.1735, Train Acc: 98.44%, Val Loss:  2.093, Val Acc: 73.91%, Time: 5.06s *\nIter: 1300, Train Loss: 0.2867, Train Acc: 95.31%, Val Loss:  2.348, Val Acc: 72.38%, Time: 35.79s \nEpoch: 8\nIter: 1400, Train Loss: 0.2886, Train Acc: 96.88%, Val Loss:  2.475, Val Acc: 72.67%, Time: 5.67s \nIter: 1500, Train Loss: 0.07713, Train Acc: 100.00%, Val Loss:  2.626, Val Acc: 69.84%, Time: 36.28s \nEpoch: 9\nIter: 1600, Train Loss:  0.052, Train Acc: 100.00%, Val Loss:  2.624, Val Acc: 72.31%, Time: 6.21s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.62      0.69      0.65       157\n                   Filter       0.58      0.71      0.64       104\n    Compute Derived Value       0.82      0.72      0.77       162\n            Find Extremum       0.65      0.78      0.71       142\n                     Sort       0.78      0.76      0.77       187\n          Determine Range       0.64      0.48      0.55       125\nCharacterize Distribution       0.88      0.80      0.84       125\n           Find Anomalies       0.70      0.52      0.59        85\n                  Cluster       0.64      0.94      0.76       124\n                Correlate       0.96      0.74      0.84       165\n\n                micro avg       0.72      0.72      0.72      1376\n                macro avg       0.73      0.71      0.71      1376\n             weighted avg       0.74      0.72      0.72      1376\n\nConfusion Matrix...\n[[109   7  11  17   3  10   0   0   0   0]\n [  7  74   3   0   0   9   0   5   6   0]\n [  9   1 117  15   0   4   9   2   5   0]\n [  3   4   0 111  20   1   1   2   0   0]\n [ 15   3   1   7 142   7   0   1  11   0]\n [ 32   2   3   5  11  60   1   0  10   1]\n [  0   3   7   0   0   2 100   5   7   1]\n [  1  30   0   0   2   1   2  44   5   0]\n [  1   0   0   0   3   0   0   1 116   3]\n [  0   4   0  15   0   0   1   3  20 122]]\nFold:  6\n", "name": "stdout"}, {"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.049, Train Acc: 48.44%, Val Loss:  3.342, Val Acc: 44.82%, Time: 31.46s *\nEpoch: 2\nIter: 200, Train Loss:  2.242, Train Acc: 64.06%, Val Loss:  2.481, Val Acc: 60.04%, Time: 2.17s *\nIter: 300, Train Loss:  1.273, Train Acc: 78.12%, Val Loss:  2.039, Val Acc: 66.60%, Time: 32.76s *\nEpoch: 3\nIter: 400, Train Loss: 0.9421, Train Acc: 82.81%, Val Loss:  1.869, Val Acc: 68.50%, Time: 2.75s *\nIter: 500, Train Loss: 0.9964, Train Acc: 85.94%, Val Loss:  1.804, Val Acc: 72.59%, Time: 33.52s *\nEpoch: 4\nIter: 600, Train Loss: 0.5853, Train Acc: 92.19%, Val Loss:  1.718, Val Acc: 75.05%, Time: 3.35s *\nIter: 700, Train Loss: 0.4241, Train Acc: 93.75%, Val Loss:  1.969, Val Acc: 72.73%, Time: 33.96s \nEpoch: 5\nIter: 800, Train Loss: 0.2677, Train Acc: 98.44%, Val Loss:  1.872, Val Acc: 72.94%, Time: 3.93s \nIter: 900, Train Loss: 0.1798, Train Acc: 100.00%, Val Loss:  2.004, Val Acc: 72.80%, Time: 34.56s \nEpoch: 6\nIter: 1000, Train Loss: 0.3574, Train Acc: 93.75%, Val Loss:  1.958, Val Acc: 74.07%, Time: 4.48s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.49      0.56      0.52        91\n                   Filter       0.67      0.73      0.70       124\n    Compute Derived Value       0.67      0.48      0.56       181\n            Find Extremum       0.72      0.89      0.80       116\n                     Sort       0.85      0.97      0.90       122\n          Determine Range       0.82      0.71      0.76       175\nCharacterize Distribution       0.76      0.80      0.78       138\n           Find Anomalies       0.84      0.76      0.80       196\n                  Cluster       0.80      0.85      0.82        80\n                Correlate       0.75      0.82      0.79       196\n\n                micro avg       0.75      0.75      0.75      1419\n                macro avg       0.74      0.75      0.74      1419\n             weighted avg       0.75      0.75      0.74      1419\n\nConfusion Matrix...\n[[ 51   1  14   2   5   0   8   0   0  10]\n [ 15  90   3   2   0   3   0  11   0   0]\n [ 18  10  86  21   2  16   7   7   0  14]\n [  3   1   1 103   1   1   1   1   0   4]\n [  0   0   0   2 118   0   0   0   2   0]\n [ 11   8  12   5   5 124   7   1   1   1]\n [  0   8   3   4   2   1 110   0   0  10]\n [  3  14   6   1   1   7   7 148   3   6]\n [  0   0   0   2   2   0   0   0  68   8]\n [  3   2   3   1   3   0   4   8  11 161]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.992, Train Acc: 54.69%, Val Loss:  3.254, Val Acc: 48.08%, Time: 31.32s *\nEpoch: 2\nIter: 200, Train Loss:  1.703, Train Acc: 76.56%, Val Loss:  2.582, Val Acc: 55.11%, Time: 2.17s *\nIter: 300, Train Loss:  1.196, Train Acc: 78.12%, Val Loss:  2.304, Val Acc: 64.56%, Time: 32.76s *\nEpoch: 3\nIter: 400, Train Loss: 0.7716, Train Acc: 92.19%, Val Loss:  2.205, Val Acc: 65.34%, Time: 2.75s *\nIter: 500, Train Loss: 0.8065, Train Acc: 85.94%, Val Loss:   2.24, Val Acc: 66.19%, Time: 33.61s *\nEpoch: 4\nIter: 600, Train Loss: 0.5443, Train Acc: 95.31%, Val Loss:  2.375, Val Acc: 66.83%, Time: 3.36s *\nIter: 700, Train Loss: 0.5073, Train Acc: 93.75%, Val Loss:  2.213, Val Acc: 66.12%, Time: 33.98s \nEpoch: 5\nIter: 800, Train Loss: 0.2265, Train Acc: 96.88%, Val Loss:  2.292, Val Acc: 65.62%, Time: 3.94s \nIter: 900, Train Loss: 0.4261, Train Acc: 96.88%, Val Loss:  2.635, Val Acc: 63.71%, Time: 34.66s \nEpoch: 6\nIter: 1000, Train Loss: 0.1455, Train Acc: 100.00%, Val Loss:  2.591, Val Acc: 64.84%, Time: 4.47s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.37      0.77      0.49        90\n                   Filter       0.54      0.59      0.56       153\n    Compute Derived Value       0.54      0.49      0.51       176\n            Find Extremum       0.77      0.62      0.69       150\n                     Sort       0.74      0.91      0.81       129\n          Determine Range       0.75      0.56      0.64       209\nCharacterize Distribution       0.89      0.55      0.68       115\n           Find Anomalies       0.55      0.36      0.44       110\n                  Cluster       0.86      0.84      0.85       120\n                Correlate       0.70      0.87      0.78       156\n\n                micro avg       0.65      0.65      0.65      1408\n                macro avg       0.67      0.66      0.65      1408\n             weighted avg       0.68      0.65      0.65      1408\n\nConfusion Matrix...\n[[ 69   9   8   0   0   0   0   0   4   0]\n [ 17  90   2   2   6   9   0  17   0  10]\n [ 61   5  86   6   4   0   0   7   2   5]\n [  7  11   9  93  13  16   0   1   0   0]\n [  3   0   0   2 117   2   0   0   4   1]\n [ 17  17  25  15   8 118   6   1   2   0]\n [  2   1  18   1   1  10  63   1   2  16]\n [ 11  26   9   2   0   2   1  40   1  18]\n [  0   1   1   0   9   0   1   0 101   7]\n [  2   7   2   0   1   1   0   6   2 135]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.839, Train Acc: 62.50%, Val Loss:   3.34, Val Acc: 44.93%, Time: 31.37s *\nEpoch: 2\nIter: 200, Train Loss:  1.418, Train Acc: 79.69%, Val Loss:  2.581, Val Acc: 59.69%, Time: 2.47s *\nIter: 300, Train Loss:  1.203, Train Acc: 81.25%, Val Loss:  2.258, Val Acc: 64.94%, Time: 33.14s *\nEpoch: 3\nIter: 400, Train Loss:   1.32, Train Acc: 78.12%, Val Loss:  2.201, Val Acc: 68.51%, Time: 3.38s *\nIter: 500, Train Loss: 0.9918, Train Acc: 90.62%, Val Loss:  2.152, Val Acc: 69.42%, Time: 34.13s *\nEpoch: 4\nIter: 600, Train Loss:  0.821, Train Acc: 90.62%, Val Loss:  2.226, Val Acc: 70.05%, Time: 4.26s *\nIter: 700, Train Loss: 0.4889, Train Acc: 90.62%, Val Loss:  2.363, Val Acc: 70.05%, Time: 35.01s \nEpoch: 5\nIter: 800, Train Loss: 0.3319, Train Acc: 95.31%, Val Loss:  2.391, Val Acc: 70.05%, Time: 5.04s \nIter: 900, Train Loss: 0.1653, Train Acc: 100.00%, Val Loss:  2.425, Val Acc: 69.63%, Time: 35.89s \nEpoch: 6\nIter: 1000, Train Loss: 0.2381, Train Acc: 96.88%, Val Loss:  2.444, Val Acc: 71.03%, Time: 5.99s *\nIter: 1100, Train Loss: 0.4661, Train Acc: 93.75%, Val Loss:  2.611, Val Acc: 69.63%, Time: 36.81s \nEpoch: 7\nIter: 1200, Train Loss: 0.1039, Train Acc: 100.00%, Val Loss:  2.807, Val Acc: 69.98%, Time: 6.81s \nIter: 1300, Train Loss: 0.3995, Train Acc: 95.31%, Val Loss:  2.864, Val Acc: 69.14%, Time: 37.62s \nEpoch: 8\nIter: 1400, Train Loss: 0.04608, Train Acc: 100.00%, Val Loss:  3.009, Val Acc: 69.28%, Time: 7.71s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.86      0.67       120\n                   Filter       0.59      0.51      0.55       139\n    Compute Derived Value       0.61      0.58      0.60       175\n            Find Extremum       0.75      0.67      0.71       183\n                     Sort       0.84      0.79      0.81        90\n          Determine Range       0.77      0.71      0.74       119\nCharacterize Distribution       0.85      0.74      0.79       137\n           Find Anomalies       0.50      0.67      0.57       124\n                  Cluster       0.80      0.88      0.84       134\n                Correlate       0.80      0.64      0.71       208\n\n                micro avg       0.69      0.69      0.69      1429\n                macro avg       0.71      0.71      0.70      1429\n             weighted avg       0.71      0.69      0.69      1429\n\nConfusion Matrix...\n[[103   3   0   1   0   0   0  13   0   0]\n [ 21  71  23   9   1   9   2   3   0   0]\n [ 37   4 102   7   0   0   1  11   5   8]\n [ 13   5  10 123   9   9   0  14   0   0]\n [  4   6   0   0  71   1   0   0   8   0]\n [  2   0   5  16   0  84  10   1   0   1]\n [  2   0  10   0   2   3 102   4   9   5]\n [  0  18   1   3   0   1   2  83   1  15]\n [  2   0   1   0   1   1   0   7 118   4]\n [  3  13  15   4   1   1   3  29   6 133]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.423, Train Acc: 51.56%, Val Loss:  3.505, Val Acc: 37.27%, Time: 31.51s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 2\nIter: 200, Train Loss:    2.0, Train Acc: 67.19%, Val Loss:   2.45, Val Acc: 59.37%, Time: 2.14s *\nIter: 300, Train Loss:  1.171, Train Acc: 81.25%, Val Loss:  2.033, Val Acc: 66.45%, Time: 32.69s *\nEpoch: 3\nIter: 400, Train Loss: 0.5959, Train Acc: 90.62%, Val Loss:  2.031, Val Acc: 67.24%, Time: 2.71s *\nIter: 500, Train Loss: 0.6809, Train Acc: 95.31%, Val Loss:  1.923, Val Acc: 70.24%, Time: 33.37s *\nEpoch: 4\nIter: 600, Train Loss: 0.4798, Train Acc: 93.75%, Val Loss:  1.908, Val Acc: 70.10%, Time: 3.28s \nIter: 700, Train Loss: 0.3822, Train Acc: 93.75%, Val Loss:  2.007, Val Acc: 69.81%, Time: 33.99s \nEpoch: 5\nIter: 800, Train Loss: 0.3285, Train Acc: 96.88%, Val Loss:  1.971, Val Acc: 70.60%, Time: 3.89s *\nIter: 900, Train Loss: 0.3015, Train Acc: 93.75%, Val Loss:  2.178, Val Acc: 68.67%, Time: 34.66s \nEpoch: 6\nIter: 1000, Train Loss: 0.1721, Train Acc: 98.44%, Val Loss:  2.288, Val Acc: 67.95%, Time: 4.45s \nIter: 1100, Train Loss: 0.1632, Train Acc: 100.00%, Val Loss:   2.29, Val Acc: 68.96%, Time: 35.14s \nEpoch: 7\nIter: 1200, Train Loss: 0.1138, Train Acc: 98.44%, Val Loss:  2.438, Val Acc: 69.17%, Time: 5.06s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.82      0.55      0.65       203\n                   Filter       0.65      0.53      0.59       135\n    Compute Derived Value       0.33      0.55      0.41       102\n            Find Extremum       0.50      0.69      0.58        86\n                     Sort       0.83      0.89      0.86       107\n          Determine Range       0.46      0.47      0.47       118\nCharacterize Distribution       0.83      0.82      0.82       200\n           Find Anomalies       0.86      0.77      0.81       177\n                  Cluster       0.85      0.77      0.81       137\n                Correlate       0.76      0.83      0.79       133\n\n                micro avg       0.69      0.69      0.69      1398\n                macro avg       0.69      0.69      0.68      1398\n             weighted avg       0.72      0.69      0.70      1398\n\nConfusion Matrix...\n[[111  15  66   4   0   2   0   1   1   3]\n [ 20  72   4   6   0  23   0   7   1   2]\n [  1   2  56  17   0  12  10   2   0   2]\n [  0   0   1  59   0  23   0   2   0   1]\n [  2   1   2   0  95   0   1   0   6   0]\n [  1   8  21   3   6  56  13   1   1   8]\n [  1   0   8  13   2   5 164   2   2   3]\n [  0   8  12  15   0   0   2 136   1   3]\n [  0   3   0   0  12   0   1   2 106  13]\n [  0   1   1   0   0   1   7   6   7 110]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:    3.3, Train Acc: 42.19%, Val Loss:  3.195, Val Acc: 47.38%, Time: 31.38s *\nEpoch: 2\nIter: 200, Train Loss:  1.949, Train Acc: 65.62%, Val Loss:   2.16, Val Acc: 67.81%, Time: 2.11s *\nIter: 300, Train Loss: 0.9993, Train Acc: 84.38%, Val Loss:  2.066, Val Acc: 68.68%, Time: 32.90s *\nEpoch: 3\nIter: 400, Train Loss: 0.9692, Train Acc: 87.50%, Val Loss:  1.915, Val Acc: 72.02%, Time: 2.71s *\nIter: 500, Train Loss: 0.8548, Train Acc: 89.06%, Val Loss:  1.846, Val Acc: 73.26%, Time: 33.42s *\nEpoch: 4\nIter: 600, Train Loss:  0.269, Train Acc: 96.88%, Val Loss:  1.972, Val Acc: 71.37%, Time: 3.32s \nIter: 700, Train Loss:  0.814, Train Acc: 90.62%, Val Loss:  2.022, Val Acc: 70.78%, Time: 34.00s \nEpoch: 5\nIter: 800, Train Loss: 0.2289, Train Acc: 100.00%, Val Loss:  2.026, Val Acc: 72.02%, Time: 3.87s \nIter: 900, Train Loss: 0.3628, Train Acc: 93.75%, Val Loss:  2.288, Val Acc: 68.90%, Time: 34.53s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.82      0.47      0.60       190\n                   Filter       0.43      0.64      0.52        90\n    Compute Derived Value       0.32      0.40      0.35       105\n            Find Extremum       0.93      0.84      0.88       173\n                     Sort       0.91      0.93      0.92       131\n          Determine Range       0.41      0.72      0.52        96\nCharacterize Distribution       0.72      0.76      0.74       147\n           Find Anomalies       0.92      0.67      0.77       165\n                  Cluster       0.73      0.86      0.79       105\n                Correlate       0.80      0.64      0.71       174\n\n                micro avg       0.69      0.69      0.69      1376\n                macro avg       0.70      0.69      0.68      1376\n             weighted avg       0.74      0.69      0.70      1376\n\nConfusion Matrix...\n[[ 90   9  56   0   0  14  14   0   6   1]\n [  2  58   6   0   0  20   1   2   1   0]\n [  3  25  42   3   0  29   1   2   0   0]\n [  8   2   4 146   2  10   1   0   0   0]\n [  0   0   0   1 122   4   0   1   3   0]\n [  4  13   0   4   1  69   3   1   1   0]\n [  0   0   4   0   0   5 111   0  12  15]\n [  2  17   8   3   0  12   2 110   0  11]\n [  0   2   0   0   8   1   1   2  90   1]\n [  1   9  13   0   1   6  20   1  11 112]]\n[0.6711937097927091, 0.674468085106383, 0.6875891583452212, 0.7263751763046544, 0.7231104651162791, 0.7463002114164905, 0.6477272727272727, 0.6927921623512946, 0.6902718168812589, 0.690406976744186]\n0.695023503478575, 0.027807485557355675, 0.02931166345449357, 0.0007732562530225445\ntable\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.634, Train Acc: 37.50%, Val Loss:  3.505, Val Acc: 43.75%, Time: 31.60s *\nEpoch: 2\nIter: 200, Train Loss:  1.559, Train Acc: 79.69%, Val Loss:   2.47, Val Acc: 59.55%, Time: 3.57s *\nIter: 300, Train Loss:   1.28, Train Acc: 87.50%, Val Loss:  2.169, Val Acc: 64.67%, Time: 34.47s *\nEpoch: 3\nIter: 400, Train Loss:  1.187, Train Acc: 81.25%, Val Loss:  1.991, Val Acc: 69.55%, Time: 5.25s *\nIter: 500, Train Loss: 0.7057, Train Acc: 89.06%, Val Loss:  2.078, Val Acc: 67.79%, Time: 36.08s \nEpoch: 4\nIter: 600, Train Loss:  0.253, Train Acc: 98.44%, Val Loss:  2.125, Val Acc: 69.01%, Time: 7.06s \nIter: 700, Train Loss: 0.4285, Train Acc: 90.62%, Val Loss:  2.123, Val Acc: 69.19%, Time: 37.83s \nEpoch: 5\nIter: 800, Train Loss: 0.3917, Train Acc: 96.88%, Val Loss:  2.322, Val Acc: 67.91%, Time: 8.86s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.43      0.62      0.51       136\n                   Filter       0.57      0.56      0.57       163\n    Compute Derived Value       0.60      0.59      0.60       192\n            Find Extremum       0.93      0.51      0.66       223\n                     Sort       0.90      0.77      0.83       158\n          Determine Range       0.76      0.60      0.67       151\nCharacterize Distribution       0.66      0.70      0.68       144\n           Find Anomalies       0.67      0.89      0.76       150\n                  Cluster       0.80      0.84      0.82       165\n                Correlate       0.60      0.73      0.66       157\n\n                micro avg       0.67      0.67      0.67      1639\n                macro avg       0.69      0.68      0.68      1639\n             weighted avg       0.70      0.67      0.67      1639\n\nConfusion Matrix...\n[[ 84  17  24   0   0   1   9   0   0   1]\n [ 10  92  16   0   0   1  11  17   8   8]\n [ 32   2 114   0   0   2  14   1   7  20]\n [ 23  29   3 114   8  12   9  14   8   3]\n [ 14   1   2   0 121   3   3   0  10   4]\n [ 18  15   4   8   0  90   1   9   1   5]\n [  4   4   5   0   0   6 101   1   0  23]\n [  0   0   9   0   0   0   0 133   1   7]\n [  4   0   6   0   5   1   3   2 139   5]\n [  5   2   8   0   0   2   3  22   0 115]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.837, Train Acc: 51.56%, Val Loss:  3.457, Val Acc: 42.17%, Time: 31.66s *\nEpoch: 2\nIter: 200, Train Loss:  1.471, Train Acc: 79.69%, Val Loss:  2.951, Val Acc: 53.36%, Time: 3.90s *\nIter: 300, Train Loss:  1.437, Train Acc: 73.44%, Val Loss:  2.782, Val Acc: 57.66%, Time: 34.86s *\nEpoch: 3\nIter: 400, Train Loss: 0.9364, Train Acc: 89.06%, Val Loss:  2.693, Val Acc: 60.90%, Time: 5.95s *\nIter: 500, Train Loss: 0.8548, Train Acc: 89.06%, Val Loss:  2.772, Val Acc: 62.19%, Time: 36.99s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 4\nIter: 600, Train Loss: 0.5416, Train Acc: 89.06%, Val Loss:  2.868, Val Acc: 60.19%, Time: 8.01s \nIter: 700, Train Loss: 0.4096, Train Acc: 95.31%, Val Loss:  3.198, Val Acc: 59.01%, Time: 39.09s \nEpoch: 5\nIter: 800, Train Loss: 0.5091, Train Acc: 92.19%, Val Loss:  3.019, Val Acc: 61.90%, Time: 10.04s \nIter: 900, Train Loss: 0.5786, Train Acc: 89.06%, Val Loss:  3.117, Val Acc: 61.25%, Time: 41.07s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.40      0.43       180\n                   Filter       0.56      0.34      0.42       191\n    Compute Derived Value       0.69      0.43      0.53       167\n            Find Extremum       0.62      0.86      0.72       173\n                     Sort       0.71      0.97      0.82       146\n          Determine Range       0.76      0.61      0.68       183\nCharacterize Distribution       0.60      0.65      0.62       141\n           Find Anomalies       0.52      0.54      0.53       182\n                  Cluster       0.82      0.64      0.72       160\n                Correlate       0.50      0.76      0.60       175\n\n                micro avg       0.61      0.61      0.61      1698\n                macro avg       0.62      0.62      0.61      1698\n             weighted avg       0.62      0.61      0.60      1698\n\nConfusion Matrix...\n[[ 72   7   7  21  19   9  15  11   0  19]\n [ 39  64   7   9  10  16   2  26   6  12]\n [ 31   3  72  31   2   3   7  10   2   6]\n [  1   3   0 149   9   5   1   3   1   1]\n [  0   0   0   0 142   0   0   0   3   1]\n [  5  24   2  11   8 112   8  10   0   3]\n [  1   2   8   1   2   2  91   0   6  28]\n [  2   3   4  18   0   0   8  99   5  43]\n [  0   3   0   1   6   1  15  10 103  21]\n [  5   6   5   0   2   0   4  20   0 133]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.778, Train Acc: 56.25%, Val Loss:  3.344, Val Acc: 44.09%, Time: 30.96s *\nIter: 200, Train Loss:  1.728, Train Acc: 81.25%, Val Loss:  2.379, Val Acc: 62.76%, Time: 61.30s *\nEpoch: 2\nIter: 300, Train Loss:  1.181, Train Acc: 81.25%, Val Loss:  2.129, Val Acc: 67.21%, Time: 29.76s *\nIter: 400, Train Loss:  1.609, Train Acc: 78.12%, Val Loss:  2.132, Val Acc: 67.04%, Time: 60.17s \nEpoch: 3\nIter: 500, Train Loss: 0.5348, Train Acc: 95.31%, Val Loss:  2.099, Val Acc: 69.26%, Time: 29.28s *\nIter: 600, Train Loss: 0.5275, Train Acc: 95.31%, Val Loss:  2.141, Val Acc: 70.63%, Time: 59.85s *\nEpoch: 4\nIter: 700, Train Loss: 0.8114, Train Acc: 87.50%, Val Loss:  2.415, Val Acc: 68.58%, Time: 28.84s \nIter: 800, Train Loss: 0.6773, Train Acc: 87.50%, Val Loss:  2.318, Val Acc: 67.64%, Time: 59.35s \nEpoch: 5\nIter: 900, Train Loss: 0.7433, Train Acc: 92.19%, Val Loss:    2.3, Val Acc: 70.12%, Time: 28.18s \nIter: 1000, Train Loss: 0.6459, Train Acc: 90.62%, Val Loss:  2.279, Val Acc: 69.78%, Time: 58.69s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.49      0.48       116\n                   Filter       0.73      0.82      0.77       110\n    Compute Derived Value       0.42      0.43      0.43        92\n            Find Extremum       0.90      0.95      0.92       135\n                     Sort       0.91      0.88      0.89        90\n          Determine Range       0.59      0.41      0.48        90\nCharacterize Distribution       0.84      0.78      0.81       112\n           Find Anomalies       0.82      0.52      0.64       105\n                  Cluster       0.78      0.75      0.76       127\n                Correlate       0.61      0.77      0.69       191\n\n                micro avg       0.70      0.70      0.70      1168\n                macro avg       0.71      0.68      0.69      1168\n             weighted avg       0.71      0.70      0.70      1168\n\nConfusion Matrix...\n[[ 57   1  20   5   0   4   0   0   0  29]\n [  9  90   9   2   0   0   0   0   0   0]\n [ 16   0  40   1   0   2   2   2   2  27]\n [  0   1   3 128   0   1   1   0   0   1]\n [  1   0   0   1  79   1   0   0   5   3]\n [ 16  14  10   1   0  37   0   0   0  12]\n [  9   0   4   0   0   3  87   0   2   7]\n [  3  18   5   3   2  14   1  55   0   4]\n [  5   0   1   0   6   1   2   7  95  10]\n [  7   0   4   1   0   0  10   3  18 148]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.061, Train Acc: 50.00%, Val Loss:  3.291, Val Acc: 44.82%, Time: 31.69s *\nEpoch: 2\nIter: 200, Train Loss:  1.644, Train Acc: 73.44%, Val Loss:  2.475, Val Acc: 61.70%, Time: 3.94s *\nIter: 300, Train Loss:  1.488, Train Acc: 75.00%, Val Loss:  2.454, Val Acc: 63.27%, Time: 34.90s *\nEpoch: 3\nIter: 400, Train Loss:  1.135, Train Acc: 78.12%, Val Loss:  2.353, Val Acc: 64.20%, Time: 5.98s *\nIter: 500, Train Loss: 0.7676, Train Acc: 85.94%, Val Loss:  2.255, Val Acc: 66.65%, Time: 37.08s *\nEpoch: 4\nIter: 600, Train Loss: 0.3955, Train Acc: 95.31%, Val Loss:  2.255, Val Acc: 67.58%, Time: 8.06s *\nIter: 700, Train Loss: 0.2368, Train Acc: 98.44%, Val Loss:  2.491, Val Acc: 67.75%, Time: 38.98s *\nEpoch: 5\nIter: 800, Train Loss: 0.2933, Train Acc: 95.31%, Val Loss:   2.42, Val Acc: 66.76%, Time: 10.08s \nIter: 900, Train Loss: 0.6583, Train Acc: 92.19%, Val Loss:   2.83, Val Acc: 64.26%, Time: 41.05s \nEpoch: 6\nIter: 1000, Train Loss: 0.1353, Train Acc: 100.00%, Val Loss:  2.832, Val Acc: 67.46%, Time: 12.10s \nIter: 1100, Train Loss: 0.1104, Train Acc: 100.00%, Val Loss:  2.986, Val Acc: 67.75%, Time: 43.16s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.40      0.72      0.52       119\n                   Filter       0.58      0.78      0.67       210\n    Compute Derived Value       0.72      0.30      0.42       192\n            Find Extremum       0.57      0.68      0.62       202\n                     Sort       0.92      0.87      0.89       163\n          Determine Range       0.56      0.78      0.66       144\nCharacterize Distribution       0.79      0.66      0.72       212\n           Find Anomalies       0.90      0.81      0.86       156\n                  Cluster       0.79      0.69      0.74       139\n                Correlate       0.82      0.49      0.61       181\n\n                micro avg       0.67      0.67      0.67      1718\n                macro avg       0.71      0.68      0.67      1718\n             weighted avg       0.71      0.67      0.67      1718\n\nConfusion Matrix...\n[[ 86  16   0   1   0  10   4   0   2   0]\n [ 20 164   8   4   0   8   2   4   0   0]\n [ 49  23  58  43   0  16   0   2   0   1]\n [ 16  22   0 137   2  20   1   2   2   0]\n [  1   3   0  13 142   4   0   0   0   0]\n [  7   4   1  12   2 113   3   0   2   0]\n [ 15  12   9  15   2   8 139   4   6   2]\n [  4  19   0   0   0   2   0 127   0   4]\n [  4   1   0   0   5  13   7   1  96  12]\n [ 12  19   5  15   2   7  19   1  13  88]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.218, Train Acc: 46.88%, Val Loss:  3.156, Val Acc: 48.27%, Time: 31.93s *\nEpoch: 2\nIter: 200, Train Loss:  1.841, Train Acc: 75.00%, Val Loss:   2.24, Val Acc: 64.01%, Time: 5.04s *\nIter: 300, Train Loss: 0.8532, Train Acc: 89.06%, Val Loss:  2.056, Val Acc: 68.51%, Time: 36.28s *\nEpoch: 3\nIter: 400, Train Loss: 0.5452, Train Acc: 96.88%, Val Loss:  1.917, Val Acc: 70.95%, Time: 7.94s *\nIter: 500, Train Loss:  1.045, Train Acc: 84.38%, Val Loss:  2.075, Val Acc: 71.31%, Time: 39.16s *\nEpoch: 4\nIter: 600, Train Loss: 0.9448, Train Acc: 87.50%, Val Loss:   2.07, Val Acc: 70.95%, Time: 10.76s \nIter: 700, Train Loss: 0.5905, Train Acc: 92.19%, Val Loss:  1.852, Val Acc: 73.80%, Time: 41.99s *\nEpoch: 5\nIter: 800, Train Loss: 0.4145, Train Acc: 95.31%, Val Loss:  1.982, Val Acc: 73.02%, Time: 13.81s \nIter: 900, Train Loss:  0.205, Train Acc: 98.44%, Val Loss:  2.123, Val Acc: 72.81%, Time: 45.06s \nEpoch: 6\nIter: 1000, Train Loss: 0.2675, Train Acc: 98.44%, Val Loss:  1.951, Val Acc: 74.05%, Time: 16.66s *\nIter: 1100, Train Loss: 0.1899, Train Acc: 98.44%, Val Loss:  2.433, Val Acc: 70.22%, Time: 47.98s \n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 7\nIter: 1200, Train Loss: 0.3803, Train Acc: 95.31%, Val Loss:  2.506, Val Acc: 71.05%, Time: 19.65s \nIter: 1300, Train Loss: 0.2449, Train Acc: 95.31%, Val Loss:  2.472, Val Acc: 71.21%, Time: 51.01s \nEpoch: 8\nIter: 1400, Train Loss: 0.2922, Train Acc: 95.31%, Val Loss:  2.545, Val Acc: 71.31%, Time: 22.59s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.59      0.67      0.63       174\n                   Filter       0.64      0.72      0.68       152\n    Compute Derived Value       0.64      0.55      0.59       228\n            Find Extremum       0.87      0.72      0.79       218\n                     Sort       0.88      0.85      0.86       171\n          Determine Range       0.68      0.59      0.63       180\nCharacterize Distribution       0.75      0.79      0.77       199\n           Find Anomalies       0.82      0.63      0.71       190\n                  Cluster       0.79      0.76      0.78       208\n                Correlate       0.56      0.81      0.66       211\n\n                micro avg       0.71      0.71      0.71      1931\n                macro avg       0.72      0.71      0.71      1931\n             weighted avg       0.72      0.71      0.71      1931\n\nConfusion Matrix...\n[[117   1  15   0   0   9   0   0   0  32]\n [ 23 110   4   0   0   1   2  10   2   0]\n [ 18  29 126   0   1  23  11   0   6  14]\n [ 17   5  24 158   0   7   0   0   0   7]\n [  0   0   0  17 145   1   1   0   6   1]\n [ 13  15   0   3  17 106  16   0   9   1]\n [  0   0   0   0   2   0 158   7   9  23]\n [  0   7   8   1   0   0  10 120   8  36]\n [  0   4   2   2   0   8   8   5 159  20]\n [ 10   1  18   0   0   0   4   5   2 171]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.182, Train Acc: 50.00%, Val Loss:  3.444, Val Acc: 41.59%, Time: 31.13s *\nIter: 200, Train Loss:  1.822, Train Acc: 73.44%, Val Loss:  2.741, Val Acc: 55.65%, Time: 61.48s *\nEpoch: 2\nIter: 300, Train Loss:  1.421, Train Acc: 71.88%, Val Loss:  2.413, Val Acc: 62.12%, Time: 29.87s *\nIter: 400, Train Loss:  1.055, Train Acc: 81.25%, Val Loss:  2.355, Val Acc: 64.54%, Time: 60.38s *\nEpoch: 3\nIter: 500, Train Loss: 0.7339, Train Acc: 90.62%, Val Loss:  2.312, Val Acc: 65.49%, Time: 29.23s *\nIter: 600, Train Loss: 0.8446, Train Acc: 87.50%, Val Loss:  2.262, Val Acc: 66.95%, Time: 59.58s *\nEpoch: 4\nIter: 700, Train Loss: 0.7635, Train Acc: 85.94%, Val Loss:  2.509, Val Acc: 65.75%, Time: 28.66s \nIter: 800, Train Loss: 0.7517, Train Acc: 87.50%, Val Loss:  2.168, Val Acc: 69.89%, Time: 59.18s *\nEpoch: 5\nIter: 900, Train Loss: 0.2859, Train Acc: 95.31%, Val Loss:  2.432, Val Acc: 69.20%, Time: 27.96s \nIter: 1000, Train Loss: 0.2396, Train Acc: 95.31%, Val Loss:  2.562, Val Acc: 67.47%, Time: 58.29s \nEpoch: 6\nIter: 1100, Train Loss: 0.4126, Train Acc: 92.19%, Val Loss:  2.666, Val Acc: 66.01%, Time: 27.45s \nIter: 1200, Train Loss: 0.2494, Train Acc: 95.31%, Val Loss:  2.708, Val Acc: 67.64%, Time: 57.73s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.57      0.56       121\n                   Filter       0.46      0.64      0.54        92\n    Compute Derived Value       0.68      0.62      0.65       157\n            Find Extremum       0.74      0.79      0.76       106\n                     Sort       0.97      0.66      0.78        99\n          Determine Range       0.67      0.62      0.64       173\nCharacterize Distribution       0.70      0.88      0.78        99\n           Find Anomalies       0.67      0.79      0.72       101\n                  Cluster       0.87      0.62      0.72        95\n                Correlate       0.64      0.58      0.61       116\n\n                micro avg       0.67      0.67      0.67      1159\n                macro avg       0.69      0.68      0.68      1159\n             weighted avg       0.69      0.67      0.67      1159\n\nConfusion Matrix...\n[[ 69  24  12   0   0  15   0   0   1   0]\n [  1  59   1   5   0   9   0  14   0   3]\n [ 16   0  98   8   0  17   5   7   2   4]\n [  3   7   7  84   0   1   4   0   0   0]\n [  0   4   0  14  65   5   2   6   2   1]\n [ 23  12  19   0   2 108   7   1   1   0]\n [  3   0   5   1   0   2  87   0   0   1]\n [  0  15   0   0   0   3   0  80   0   3]\n [  3   3   0   2   0   0   1   1  59  26]\n [  9   4   2   0   0   2  18  11   3  67]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  3.198, Train Acc: 53.12%, Val Loss:  3.381, Val Acc: 43.76%, Time: 30.99s *\nIter: 200, Train Loss:  1.902, Train Acc: 71.88%, Val Loss:  2.443, Val Acc: 59.72%, Time: 61.18s *\nEpoch: 2\nIter: 300, Train Loss:  1.633, Train Acc: 78.12%, Val Loss:  2.018, Val Acc: 68.35%, Time: 29.44s *\nIter: 400, Train Loss:  1.037, Train Acc: 82.81%, Val Loss:  1.922, Val Acc: 71.01%, Time: 59.64s *\nEpoch: 3\nIter: 500, Train Loss:  0.536, Train Acc: 90.62%, Val Loss:  2.151, Val Acc: 69.63%, Time: 28.43s \nIter: 600, Train Loss: 0.8595, Train Acc: 82.81%, Val Loss:  1.945, Val Acc: 71.47%, Time: 58.63s *\nEpoch: 4\nIter: 700, Train Loss: 0.6975, Train Acc: 90.62%, Val Loss:  2.232, Val Acc: 68.90%, Time: 27.63s \nIter: 800, Train Loss: 0.5432, Train Acc: 90.62%, Val Loss:  2.027, Val Acc: 71.10%, Time: 57.72s \nEpoch: 5\nIter: 900, Train Loss: 0.3823, Train Acc: 90.62%, Val Loss:  2.317, Val Acc: 70.28%, Time: 26.65s \nIter: 1000, Train Loss: 0.3903, Train Acc: 95.31%, Val Loss:  2.375, Val Acc: 67.89%, Time: 56.87s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.58      0.67      0.62       101\n                   Filter       0.65      0.37      0.47       122\n    Compute Derived Value       0.53      0.61      0.57       115\n            Find Extremum       0.85      0.75      0.79        95\n                     Sort       0.71      0.80      0.75        93\n          Determine Range       0.48      0.62      0.54       112\nCharacterize Distribution       0.72      0.79      0.76       102\n           Find Anomalies       0.93      0.73      0.82       125\n                  Cluster       0.89      0.66      0.76       110\n                Correlate       0.77      0.99      0.87       115\n\n                micro avg       0.69      0.69      0.69      1090\n                macro avg       0.71      0.70      0.70      1090\n             weighted avg       0.71      0.69      0.69      1090\n\nConfusion Matrix...\n[[ 68   1  28   0   0   2   1   0   1   0]\n [ 17  45   7   0   6  45   1   0   1   0]\n [ 10   3  70   0  12  11   5   4   0   0]\n [  2   3  11  71   1   4   0   1   1   1]\n [  1   0   2   0  74   0  15   0   1   0]\n [ 10   5   8   8   7  70   3   0   0   1]\n [  2   0   2   0   0   2  81   1   4  10]\n [  1  12   3   4   0   9   1  91   1   3]\n [  6   0   0   1   4   1   5   1  73  19]\n [  0   0   0   0   0   1   0   0   0 114]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.991, Train Acc: 54.69%, Val Loss:  3.254, Val Acc: 46.55%, Time: 31.44s *\nEpoch: 2\nIter: 200, Train Loss:  1.472, Train Acc: 81.25%, Val Loss:   2.39, Val Acc: 61.83%, Time: 2.88s *\nIter: 300, Train Loss:  1.909, Train Acc: 70.31%, Val Loss:  1.982, Val Acc: 71.12%, Time: 33.71s *\nEpoch: 3\nIter: 400, Train Loss:  1.193, Train Acc: 82.81%, Val Loss:  1.942, Val Acc: 69.63%, Time: 4.03s \nIter: 500, Train Loss: 0.6389, Train Acc: 92.19%, Val Loss:  1.911, Val Acc: 72.73%, Time: 34.80s *\nEpoch: 4\nIter: 600, Train Loss:  0.891, Train Acc: 87.50%, Val Loss:  1.823, Val Acc: 73.95%, Time: 5.18s *\nIter: 700, Train Loss: 0.6075, Train Acc: 92.19%, Val Loss:  1.878, Val Acc: 73.05%, Time: 35.92s \nEpoch: 5\nIter: 800, Train Loss: 0.3547, Train Acc: 93.75%, Val Loss:  1.933, Val Acc: 74.40%, Time: 6.40s *\nIter: 900, Train Loss: 0.3742, Train Acc: 93.75%, Val Loss:  1.831, Val Acc: 74.08%, Time: 37.17s \nEpoch: 6\nIter: 1000, Train Loss: 0.1551, Train Acc: 96.88%, Val Loss:  2.044, Val Acc: 71.31%, Time: 7.53s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1100, Train Loss: 0.2051, Train Acc: 96.88%, Val Loss:  2.007, Val Acc: 74.02%, Time: 38.32s \nEpoch: 7\nIter: 1200, Train Loss: 0.1001, Train Acc: 98.44%, Val Loss:  2.174, Val Acc: 71.18%, Time: 8.76s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.66      0.78      0.71       146\n                   Filter       0.57      0.64      0.60       178\n    Compute Derived Value       0.49      0.75      0.60       145\n            Find Extremum       0.85      0.56      0.68       172\n                     Sort       0.76      0.73      0.75       142\n          Determine Range       0.79      0.74      0.77       154\nCharacterize Distribution       0.87      0.79      0.83       155\n           Find Anomalies       0.76      0.70      0.73       148\n                  Cluster       0.79      0.78      0.78       156\n                Correlate       0.85      0.72      0.78       155\n\n                micro avg       0.72      0.72      0.72      1551\n                macro avg       0.74      0.72      0.72      1551\n             weighted avg       0.74      0.72      0.72      1551\n\nConfusion Matrix...\n[[114   3  24   0   0   2   2   1   0   0]\n [ 29 114   7   7   5   7   0   4   3   2]\n [ 14  12 109   0   0   0   3   0   1   6]\n [  4   7  40  97   4   8   2   2   5   3]\n [  3   6   4   6 104   7   1   1  10   0]\n [  3   1  20   3   1 114   4   1   1   6]\n [  4   2   7   0  12   4 123   0   3   0]\n [  0  33   4   1   0   1   0 103   3   3]\n [  2   7   2   0  10   1   5   8 121   0]\n [  0  16   4   0   1   0   2  15   6 111]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.921, Train Acc: 60.94%, Val Loss:  3.012, Val Acc: 52.24%, Time: 30.32s *\nIter: 200, Train Loss:  1.693, Train Acc: 75.00%, Val Loss:   2.49, Val Acc: 55.48%, Time: 60.09s *\nEpoch: 2\nIter: 300, Train Loss:  1.264, Train Acc: 82.81%, Val Loss:  2.235, Val Acc: 64.09%, Time: 26.49s *\nIter: 400, Train Loss:   1.25, Train Acc: 81.25%, Val Loss:  1.939, Val Acc: 68.76%, Time: 56.22s *\nEpoch: 3\nIter: 500, Train Loss: 0.8831, Train Acc: 89.06%, Val Loss:  1.991, Val Acc: 70.02%, Time: 23.25s *\nIter: 600, Train Loss: 0.4332, Train Acc: 95.31%, Val Loss:  2.132, Val Acc: 67.32%, Time: 52.98s \nEpoch: 4\nIter: 700, Train Loss: 0.3245, Train Acc: 96.88%, Val Loss:  2.424, Val Acc: 67.15%, Time: 20.17s \nIter: 800, Train Loss: 0.7689, Train Acc: 90.62%, Val Loss:   2.38, Val Acc: 67.32%, Time: 49.90s \nEpoch: 5\nIter: 900, Train Loss: 0.6896, Train Acc: 87.50%, Val Loss:  2.584, Val Acc: 67.32%, Time: 16.91s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.56      0.78      0.65        65\n                   Filter       0.56      1.00      0.72        32\n    Compute Derived Value       0.82      0.34      0.48        82\n            Find Extremum       0.76      0.55      0.64       107\n                     Sort       0.81      0.78      0.80        60\n          Determine Range       0.69      0.86      0.77        44\nCharacterize Distribution       0.81      0.93      0.87        46\n           Find Anomalies       1.00      0.53      0.69        47\n                  Cluster       0.55      1.00      0.71        26\n                Correlate       0.58      0.71      0.64        48\n\n                micro avg       0.69      0.69      0.69       557\n                macro avg       0.71      0.75      0.70       557\n             weighted avg       0.73      0.69      0.68       557\n\nConfusion Matrix...\n[[51  2  4  0  1  5  0  0  0  2]\n [ 0 32  0  0  0  0  0  0  0  0]\n [18  1 28 18  1  2  5  0  4  5]\n [18 13  1 59  5  0  2  0  3  6]\n [ 1  0  0  0 47  8  0  0  4  0]\n [ 0  0  0  1  1 38  3  0  0  1]\n [ 0  0  1  0  0  2 43  0  0  0]\n [ 0  9  0  0  0  0  0 25  2 11]\n [ 0  0  0  0  0  0  0  0 26  0]\n [ 3  0  0  0  3  0  0  0  8 34]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 100, Train Loss:  2.981, Train Acc: 46.88%, Val Loss:  3.218, Val Acc: 50.66%, Time: 31.52s *\nEpoch: 2\nIter: 200, Train Loss:    1.7, Train Acc: 68.75%, Val Loss:  2.239, Val Acc: 64.04%, Time: 2.85s *\nIter: 300, Train Loss:   1.19, Train Acc: 81.25%, Val Loss:  2.126, Val Acc: 66.93%, Time: 33.61s *\nEpoch: 3\nIter: 400, Train Loss:  1.231, Train Acc: 84.38%, Val Loss:  2.121, Val Acc: 68.24%, Time: 4.07s *\nIter: 500, Train Loss: 0.6857, Train Acc: 90.62%, Val Loss:  1.922, Val Acc: 69.16%, Time: 34.88s *\nEpoch: 4\nIter: 600, Train Loss: 0.6579, Train Acc: 90.62%, Val Loss:  2.087, Val Acc: 68.64%, Time: 5.20s \nIter: 700, Train Loss: 0.5875, Train Acc: 92.19%, Val Loss:  2.277, Val Acc: 66.99%, Time: 36.02s \nEpoch: 5\nIter: 800, Train Loss: 0.7234, Train Acc: 89.06%, Val Loss:  2.554, Val Acc: 64.70%, Time: 6.32s \nIter: 900, Train Loss: 0.5397, Train Acc: 90.62%, Val Loss:  2.376, Val Acc: 67.72%, Time: 37.06s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.54      0.68      0.60       206\n                   Filter       0.47      0.59      0.52       202\n    Compute Derived Value       0.69      0.24      0.36       165\n            Find Extremum       0.76      0.83      0.79       229\n                     Sort       0.93      0.75      0.83        84\n          Determine Range       0.33      0.30      0.31        91\nCharacterize Distribution       0.82      0.87      0.84       119\n           Find Anomalies       0.69      0.65      0.67       169\n                  Cluster       0.95      0.93      0.94        84\n                Correlate       0.94      0.97      0.96       175\n\n                micro avg       0.68      0.68      0.68      1524\n                macro avg       0.71      0.68      0.68      1524\n             weighted avg       0.69      0.68      0.67      1524\n\nConfusion Matrix...\n[[141   7   3  19   0   6   0  24   0   6]\n [ 21 119   0  15   0  37   1   9   0   0]\n [ 60  40  40  14   0   0   4   6   0   1]\n [ 10  17   0 190   5   1   0   6   0   0]\n [  6   3   1   3  63   4   1   0   3   0]\n [ 11  42   6   5   0  27   0   0   0   0]\n [  6   1   4   0   0   5 103   0   0   0]\n [  5  25   4   3   0   3  15 110   0   4]\n [  2   0   0   1   0   0   1   2  78   0]\n [  0   1   0   0   0   0   0   3   1 170]]\n[0.6729713239780354, 0.610718492343934, 0.6986301369863014, 0.6693830034924331, 0.709476954945624, 0.6695427092320967, 0.6944954128440367, 0.7156673114119922, 0.6876122082585279, 0.6830708661417323]\n0.6811568419634715, 0.027970779768870198, 0.029483790666862627, 0.0007823645208786383\n", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data,test_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, test_data, split_type))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "random\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.414, Train Acc: 23.44%, Val Loss:  4.496, Val Acc: 20.73%, Time: 8.94s *\nIter: 60, Train Loss:  4.081, Train Acc: 37.50%, Val Loss:  4.005, Val Acc: 33.90%, Time: 14.99s *\nIter: 90, Train Loss:   3.11, Train Acc: 48.44%, Val Loss:  3.324, Val Acc: 45.30%, Time: 21.09s *\nIter: 120, Train Loss:  2.557, Train Acc: 64.06%, Val Loss:  2.783, Val Acc: 57.26%, Time: 27.10s *\nIter: 150, Train Loss:  2.509, Train Acc: 59.38%, Val Loss:  2.489, Val Acc: 60.40%, Time: 33.15s *\nIter: 180, Train Loss:  2.189, Train Acc: 70.31%, Val Loss:  2.067, Val Acc: 67.81%, Time: 39.19s *\nEpoch: 2\nIter: 210, Train Loss:  1.462, Train Acc: 79.69%, Val Loss:  1.929, Val Acc: 70.58%, Time: 3.33s *\nIter: 240, Train Loss:   1.39, Train Acc: 76.56%, Val Loss:  1.716, Val Acc: 72.86%, Time: 9.38s *\nIter: 270, Train Loss:  1.408, Train Acc: 79.69%, Val Loss:  1.533, Val Acc: 76.14%, Time: 15.43s *\nIter: 300, Train Loss: 0.8514, Train Acc: 87.50%, Val Loss:  1.446, Val Acc: 76.92%, Time: 21.55s *\nIter: 330, Train Loss:  1.495, Train Acc: 78.12%, Val Loss:  1.331, Val Acc: 79.49%, Time: 27.55s *\nIter: 360, Train Loss:  1.529, Train Acc: 78.12%, Val Loss:  1.352, Val Acc: 78.35%, Time: 33.62s \nIter: 390, Train Loss:  1.277, Train Acc: 76.56%, Val Loss:  1.188, Val Acc: 81.70%, Time: 39.70s *\nEpoch: 3\nIter: 420, Train Loss:  1.751, Train Acc: 76.56%, Val Loss:  1.121, Val Acc: 82.91%, Time: 5.17s *\nIter: 450, Train Loss:  1.061, Train Acc: 81.25%, Val Loss:  1.092, Val Acc: 82.98%, Time: 11.26s *\nIter: 480, Train Loss: 0.9507, Train Acc: 82.81%, Val Loss:  1.073, Val Acc: 83.05%, Time: 17.30s *\nIter: 510, Train Loss: 0.5911, Train Acc: 87.50%, Val Loss:  1.018, Val Acc: 84.40%, Time: 23.35s *\nIter: 540, Train Loss: 0.8124, Train Acc: 87.50%, Val Loss:  1.076, Val Acc: 82.12%, Time: 29.37s \nIter: 570, Train Loss: 0.4745, Train Acc: 93.75%, Val Loss: 0.9637, Val Acc: 85.19%, Time: 35.36s *\nEpoch: 4\nIter: 600, Train Loss: 0.7241, Train Acc: 90.62%, Val Loss: 0.9188, Val Acc: 85.61%, Time: 2.42s *\nIter: 630, Train Loss: 0.4021, Train Acc: 93.75%, Val Loss: 0.9118, Val Acc: 85.75%, Time: 8.47s *\nIter: 660, Train Loss:  0.337, Train Acc: 98.44%, Val Loss: 0.8664, Val Acc: 87.04%, Time: 14.55s *\nIter: 690, Train Loss:   0.58, Train Acc: 93.75%, Val Loss: 0.8617, Val Acc: 86.75%, Time: 20.66s \nIter: 720, Train Loss: 0.3303, Train Acc: 98.44%, Val Loss: 0.8169, Val Acc: 86.54%, Time: 26.69s \nIter: 750, Train Loss: 0.6646, Train Acc: 90.62%, Val Loss: 0.8314, Val Acc: 85.83%, Time: 32.71s \nIter: 780, Train Loss: 0.6195, Train Acc: 93.75%, Val Loss: 0.8075, Val Acc: 87.39%, Time: 38.85s *\nEpoch: 5\nIter: 810, Train Loss: 0.9282, Train Acc: 85.94%, Val Loss: 0.7882, Val Acc: 87.46%, Time: 4.37s *\nIter: 840, Train Loss: 0.2997, Train Acc: 93.75%, Val Loss: 0.8935, Val Acc: 86.47%, Time: 10.43s \nIter: 870, Train Loss: 0.6429, Train Acc: 90.62%, Val Loss: 0.7648, Val Acc: 87.82%, Time: 16.50s *\nIter: 900, Train Loss: 0.5874, Train Acc: 92.19%, Val Loss:  0.751, Val Acc: 88.18%, Time: 22.58s *\nIter: 930, Train Loss: 0.3749, Train Acc: 92.19%, Val Loss: 0.7458, Val Acc: 88.46%, Time: 28.71s *\nIter: 960, Train Loss: 0.2987, Train Acc: 93.75%, Val Loss:  0.759, Val Acc: 89.10%, Time: 34.74s *\nIter: 990, Train Loss: 0.1393, Train Acc: 100.00%, Val Loss: 0.6721, Val Acc: 89.74%, Time: 40.85s *\nEpoch: 6\nIter: 1020, Train Loss: 0.3706, Train Acc: 93.75%, Val Loss:  0.717, Val Acc: 89.67%, Time: 6.03s \nIter: 1050, Train Loss: 0.08887, Train Acc: 100.00%, Val Loss: 0.6584, Val Acc: 90.17%, Time: 12.06s *\nIter: 1080, Train Loss: 0.2217, Train Acc: 98.44%, Val Loss: 0.7124, Val Acc: 89.10%, Time: 18.21s \nIter: 1110, Train Loss: 0.3592, Train Acc: 96.88%, Val Loss: 0.6053, Val Acc: 90.95%, Time: 24.28s *\nIter: 1140, Train Loss: 0.3832, Train Acc: 93.75%, Val Loss: 0.6125, Val Acc: 90.74%, Time: 30.32s \nIter: 1170, Train Loss: 0.1237, Train Acc: 100.00%, Val Loss: 0.6923, Val Acc: 89.89%, Time: 36.37s \nEpoch: 7\nIter: 1200, Train Loss: 0.3273, Train Acc: 95.31%, Val Loss: 0.6407, Val Acc: 90.60%, Time: 3.35s \nIter: 1230, Train Loss: 0.2198, Train Acc: 96.88%, Val Loss: 0.7158, Val Acc: 90.17%, Time: 9.39s \nIter: 1260, Train Loss: 0.2168, Train Acc: 98.44%, Val Loss: 0.6482, Val Acc: 90.67%, Time: 15.39s \nIter: 1290, Train Loss: 0.1907, Train Acc: 96.88%, Val Loss: 0.6077, Val Acc: 91.60%, Time: 21.45s *\nIter: 1320, Train Loss: 0.2005, Train Acc: 98.44%, Val Loss: 0.7151, Val Acc: 90.38%, Time: 27.47s \nIter: 1350, Train Loss: 0.3352, Train Acc: 96.88%, Val Loss: 0.6932, Val Acc: 90.46%, Time: 33.44s \nIter: 1380, Train Loss:  0.401, Train Acc: 96.88%, Val Loss: 0.7105, Val Acc: 89.67%, Time: 39.45s \nEpoch: 8\nIter: 1410, Train Loss: 0.1312, Train Acc: 98.44%, Val Loss: 0.6139, Val Acc: 91.10%, Time: 5.19s \nIter: 1440, Train Loss: 0.1321, Train Acc: 98.44%, Val Loss: 0.6763, Val Acc: 90.81%, Time: 11.29s \nIter: 1470, Train Loss: 0.1546, Train Acc: 98.44%, Val Loss: 0.6022, Val Acc: 91.95%, Time: 17.31s *\nIter: 1500, Train Loss: 0.2437, Train Acc: 98.44%, Val Loss: 0.6647, Val Acc: 91.03%, Time: 23.39s \nIter: 1530, Train Loss: 0.2101, Train Acc: 96.88%, Val Loss: 0.7024, Val Acc: 90.81%, Time: 29.48s \nIter: 1560, Train Loss: 0.1483, Train Acc: 98.44%, Val Loss:  0.674, Val Acc: 91.10%, Time: 35.57s \nEpoch: 9\nIter: 1590, Train Loss: 0.1332, Train Acc: 98.44%, Val Loss: 0.6572, Val Acc: 91.38%, Time: 2.41s \nIter: 1620, Train Loss: 0.1313, Train Acc: 96.88%, Val Loss: 0.6504, Val Acc: 91.38%, Time: 8.53s \nIter: 1650, Train Loss: 0.08099, Train Acc: 98.44%, Val Loss: 0.7943, Val Acc: 89.46%, Time: 14.58s \nIter: 1680, Train Loss: 0.08035, Train Acc: 100.00%, Val Loss: 0.7275, Val Acc: 90.88%, Time: 20.68s \nIter: 1710, Train Loss: 0.07123, Train Acc: 100.00%, Val Loss: 0.6869, Val Acc: 91.17%, Time: 26.78s \nIter: 1740, Train Loss: 0.04329, Train Acc: 100.00%, Val Loss: 0.7307, Val Acc: 90.60%, Time: 32.79s \nIter: 1770, Train Loss: 0.04291, Train Acc: 100.00%, Val Loss: 0.7031, Val Acc: 91.67%, Time: 38.99s \nEpoch: 10\nIter: 1800, Train Loss: 0.02727, Train Acc: 100.00%, Val Loss: 0.6777, Val Acc: 90.88%, Time: 4.28s \nIter: 1830, Train Loss: 0.1326, Train Acc: 98.44%, Val Loss: 0.7345, Val Acc: 90.17%, Time: 10.35s \nIter: 1860, Train Loss: 0.08144, Train Acc: 98.44%, Val Loss: 0.6502, Val Acc: 91.10%, Time: 16.42s \nIter: 1890, Train Loss: 0.03616, Train Acc: 100.00%, Val Loss: 0.6551, Val Acc: 90.60%, Time: 22.50s \nIter: 1920, Train Loss: 0.05453, Train Acc: 100.00%, Val Loss:  0.617, Val Acc: 92.24%, Time: 28.62s *\nIter: 1950, Train Loss: 0.2592, Train Acc: 93.75%, Val Loss: 0.6678, Val Acc: 91.45%, Time: 34.71s \nIter: 1980, Train Loss: 0.04262, Train Acc: 100.00%, Val Loss: 0.6969, Val Acc: 91.31%, Time: 40.84s \nEpoch: 11\nIter: 2010, Train Loss: 0.1168, Train Acc: 98.44%, Val Loss: 0.7825, Val Acc: 90.74%, Time: 6.11s \nIter: 2040, Train Loss: 0.04724, Train Acc: 98.44%, Val Loss: 0.6928, Val Acc: 91.38%, Time: 12.12s \nIter: 2070, Train Loss: 0.1771, Train Acc: 96.88%, Val Loss: 0.6722, Val Acc: 91.81%, Time: 18.16s \nIter: 2100, Train Loss: 0.04227, Train Acc: 100.00%, Val Loss: 0.6478, Val Acc: 92.66%, Time: 24.18s *\nIter: 2130, Train Loss: 0.03434, Train Acc: 100.00%, Val Loss: 0.6784, Val Acc: 92.52%, Time: 30.25s \nIter: 2160, Train Loss: 0.08879, Train Acc: 98.44%, Val Loss: 0.7376, Val Acc: 90.74%, Time: 36.27s \nEpoch: 12\nIter: 2190, Train Loss: 0.03069, Train Acc: 98.44%, Val Loss:  0.764, Val Acc: 91.24%, Time: 3.37s \nIter: 2220, Train Loss: 0.01112, Train Acc: 100.00%, Val Loss: 0.7211, Val Acc: 92.24%, Time: 9.43s \nIter: 2250, Train Loss: 0.01612, Train Acc: 100.00%, Val Loss: 0.7138, Val Acc: 92.02%, Time: 15.46s \nIter: 2280, Train Loss: 0.1724, Train Acc: 95.31%, Val Loss: 0.8214, Val Acc: 90.60%, Time: 21.57s \nIter: 2310, Train Loss: 0.01598, Train Acc: 100.00%, Val Loss: 0.7419, Val Acc: 91.81%, Time: 27.72s \nIter: 2340, Train Loss: 0.02858, Train Acc: 100.00%, Val Loss: 0.7517, Val Acc: 91.38%, Time: 33.78s \nIter: 2370, Train Loss: 0.01233, Train Acc: 100.00%, Val Loss: 0.7394, Val Acc: 91.45%, Time: 39.92s \nEpoch: 13\nIter: 2400, Train Loss: 0.01447, Train Acc: 100.00%, Val Loss: 0.7022, Val Acc: 91.31%, Time: 5.14s \nIter: 2430, Train Loss: 0.05738, Train Acc: 98.44%, Val Loss: 0.7335, Val Acc: 91.45%, Time: 11.20s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 2460, Train Loss: 0.1755, Train Acc: 95.31%, Val Loss: 0.7304, Val Acc: 91.45%, Time: 17.20s \nIter: 2490, Train Loss: 0.0112, Train Acc: 100.00%, Val Loss: 0.7191, Val Acc: 91.31%, Time: 23.27s \nIter: 2520, Train Loss: 0.05898, Train Acc: 98.44%, Val Loss: 0.7248, Val Acc: 92.02%, Time: 29.37s \nIter: 2550, Train Loss: 0.05714, Train Acc: 98.44%, Val Loss: 0.7664, Val Acc: 91.88%, Time: 35.39s \nEpoch: 14\nIter: 2580, Train Loss: 0.01778, Train Acc: 100.00%, Val Loss: 0.7104, Val Acc: 91.74%, Time: 2.54s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.87      0.87      0.87       143\n                   Filter       0.92      0.89      0.90       170\n    Compute Derived Value       0.87      0.96      0.91       138\n            Find Extremum       0.98      0.98      0.98       180\n                     Sort       0.96      0.98      0.97       110\n          Determine Range       0.94      0.85      0.89       126\nCharacterize Distribution       0.91      0.89      0.90       129\n           Find Anomalies       0.91      0.94      0.92       125\n                  Cluster       0.90      0.94      0.92       120\n                Correlate       0.96      0.92      0.94       163\n\n                micro avg       0.92      0.92      0.92      1404\n                macro avg       0.92      0.92      0.92      1404\n             weighted avg       0.92      0.92      0.92      1404\n\nConfusion Matrix...\n[[124   5   5   0   0   2   1   2   1   3]\n [  5 152   3   0   0   3   1   3   3   0]\n [  2   2 132   0   0   0   1   1   0   0]\n [  0   0   1 177   1   0   0   1   0   0]\n [  0   0   0   0 108   0   0   0   2   0]\n [  6   5   1   1   0 107   4   0   1   1]\n [  1   1   4   1   2   1 115   0   2   2]\n [  1   1   1   0   0   0   5 117   0   0]\n [  0   0   0   2   1   0   0   3 113   1]\n [  3   0   4   0   0   1   0   2   3 150]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.566, Train Acc: 10.94%, Val Loss:  4.506, Val Acc: 11.25%, Time: 7.36s *\nIter: 60, Train Loss:  3.817, Train Acc: 42.19%, Val Loss:  3.925, Val Acc: 35.83%, Time: 13.46s *\nIter: 90, Train Loss:  3.324, Train Acc: 43.75%, Val Loss:  3.286, Val Acc: 44.52%, Time: 19.43s *\nIter: 120, Train Loss:  2.764, Train Acc: 59.38%, Val Loss:  2.686, Val Acc: 58.19%, Time: 25.66s *\nIter: 150, Train Loss:  2.193, Train Acc: 67.19%, Val Loss:  2.359, Val Acc: 62.75%, Time: 31.78s *\nIter: 180, Train Loss:  1.721, Train Acc: 78.12%, Val Loss:  2.052, Val Acc: 65.60%, Time: 37.76s *\nEpoch: 2\nIter: 210, Train Loss:  1.935, Train Acc: 76.56%, Val Loss:  1.956, Val Acc: 69.16%, Time: 3.31s *\nIter: 240, Train Loss:  1.717, Train Acc: 75.00%, Val Loss:  1.747, Val Acc: 71.72%, Time: 9.35s *\nIter: 270, Train Loss:  2.106, Train Acc: 64.06%, Val Loss:  1.667, Val Acc: 73.65%, Time: 15.48s *\nIter: 300, Train Loss:  1.475, Train Acc: 76.56%, Val Loss:  1.547, Val Acc: 75.71%, Time: 21.56s *\nIter: 330, Train Loss:  1.117, Train Acc: 81.25%, Val Loss:  1.433, Val Acc: 77.42%, Time: 27.64s *\nIter: 360, Train Loss:  1.143, Train Acc: 82.81%, Val Loss:  1.404, Val Acc: 77.28%, Time: 33.70s \nIter: 390, Train Loss:  1.372, Train Acc: 81.25%, Val Loss:   1.27, Val Acc: 80.48%, Time: 39.71s *\nEpoch: 3\nIter: 420, Train Loss: 0.7626, Train Acc: 87.50%, Val Loss:  1.275, Val Acc: 79.56%, Time: 5.09s \nIter: 450, Train Loss:  1.146, Train Acc: 84.38%, Val Loss:  1.286, Val Acc: 79.49%, Time: 11.08s \nIter: 480, Train Loss: 0.8162, Train Acc: 89.06%, Val Loss:  1.191, Val Acc: 82.26%, Time: 17.12s *\nIter: 510, Train Loss: 0.7942, Train Acc: 92.19%, Val Loss:  1.194, Val Acc: 80.20%, Time: 23.15s \nIter: 540, Train Loss: 0.8012, Train Acc: 87.50%, Val Loss:  1.097, Val Acc: 82.62%, Time: 29.24s *\nIter: 570, Train Loss: 0.5902, Train Acc: 93.75%, Val Loss:  1.052, Val Acc: 83.83%, Time: 35.38s *\nEpoch: 4\nIter: 600, Train Loss: 0.5848, Train Acc: 90.62%, Val Loss:  1.073, Val Acc: 83.55%, Time: 2.41s \nIter: 630, Train Loss: 0.2765, Train Acc: 96.88%, Val Loss:  1.113, Val Acc: 82.48%, Time: 8.43s \nIter: 660, Train Loss: 0.6032, Train Acc: 89.06%, Val Loss: 0.9647, Val Acc: 84.97%, Time: 14.48s *\nIter: 690, Train Loss: 0.5667, Train Acc: 90.62%, Val Loss: 0.9577, Val Acc: 84.54%, Time: 20.47s \nIter: 720, Train Loss: 0.5264, Train Acc: 95.31%, Val Loss:  0.898, Val Acc: 86.04%, Time: 26.56s *\nIter: 750, Train Loss:  0.709, Train Acc: 87.50%, Val Loss: 0.9325, Val Acc: 86.18%, Time: 32.60s *\nIter: 780, Train Loss: 0.6734, Train Acc: 90.62%, Val Loss: 0.9073, Val Acc: 86.47%, Time: 38.60s *\nEpoch: 5\nIter: 810, Train Loss: 0.4247, Train Acc: 92.19%, Val Loss: 0.8642, Val Acc: 86.89%, Time: 4.20s *\nIter: 840, Train Loss:  0.615, Train Acc: 92.19%, Val Loss:  0.968, Val Acc: 85.04%, Time: 10.28s \nIter: 870, Train Loss: 0.3098, Train Acc: 95.31%, Val Loss:  0.931, Val Acc: 85.33%, Time: 16.41s \nIter: 900, Train Loss: 0.5151, Train Acc: 92.19%, Val Loss:  0.858, Val Acc: 86.68%, Time: 22.49s \nIter: 930, Train Loss: 0.3269, Train Acc: 93.75%, Val Loss: 0.8568, Val Acc: 87.25%, Time: 28.53s *\nIter: 960, Train Loss: 0.3484, Train Acc: 96.88%, Val Loss: 0.8579, Val Acc: 86.97%, Time: 34.54s \nIter: 990, Train Loss: 0.1235, Train Acc: 100.00%, Val Loss: 0.8107, Val Acc: 88.03%, Time: 40.49s *\nEpoch: 6\nIter: 1020, Train Loss: 0.1548, Train Acc: 98.44%, Val Loss: 0.8246, Val Acc: 87.68%, Time: 5.98s \nIter: 1050, Train Loss: 0.5986, Train Acc: 92.19%, Val Loss: 0.8507, Val Acc: 87.39%, Time: 12.16s \nIter: 1080, Train Loss: 0.1833, Train Acc: 98.44%, Val Loss:  0.815, Val Acc: 88.32%, Time: 18.27s *\nIter: 1110, Train Loss: 0.2332, Train Acc: 96.88%, Val Loss: 0.8697, Val Acc: 87.46%, Time: 24.38s \nIter: 1140, Train Loss: 0.1584, Train Acc: 98.44%, Val Loss: 0.8342, Val Acc: 88.46%, Time: 30.35s *\nIter: 1170, Train Loss: 0.1233, Train Acc: 98.44%, Val Loss: 0.8415, Val Acc: 88.39%, Time: 36.50s \nEpoch: 7\nIter: 1200, Train Loss: 0.2184, Train Acc: 98.44%, Val Loss: 0.7494, Val Acc: 89.39%, Time: 3.30s *\nIter: 1230, Train Loss:  0.258, Train Acc: 96.88%, Val Loss: 0.8728, Val Acc: 87.68%, Time: 9.30s \nIter: 1260, Train Loss: 0.1929, Train Acc: 98.44%, Val Loss: 0.8804, Val Acc: 88.18%, Time: 15.34s \nIter: 1290, Train Loss: 0.06466, Train Acc: 100.00%, Val Loss: 0.8189, Val Acc: 87.96%, Time: 21.40s \nIter: 1320, Train Loss:  0.136, Train Acc: 98.44%, Val Loss: 0.8085, Val Acc: 88.46%, Time: 27.46s \nIter: 1350, Train Loss: 0.2129, Train Acc: 98.44%, Val Loss: 0.8203, Val Acc: 89.46%, Time: 33.53s *\nIter: 1380, Train Loss: 0.09517, Train Acc: 98.44%, Val Loss: 0.8877, Val Acc: 87.75%, Time: 39.62s \nEpoch: 8\nIter: 1410, Train Loss: 0.05359, Train Acc: 100.00%, Val Loss: 0.7831, Val Acc: 89.53%, Time: 5.16s *\nIter: 1440, Train Loss: 0.3064, Train Acc: 96.88%, Val Loss: 0.7969, Val Acc: 88.89%, Time: 11.15s \nIter: 1470, Train Loss: 0.07706, Train Acc: 100.00%, Val Loss: 0.7403, Val Acc: 89.81%, Time: 17.32s *\nIter: 1500, Train Loss: 0.06967, Train Acc: 98.44%, Val Loss: 0.7859, Val Acc: 88.89%, Time: 23.32s \nIter: 1530, Train Loss: 0.1077, Train Acc: 100.00%, Val Loss: 0.8267, Val Acc: 89.03%, Time: 29.41s \nIter: 1560, Train Loss: 0.1721, Train Acc: 98.44%, Val Loss: 0.8515, Val Acc: 88.32%, Time: 35.47s \nEpoch: 9\nIter: 1590, Train Loss: 0.2056, Train Acc: 98.44%, Val Loss: 0.8739, Val Acc: 88.39%, Time: 2.46s \nIter: 1620, Train Loss: 0.1193, Train Acc: 98.44%, Val Loss: 0.8883, Val Acc: 87.61%, Time: 8.59s \nIter: 1650, Train Loss: 0.0546, Train Acc: 98.44%, Val Loss: 0.8775, Val Acc: 89.10%, Time: 14.63s \nIter: 1680, Train Loss: 0.04216, Train Acc: 100.00%, Val Loss: 0.8622, Val Acc: 88.60%, Time: 20.77s \nIter: 1710, Train Loss: 0.08188, Train Acc: 98.44%, Val Loss: 0.9105, Val Acc: 89.03%, Time: 26.89s \nIter: 1740, Train Loss: 0.09277, Train Acc: 98.44%, Val Loss: 0.8057, Val Acc: 88.75%, Time: 33.00s \nIter: 1770, Train Loss: 0.09816, Train Acc: 98.44%, Val Loss: 0.8337, Val Acc: 88.96%, Time: 39.13s \nEpoch: 10\nIter: 1800, Train Loss: 0.1867, Train Acc: 96.88%, Val Loss: 0.9535, Val Acc: 88.11%, Time: 4.23s \nIter: 1830, Train Loss: 0.1009, Train Acc: 100.00%, Val Loss: 0.9559, Val Acc: 88.18%, Time: 10.24s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1860, Train Loss: 0.03261, Train Acc: 100.00%, Val Loss: 0.8579, Val Acc: 88.96%, Time: 16.24s \nIter: 1890, Train Loss: 0.06901, Train Acc: 100.00%, Val Loss:  0.834, Val Acc: 89.74%, Time: 22.32s \nIter: 1920, Train Loss: 0.02308, Train Acc: 100.00%, Val Loss: 0.8916, Val Acc: 88.39%, Time: 28.41s \nIter: 1950, Train Loss: 0.02041, Train Acc: 100.00%, Val Loss: 0.8813, Val Acc: 88.53%, Time: 34.55s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.90      0.89      0.89       148\n                   Filter       0.85      0.85      0.85       163\n    Compute Derived Value       0.85      0.91      0.88       153\n            Find Extremum       0.91      0.93      0.92       160\n                     Sort       0.94      0.95      0.94       124\n          Determine Range       0.83      0.83      0.83       120\nCharacterize Distribution       0.97      0.90      0.93       138\n           Find Anomalies       0.91      0.93      0.92       126\n                  Cluster       0.95      0.94      0.94       125\n                Correlate       0.94      0.92      0.93       147\n\n                micro avg       0.90      0.90      0.90      1404\n                macro avg       0.90      0.90      0.90      1404\n             weighted avg       0.90      0.90      0.90      1404\n\nConfusion Matrix...\n[[131   2   9   2   0   3   1   0   0   0]\n [  7 138   2   2   1   7   0   5   1   0]\n [  3   2 139   3   1   2   0   1   0   2]\n [  1   3   1 148   2   3   1   0   0   1]\n [  1   0   3   0 118   0   1   1   0   0]\n [  2   6   5   4   0 100   0   0   1   2]\n [  1   3   2   0   0   2 124   2   2   2]\n [  0   6   0   0   0   1   0 117   1   1]\n [  0   1   0   0   4   1   0   1 117   1]\n [  0   1   3   3   0   1   1   2   1 135]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.456, Train Acc: 12.50%, Val Loss:  4.498, Val Acc: 13.60%, Time: 7.59s *\nIter: 60, Train Loss:  3.846, Train Acc: 34.38%, Val Loss:  3.966, Val Acc: 33.12%, Time: 13.78s *\nIter: 90, Train Loss:  3.148, Train Acc: 48.44%, Val Loss:  3.291, Val Acc: 46.01%, Time: 19.88s *\nIter: 120, Train Loss:  2.862, Train Acc: 48.44%, Val Loss:  2.728, Val Acc: 58.05%, Time: 25.93s *\nIter: 150, Train Loss:  2.466, Train Acc: 62.50%, Val Loss:  2.337, Val Acc: 62.68%, Time: 32.02s *\nIter: 180, Train Loss:   2.12, Train Acc: 62.50%, Val Loss:  2.198, Val Acc: 65.31%, Time: 38.02s *\nEpoch: 2\nIter: 210, Train Loss:  1.763, Train Acc: 73.44%, Val Loss:  1.801, Val Acc: 71.79%, Time: 3.38s *\nIter: 240, Train Loss:  1.839, Train Acc: 75.00%, Val Loss:  1.725, Val Acc: 73.43%, Time: 9.39s *\nIter: 270, Train Loss:  1.525, Train Acc: 76.56%, Val Loss:  1.518, Val Acc: 76.50%, Time: 15.46s *\nIter: 300, Train Loss:  1.774, Train Acc: 71.88%, Val Loss:  1.493, Val Acc: 76.85%, Time: 21.54s *\nIter: 330, Train Loss:  1.139, Train Acc: 78.12%, Val Loss:  1.396, Val Acc: 78.77%, Time: 27.59s *\nIter: 360, Train Loss:  1.345, Train Acc: 78.12%, Val Loss:  1.359, Val Acc: 77.85%, Time: 33.74s \nIter: 390, Train Loss: 0.8848, Train Acc: 82.81%, Val Loss:   1.29, Val Acc: 79.99%, Time: 39.82s *\nEpoch: 3\nIter: 420, Train Loss:  1.236, Train Acc: 82.81%, Val Loss:  1.203, Val Acc: 80.41%, Time: 5.11s *\nIter: 450, Train Loss: 0.7444, Train Acc: 89.06%, Val Loss:  1.225, Val Acc: 80.41%, Time: 11.17s *\nIter: 480, Train Loss:  0.407, Train Acc: 95.31%, Val Loss:   1.15, Val Acc: 82.55%, Time: 17.18s *\nIter: 510, Train Loss: 0.5278, Train Acc: 90.62%, Val Loss:  1.096, Val Acc: 83.48%, Time: 23.21s *\nIter: 540, Train Loss:  1.001, Train Acc: 79.69%, Val Loss:  1.094, Val Acc: 83.26%, Time: 29.26s \nIter: 570, Train Loss: 0.7562, Train Acc: 87.50%, Val Loss:   1.05, Val Acc: 82.91%, Time: 35.35s \nEpoch: 4\nIter: 600, Train Loss: 0.7528, Train Acc: 89.06%, Val Loss:  1.023, Val Acc: 84.62%, Time: 2.47s *\nIter: 630, Train Loss: 0.6546, Train Acc: 92.19%, Val Loss:  1.002, Val Acc: 85.61%, Time: 8.48s *\nIter: 660, Train Loss: 0.5055, Train Acc: 93.75%, Val Loss:  0.979, Val Acc: 85.61%, Time: 14.59s *\nIter: 690, Train Loss: 0.8726, Train Acc: 87.50%, Val Loss: 0.9297, Val Acc: 85.61%, Time: 20.61s \nIter: 720, Train Loss: 0.3741, Train Acc: 95.31%, Val Loss: 0.9483, Val Acc: 84.62%, Time: 26.61s \nIter: 750, Train Loss: 0.6887, Train Acc: 87.50%, Val Loss:  0.958, Val Acc: 85.61%, Time: 32.64s \nIter: 780, Train Loss: 0.3457, Train Acc: 93.75%, Val Loss: 0.9077, Val Acc: 85.75%, Time: 38.76s *\nEpoch: 5\nIter: 810, Train Loss: 0.4746, Train Acc: 90.62%, Val Loss: 0.8684, Val Acc: 86.82%, Time: 4.26s *\nIter: 840, Train Loss: 0.3037, Train Acc: 93.75%, Val Loss: 0.8794, Val Acc: 85.90%, Time: 10.38s \nIter: 870, Train Loss: 0.3131, Train Acc: 98.44%, Val Loss: 0.8658, Val Acc: 87.89%, Time: 16.36s *\nIter: 900, Train Loss: 0.3373, Train Acc: 93.75%, Val Loss: 0.8125, Val Acc: 88.82%, Time: 22.54s *\nIter: 930, Train Loss: 0.1845, Train Acc: 96.88%, Val Loss: 0.9206, Val Acc: 86.61%, Time: 28.75s \nIter: 960, Train Loss: 0.4134, Train Acc: 93.75%, Val Loss: 0.7902, Val Acc: 87.68%, Time: 34.94s \nIter: 990, Train Loss: 0.7569, Train Acc: 86.96%, Val Loss: 0.7824, Val Acc: 87.68%, Time: 41.04s \nEpoch: 6\nIter: 1020, Train Loss: 0.1718, Train Acc: 96.88%, Val Loss: 0.7528, Val Acc: 88.60%, Time: 6.03s \nIter: 1050, Train Loss: 0.2252, Train Acc: 96.88%, Val Loss: 0.9115, Val Acc: 86.54%, Time: 12.12s \nIter: 1080, Train Loss: 0.3495, Train Acc: 89.06%, Val Loss: 0.8146, Val Acc: 88.46%, Time: 18.21s \nIter: 1110, Train Loss: 0.4354, Train Acc: 92.19%, Val Loss: 0.8853, Val Acc: 88.68%, Time: 24.26s \nIter: 1140, Train Loss: 0.3071, Train Acc: 95.31%, Val Loss: 0.7362, Val Acc: 89.03%, Time: 30.34s *\nIter: 1170, Train Loss: 0.2246, Train Acc: 96.88%, Val Loss: 0.7588, Val Acc: 88.68%, Time: 36.39s \nEpoch: 7\nIter: 1200, Train Loss: 0.2752, Train Acc: 93.75%, Val Loss:  0.759, Val Acc: 89.46%, Time: 3.40s *\nIter: 1230, Train Loss: 0.1141, Train Acc: 100.00%, Val Loss: 0.7915, Val Acc: 90.24%, Time: 9.47s *\nIter: 1260, Train Loss: 0.1538, Train Acc: 98.44%, Val Loss: 0.8198, Val Acc: 88.89%, Time: 15.60s \nIter: 1290, Train Loss: 0.08927, Train Acc: 100.00%, Val Loss: 0.9041, Val Acc: 88.32%, Time: 21.68s \nIter: 1320, Train Loss: 0.4145, Train Acc: 93.75%, Val Loss: 0.7759, Val Acc: 89.25%, Time: 27.72s \nIter: 1350, Train Loss: 0.2548, Train Acc: 98.44%, Val Loss: 0.7389, Val Acc: 88.89%, Time: 33.77s \nIter: 1380, Train Loss: 0.4567, Train Acc: 90.62%, Val Loss: 0.7362, Val Acc: 89.81%, Time: 39.76s \nEpoch: 8\nIter: 1410, Train Loss: 0.06385, Train Acc: 98.44%, Val Loss: 0.8159, Val Acc: 89.81%, Time: 5.22s \nIter: 1440, Train Loss:  0.217, Train Acc: 95.31%, Val Loss: 0.7616, Val Acc: 90.17%, Time: 11.23s \nIter: 1470, Train Loss: 0.1806, Train Acc: 96.88%, Val Loss: 0.7579, Val Acc: 89.10%, Time: 17.26s \nIter: 1500, Train Loss: 0.2631, Train Acc: 96.88%, Val Loss:  0.764, Val Acc: 89.39%, Time: 23.31s \nIter: 1530, Train Loss: 0.08091, Train Acc: 100.00%, Val Loss:   0.83, Val Acc: 89.53%, Time: 29.29s \nIter: 1560, Train Loss: 0.07727, Train Acc: 98.44%, Val Loss: 0.7399, Val Acc: 90.17%, Time: 35.45s \nEpoch: 9\nIter: 1590, Train Loss: 0.0948, Train Acc: 98.44%, Val Loss: 0.8079, Val Acc: 88.82%, Time: 2.44s \nIter: 1620, Train Loss: 0.0912, Train Acc: 98.44%, Val Loss: 0.7635, Val Acc: 90.10%, Time: 8.46s \nIter: 1650, Train Loss: 0.1173, Train Acc: 98.44%, Val Loss: 0.7372, Val Acc: 90.95%, Time: 14.48s *\nIter: 1680, Train Loss: 0.3009, Train Acc: 98.44%, Val Loss: 0.7767, Val Acc: 89.96%, Time: 20.58s \nIter: 1710, Train Loss: 0.0684, Train Acc: 98.44%, Val Loss: 0.8045, Val Acc: 89.67%, Time: 26.61s \nIter: 1740, Train Loss: 0.0671, Train Acc: 100.00%, Val Loss: 0.7133, Val Acc: 90.53%, Time: 32.63s \nIter: 1770, Train Loss: 0.04835, Train Acc: 100.00%, Val Loss:  0.717, Val Acc: 89.67%, Time: 38.66s \nEpoch: 10\nIter: 1800, Train Loss: 0.3737, Train Acc: 96.88%, Val Loss: 0.7829, Val Acc: 90.03%, Time: 4.26s \nIter: 1830, Train Loss: 0.04007, Train Acc: 100.00%, Val Loss: 0.7255, Val Acc: 90.38%, Time: 10.23s \nIter: 1860, Train Loss: 0.1342, Train Acc: 98.44%, Val Loss: 0.7452, Val Acc: 90.38%, Time: 16.19s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1890, Train Loss: 0.2174, Train Acc: 96.88%, Val Loss: 0.8247, Val Acc: 90.17%, Time: 22.26s \nIter: 1920, Train Loss: 0.1377, Train Acc: 96.88%, Val Loss: 0.8505, Val Acc: 88.53%, Time: 28.34s \nIter: 1950, Train Loss: 0.1091, Train Acc: 98.44%, Val Loss: 0.7132, Val Acc: 90.60%, Time: 34.47s \nIter: 1980, Train Loss: 0.0194, Train Acc: 100.00%, Val Loss:   0.82, Val Acc: 89.39%, Time: 40.49s \nEpoch: 11\nIter: 2010, Train Loss: 0.3003, Train Acc: 95.31%, Val Loss:  0.781, Val Acc: 90.17%, Time: 6.04s \nIter: 2040, Train Loss: 0.1353, Train Acc: 98.44%, Val Loss: 0.8405, Val Acc: 90.03%, Time: 12.07s \nIter: 2070, Train Loss: 0.1537, Train Acc: 96.88%, Val Loss: 0.8023, Val Acc: 90.60%, Time: 18.08s \nIter: 2100, Train Loss: 0.06733, Train Acc: 100.00%, Val Loss:  0.894, Val Acc: 89.74%, Time: 24.08s \nIter: 2130, Train Loss: 0.05315, Train Acc: 100.00%, Val Loss: 0.8548, Val Acc: 89.53%, Time: 30.06s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.89      0.89      0.89       130\n                   Filter       0.81      0.93      0.87       135\n    Compute Derived Value       0.84      0.93      0.88       160\n            Find Extremum       0.93      0.97      0.95       173\n                     Sort       0.96      0.88      0.92       119\n          Determine Range       0.88      0.81      0.85       129\nCharacterize Distribution       0.98      0.88      0.93       142\n           Find Anomalies       0.91      0.90      0.90       144\n                  Cluster       0.98      0.94      0.96       127\n                Correlate       0.92      0.90      0.91       145\n\n                micro avg       0.91      0.91      0.91      1404\n                macro avg       0.91      0.90      0.91      1404\n             weighted avg       0.91      0.91      0.91      1404\n\nConfusion Matrix...\n[[116   3   7   1   0   1   0   1   0   1]\n [  0 125   4   1   0   1   0   4   0   0]\n [  4   3 149   2   0   2   0   0   0   0]\n [  1   1   1 168   0   0   0   1   1   0]\n [  1   2   1   5 105   4   1   0   0   0]\n [  7   7   3   1   2 105   1   1   1   1]\n [  1   3   4   1   0   5 125   0   0   3]\n [  1   7   1   0   0   1   0 129   0   5]\n [  0   1   4   0   1   0   0   1 119   1]\n [  0   2   4   1   1   0   1   5   0 131]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.506, Train Acc: 18.75%, Val Loss:  4.496, Val Acc: 13.68%, Time: 7.25s *\nIter: 60, Train Loss:  3.908, Train Acc: 45.31%, Val Loss:  4.006, Val Acc: 36.82%, Time: 13.32s *\nIter: 90, Train Loss:  3.345, Train Acc: 46.88%, Val Loss:  3.398, Val Acc: 44.02%, Time: 19.38s *\nIter: 120, Train Loss:  2.762, Train Acc: 48.44%, Val Loss:  2.768, Val Acc: 55.48%, Time: 25.44s *\nIter: 150, Train Loss:  2.518, Train Acc: 64.06%, Val Loss:  2.555, Val Acc: 58.76%, Time: 31.43s *\nIter: 180, Train Loss:  2.295, Train Acc: 62.50%, Val Loss:  2.208, Val Acc: 65.03%, Time: 37.42s *\nEpoch: 2\nIter: 210, Train Loss:  1.521, Train Acc: 71.88%, Val Loss:   2.02, Val Acc: 68.95%, Time: 3.30s *\nIter: 240, Train Loss:  1.194, Train Acc: 79.69%, Val Loss:  1.899, Val Acc: 71.51%, Time: 9.29s *\nIter: 270, Train Loss:  1.584, Train Acc: 73.44%, Val Loss:  1.723, Val Acc: 73.86%, Time: 15.40s *\nIter: 300, Train Loss: 0.9752, Train Acc: 84.38%, Val Loss:   1.59, Val Acc: 76.85%, Time: 21.40s *\nIter: 330, Train Loss:  1.357, Train Acc: 81.25%, Val Loss:  1.566, Val Acc: 77.49%, Time: 27.38s *\nIter: 360, Train Loss:  1.196, Train Acc: 85.94%, Val Loss:  1.506, Val Acc: 78.70%, Time: 33.46s *\nIter: 390, Train Loss: 0.9748, Train Acc: 87.50%, Val Loss:  1.491, Val Acc: 78.63%, Time: 39.41s \nEpoch: 3\nIter: 420, Train Loss: 0.9885, Train Acc: 81.25%, Val Loss:  1.384, Val Acc: 79.70%, Time: 5.14s *\nIter: 450, Train Loss:  1.058, Train Acc: 89.06%, Val Loss:  1.316, Val Acc: 81.91%, Time: 11.12s *\nIter: 480, Train Loss: 0.8081, Train Acc: 87.50%, Val Loss:  1.294, Val Acc: 81.41%, Time: 17.18s \nIter: 510, Train Loss: 0.9671, Train Acc: 90.62%, Val Loss:  1.233, Val Acc: 82.12%, Time: 23.19s *\nIter: 540, Train Loss: 0.6805, Train Acc: 90.62%, Val Loss:  1.236, Val Acc: 82.12%, Time: 29.18s *\nIter: 570, Train Loss: 0.6587, Train Acc: 92.19%, Val Loss:  1.147, Val Acc: 84.33%, Time: 35.28s *\nEpoch: 4\nIter: 600, Train Loss: 0.7849, Train Acc: 90.62%, Val Loss:  1.164, Val Acc: 83.76%, Time: 2.44s \nIter: 630, Train Loss: 0.8312, Train Acc: 87.50%, Val Loss:  1.157, Val Acc: 83.90%, Time: 8.49s \nIter: 660, Train Loss: 0.6492, Train Acc: 90.62%, Val Loss:  1.086, Val Acc: 84.76%, Time: 14.43s *\nIter: 690, Train Loss: 0.5851, Train Acc: 89.06%, Val Loss:  1.106, Val Acc: 85.33%, Time: 20.44s *\nIter: 720, Train Loss: 0.4808, Train Acc: 93.75%, Val Loss:  1.098, Val Acc: 84.90%, Time: 26.61s \nIter: 750, Train Loss: 0.3374, Train Acc: 96.88%, Val Loss:   1.11, Val Acc: 84.97%, Time: 32.69s \nIter: 780, Train Loss: 0.2646, Train Acc: 96.88%, Val Loss:  1.031, Val Acc: 86.18%, Time: 38.72s *\nEpoch: 5\nIter: 810, Train Loss: 0.2596, Train Acc: 98.44%, Val Loss:  1.134, Val Acc: 86.11%, Time: 4.20s \nIter: 840, Train Loss: 0.1878, Train Acc: 100.00%, Val Loss:  1.018, Val Acc: 85.68%, Time: 10.21s \nIter: 870, Train Loss: 0.4789, Train Acc: 93.75%, Val Loss:  1.093, Val Acc: 85.11%, Time: 16.21s \nIter: 900, Train Loss: 0.2519, Train Acc: 93.75%, Val Loss:  1.016, Val Acc: 85.68%, Time: 22.17s \nIter: 930, Train Loss: 0.5877, Train Acc: 90.62%, Val Loss:    1.0, Val Acc: 86.75%, Time: 28.11s *\nIter: 960, Train Loss: 0.4272, Train Acc: 96.88%, Val Loss:  1.006, Val Acc: 86.32%, Time: 34.22s \nIter: 990, Train Loss: 0.3346, Train Acc: 95.65%, Val Loss:  1.021, Val Acc: 85.68%, Time: 40.23s \nEpoch: 6\nIter: 1020, Train Loss: 0.3697, Train Acc: 95.31%, Val Loss:  1.049, Val Acc: 86.18%, Time: 6.08s \nIter: 1050, Train Loss:  0.237, Train Acc: 96.88%, Val Loss:   1.02, Val Acc: 85.97%, Time: 12.09s \nIter: 1080, Train Loss: 0.6783, Train Acc: 90.62%, Val Loss:  1.004, Val Acc: 85.90%, Time: 18.04s \nIter: 1110, Train Loss: 0.2023, Train Acc: 96.88%, Val Loss: 0.9981, Val Acc: 86.82%, Time: 24.09s *\nIter: 1140, Train Loss:  0.368, Train Acc: 95.31%, Val Loss: 0.9639, Val Acc: 87.18%, Time: 30.09s *\nIter: 1170, Train Loss:  0.127, Train Acc: 100.00%, Val Loss: 0.8916, Val Acc: 87.54%, Time: 36.16s *\nEpoch: 7\nIter: 1200, Train Loss: 0.3441, Train Acc: 96.88%, Val Loss: 0.9522, Val Acc: 87.18%, Time: 3.36s \nIter: 1230, Train Loss: 0.5982, Train Acc: 95.31%, Val Loss: 0.9752, Val Acc: 87.32%, Time: 9.41s \nIter: 1260, Train Loss: 0.2724, Train Acc: 95.31%, Val Loss: 0.9206, Val Acc: 87.96%, Time: 15.42s *\nIter: 1290, Train Loss: 0.07574, Train Acc: 100.00%, Val Loss: 0.9683, Val Acc: 87.82%, Time: 21.49s \nIter: 1320, Train Loss: 0.3092, Train Acc: 90.62%, Val Loss:  1.032, Val Acc: 86.54%, Time: 27.46s \nIter: 1350, Train Loss: 0.1688, Train Acc: 98.44%, Val Loss: 0.9159, Val Acc: 88.18%, Time: 33.43s *\nIter: 1380, Train Loss: 0.2627, Train Acc: 95.31%, Val Loss: 0.9758, Val Acc: 87.04%, Time: 39.49s \nEpoch: 8\nIter: 1410, Train Loss: 0.0232, Train Acc: 100.00%, Val Loss:   1.08, Val Acc: 86.47%, Time: 5.15s \nIter: 1440, Train Loss: 0.09515, Train Acc: 98.44%, Val Loss:  1.054, Val Acc: 86.68%, Time: 11.21s \nIter: 1470, Train Loss: 0.08332, Train Acc: 100.00%, Val Loss:  1.004, Val Acc: 87.61%, Time: 17.31s \nIter: 1500, Train Loss: 0.0806, Train Acc: 98.44%, Val Loss: 0.9717, Val Acc: 87.32%, Time: 23.31s \nIter: 1530, Train Loss: 0.3333, Train Acc: 93.75%, Val Loss: 0.9615, Val Acc: 87.18%, Time: 29.33s \nIter: 1560, Train Loss: 0.1844, Train Acc: 96.88%, Val Loss: 0.9815, Val Acc: 87.68%, Time: 35.39s \nEpoch: 9\nIter: 1590, Train Loss: 0.1168, Train Acc: 100.00%, Val Loss: 0.9972, Val Acc: 87.61%, Time: 2.42s \nIter: 1620, Train Loss: 0.09148, Train Acc: 98.44%, Val Loss: 0.9922, Val Acc: 87.54%, Time: 8.53s \nIter: 1650, Train Loss: 0.1798, Train Acc: 95.31%, Val Loss:  0.977, Val Acc: 87.39%, Time: 14.54s \nIter: 1680, Train Loss: 0.04724, Train Acc: 100.00%, Val Loss:  1.007, Val Acc: 87.46%, Time: 20.48s \nIter: 1710, Train Loss: 0.03602, Train Acc: 100.00%, Val Loss:  1.028, Val Acc: 87.46%, Time: 26.52s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1740, Train Loss: 0.07308, Train Acc: 100.00%, Val Loss: 0.9676, Val Acc: 87.54%, Time: 32.59s \nIter: 1770, Train Loss: 0.1315, Train Acc: 96.88%, Val Loss:  1.025, Val Acc: 86.32%, Time: 38.58s \nEpoch: 10\nIter: 1800, Train Loss: 0.06518, Train Acc: 100.00%, Val Loss: 0.9404, Val Acc: 88.39%, Time: 4.19s *\nIter: 1830, Train Loss: 0.05791, Train Acc: 100.00%, Val Loss: 0.9107, Val Acc: 88.82%, Time: 10.22s *\nIter: 1860, Train Loss: 0.02594, Train Acc: 100.00%, Val Loss:  1.148, Val Acc: 87.32%, Time: 16.15s \nIter: 1890, Train Loss: 0.05869, Train Acc: 100.00%, Val Loss: 0.9907, Val Acc: 87.68%, Time: 22.12s \nIter: 1920, Train Loss: 0.1431, Train Acc: 95.31%, Val Loss:  1.009, Val Acc: 87.89%, Time: 28.18s \nIter: 1950, Train Loss: 0.07602, Train Acc: 100.00%, Val Loss:  1.014, Val Acc: 87.25%, Time: 34.16s \nIter: 1980, Train Loss: 0.02152, Train Acc: 100.00%, Val Loss:  1.033, Val Acc: 86.82%, Time: 40.22s \nEpoch: 11\nIter: 2010, Train Loss: 0.03747, Train Acc: 100.00%, Val Loss:  1.043, Val Acc: 89.03%, Time: 6.07s *\nIter: 2040, Train Loss: 0.2306, Train Acc: 96.88%, Val Loss: 0.9494, Val Acc: 87.61%, Time: 12.09s \nIter: 2070, Train Loss: 0.04977, Train Acc: 100.00%, Val Loss:  0.961, Val Acc: 87.54%, Time: 18.16s \nIter: 2100, Train Loss: 0.0438, Train Acc: 98.44%, Val Loss:  1.056, Val Acc: 88.11%, Time: 24.12s \nIter: 2130, Train Loss: 0.05092, Train Acc: 98.44%, Val Loss:  1.113, Val Acc: 87.46%, Time: 30.08s \nIter: 2160, Train Loss: 0.1307, Train Acc: 96.88%, Val Loss:  1.082, Val Acc: 87.54%, Time: 36.09s \nEpoch: 12\nIter: 2190, Train Loss: 0.01594, Train Acc: 100.00%, Val Loss:    1.2, Val Acc: 86.82%, Time: 3.30s \nIter: 2220, Train Loss: 0.02404, Train Acc: 100.00%, Val Loss:  1.129, Val Acc: 87.04%, Time: 9.30s \nIter: 2250, Train Loss: 0.05643, Train Acc: 98.44%, Val Loss:  1.116, Val Acc: 86.89%, Time: 15.31s \nIter: 2280, Train Loss: 0.0678, Train Acc: 96.88%, Val Loss:  1.108, Val Acc: 86.97%, Time: 21.35s \nIter: 2310, Train Loss: 0.01945, Train Acc: 100.00%, Val Loss:  1.089, Val Acc: 87.82%, Time: 27.35s \nIter: 2340, Train Loss: 0.07398, Train Acc: 98.44%, Val Loss:  1.032, Val Acc: 88.25%, Time: 33.34s \nIter: 2370, Train Loss: 0.2938, Train Acc: 98.44%, Val Loss:  1.102, Val Acc: 87.96%, Time: 39.29s \nEpoch: 13\nIter: 2400, Train Loss: 0.02646, Train Acc: 100.00%, Val Loss:  1.089, Val Acc: 87.61%, Time: 5.14s \nIter: 2430, Train Loss: 0.03003, Train Acc: 100.00%, Val Loss:  1.076, Val Acc: 88.53%, Time: 11.22s \nIter: 2460, Train Loss: 0.02257, Train Acc: 100.00%, Val Loss:  1.141, Val Acc: 87.75%, Time: 17.18s \nIter: 2490, Train Loss: 0.06295, Train Acc: 98.44%, Val Loss:  1.153, Val Acc: 88.18%, Time: 23.23s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.87      0.92      0.89       148\n                   Filter       0.87      0.88      0.87       130\n    Compute Derived Value       0.86      0.87      0.87       152\n            Find Extremum       0.97      0.89      0.92       167\n                     Sort       0.86      0.95      0.90       120\n          Determine Range       0.82      0.84      0.83       140\nCharacterize Distribution       0.88      0.86      0.87       117\n           Find Anomalies       0.85      0.90      0.87       146\n                  Cluster       0.95      0.93      0.94       120\n                Correlate       0.93      0.85      0.89       164\n\n                micro avg       0.89      0.89      0.89      1404\n                macro avg       0.89      0.89      0.89      1404\n             weighted avg       0.89      0.89      0.89      1404\n\nConfusion Matrix...\n[[136   1   4   0   3   3   0   0   0   1]\n [  5 114   1   1   1   4   0   4   0   0]\n [  6   1 132   1   0   7   1   3   0   1]\n [  2   2   0 148   7   3   3   2   0   0]\n [  0   1   0   1 114   1   0   2   1   0]\n [  4   3   7   1   1 117   4   2   1   0]\n [  1   0   1   0   2   5 101   3   1   3]\n [  2   5   2   0   0   1   2 131   1   2]\n [  0   0   0   1   3   0   0   2 111   3]\n [  1   4   6   0   1   1   4   6   2 139]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:   4.51, Train Acc: 14.06%, Val Loss:  4.516, Val Acc: 14.46%, Time: 7.24s *\nIter: 60, Train Loss:   4.06, Train Acc: 29.69%, Val Loss:  3.971, Val Acc: 29.27%, Time: 13.19s *\nIter: 90, Train Loss:  3.314, Train Acc: 48.44%, Val Loss:  3.409, Val Acc: 45.80%, Time: 19.18s *\nIter: 120, Train Loss:  2.714, Train Acc: 59.38%, Val Loss:  2.756, Val Acc: 55.34%, Time: 25.06s *\nIter: 150, Train Loss:  2.301, Train Acc: 64.06%, Val Loss:  2.477, Val Acc: 60.47%, Time: 31.23s *\nIter: 180, Train Loss:  1.459, Train Acc: 81.25%, Val Loss:  2.045, Val Acc: 68.52%, Time: 37.28s *\nEpoch: 2\nIter: 210, Train Loss:  2.307, Train Acc: 64.06%, Val Loss:  1.891, Val Acc: 70.51%, Time: 3.26s *\nIter: 240, Train Loss:   1.51, Train Acc: 78.12%, Val Loss:  1.731, Val Acc: 72.58%, Time: 9.27s *\nIter: 270, Train Loss:  1.336, Train Acc: 76.56%, Val Loss:  1.564, Val Acc: 74.43%, Time: 15.19s *\nIter: 300, Train Loss:  1.085, Train Acc: 81.25%, Val Loss:  1.432, Val Acc: 77.64%, Time: 21.22s *\nIter: 330, Train Loss:  1.325, Train Acc: 82.81%, Val Loss:  1.352, Val Acc: 78.42%, Time: 27.11s *\nIter: 360, Train Loss:  1.412, Train Acc: 79.69%, Val Loss:  1.267, Val Acc: 79.91%, Time: 33.06s *\nIter: 390, Train Loss:  1.051, Train Acc: 87.50%, Val Loss:   1.29, Val Acc: 79.42%, Time: 39.05s \nEpoch: 3\nIter: 420, Train Loss: 0.6417, Train Acc: 89.06%, Val Loss:  1.127, Val Acc: 82.26%, Time: 5.11s *\nIter: 450, Train Loss: 0.8807, Train Acc: 90.62%, Val Loss:  1.126, Val Acc: 81.91%, Time: 11.16s \nIter: 480, Train Loss:  1.038, Train Acc: 85.94%, Val Loss:  1.037, Val Acc: 83.97%, Time: 17.12s *\nIter: 510, Train Loss:  0.822, Train Acc: 89.06%, Val Loss:   1.05, Val Acc: 83.19%, Time: 23.08s \nIter: 540, Train Loss: 0.6397, Train Acc: 89.06%, Val Loss:  1.031, Val Acc: 84.05%, Time: 29.17s *\nIter: 570, Train Loss: 0.8293, Train Acc: 85.94%, Val Loss: 0.9553, Val Acc: 85.40%, Time: 35.15s *\nEpoch: 4\nIter: 600, Train Loss: 0.6315, Train Acc: 89.06%, Val Loss: 0.9545, Val Acc: 84.97%, Time: 2.41s \nIter: 630, Train Loss: 0.7533, Train Acc: 89.06%, Val Loss: 0.8785, Val Acc: 87.32%, Time: 8.38s *\nIter: 660, Train Loss: 0.5383, Train Acc: 93.75%, Val Loss: 0.9055, Val Acc: 85.33%, Time: 14.40s \nIter: 690, Train Loss: 0.4162, Train Acc: 95.31%, Val Loss: 0.9161, Val Acc: 84.97%, Time: 20.41s \nIter: 720, Train Loss: 0.4329, Train Acc: 96.88%, Val Loss: 0.8403, Val Acc: 85.75%, Time: 26.45s \nIter: 750, Train Loss: 0.9862, Train Acc: 84.38%, Val Loss: 0.8976, Val Acc: 86.40%, Time: 32.46s \nIter: 780, Train Loss: 0.6394, Train Acc: 89.06%, Val Loss: 0.9461, Val Acc: 85.19%, Time: 38.35s \nEpoch: 5\nIter: 810, Train Loss: 0.2872, Train Acc: 96.88%, Val Loss: 0.8595, Val Acc: 85.90%, Time: 4.19s \nIter: 840, Train Loss: 0.4925, Train Acc: 90.62%, Val Loss: 0.8169, Val Acc: 86.54%, Time: 10.19s \nIter: 870, Train Loss: 0.3212, Train Acc: 95.31%, Val Loss:  0.824, Val Acc: 87.25%, Time: 16.18s \nIter: 900, Train Loss: 0.1805, Train Acc: 100.00%, Val Loss: 0.7744, Val Acc: 87.11%, Time: 22.07s \nIter: 930, Train Loss: 0.4308, Train Acc: 93.75%, Val Loss: 0.7474, Val Acc: 87.46%, Time: 28.06s *\nIter: 960, Train Loss: 0.4689, Train Acc: 93.75%, Val Loss: 0.7599, Val Acc: 87.54%, Time: 33.96s *\nIter: 990, Train Loss: 0.09975, Train Acc: 100.00%, Val Loss: 0.7919, Val Acc: 87.46%, Time: 39.91s \nEpoch: 6\nIter: 1020, Train Loss:  0.152, Train Acc: 98.44%, Val Loss: 0.7507, Val Acc: 88.68%, Time: 5.99s *\nIter: 1050, Train Loss: 0.2455, Train Acc: 95.31%, Val Loss: 0.7337, Val Acc: 88.46%, Time: 12.02s \nIter: 1080, Train Loss: 0.2917, Train Acc: 96.88%, Val Loss: 0.7165, Val Acc: 88.32%, Time: 17.96s \nIter: 1110, Train Loss: 0.3693, Train Acc: 95.31%, Val Loss: 0.7841, Val Acc: 87.82%, Time: 23.92s \nIter: 1140, Train Loss: 0.2117, Train Acc: 96.88%, Val Loss: 0.8005, Val Acc: 87.04%, Time: 29.89s \nIter: 1170, Train Loss: 0.4362, Train Acc: 95.31%, Val Loss: 0.7138, Val Acc: 88.82%, Time: 35.97s *\nEpoch: 7\nIter: 1200, Train Loss: 0.4728, Train Acc: 95.31%, Val Loss: 0.6601, Val Acc: 89.39%, Time: 3.26s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1230, Train Loss: 0.4143, Train Acc: 96.88%, Val Loss: 0.8044, Val Acc: 88.32%, Time: 9.16s \nIter: 1260, Train Loss:  0.287, Train Acc: 96.88%, Val Loss: 0.7719, Val Acc: 88.89%, Time: 15.08s \nIter: 1290, Train Loss: 0.1326, Train Acc: 96.88%, Val Loss: 0.8047, Val Acc: 88.32%, Time: 21.00s \nIter: 1320, Train Loss: 0.4163, Train Acc: 92.19%, Val Loss: 0.7834, Val Acc: 89.10%, Time: 26.95s \nIter: 1350, Train Loss: 0.1521, Train Acc: 100.00%, Val Loss: 0.6779, Val Acc: 88.96%, Time: 32.92s \nIter: 1380, Train Loss: 0.1103, Train Acc: 100.00%, Val Loss: 0.7005, Val Acc: 89.96%, Time: 38.85s *\nEpoch: 8\nIter: 1410, Train Loss: 0.1918, Train Acc: 95.31%, Val Loss:  0.702, Val Acc: 89.60%, Time: 5.09s \nIter: 1440, Train Loss: 0.0441, Train Acc: 100.00%, Val Loss: 0.8634, Val Acc: 88.32%, Time: 11.01s \nIter: 1470, Train Loss: 0.07614, Train Acc: 100.00%, Val Loss: 0.8142, Val Acc: 88.82%, Time: 17.00s \nIter: 1500, Train Loss: 0.4633, Train Acc: 93.75%, Val Loss: 0.6875, Val Acc: 89.60%, Time: 23.06s \nIter: 1530, Train Loss: 0.3821, Train Acc: 93.75%, Val Loss: 0.8447, Val Acc: 88.68%, Time: 29.00s \nIter: 1560, Train Loss: 0.1126, Train Acc: 100.00%, Val Loss: 0.7251, Val Acc: 89.39%, Time: 34.98s \nEpoch: 9\nIter: 1590, Train Loss: 0.1204, Train Acc: 96.88%, Val Loss: 0.6892, Val Acc: 89.46%, Time: 2.35s \nIter: 1620, Train Loss: 0.1727, Train Acc: 98.44%, Val Loss:  0.821, Val Acc: 87.96%, Time: 8.32s \nIter: 1650, Train Loss: 0.1656, Train Acc: 96.88%, Val Loss: 0.7377, Val Acc: 89.25%, Time: 14.30s \nIter: 1680, Train Loss: 0.3649, Train Acc: 95.31%, Val Loss: 0.8757, Val Acc: 87.96%, Time: 20.28s \nIter: 1710, Train Loss: 0.2907, Train Acc: 95.31%, Val Loss: 0.8003, Val Acc: 88.18%, Time: 26.21s \nIter: 1740, Train Loss: 0.04731, Train Acc: 100.00%, Val Loss: 0.7463, Val Acc: 88.89%, Time: 32.17s \nIter: 1770, Train Loss: 0.1564, Train Acc: 96.88%, Val Loss:   0.74, Val Acc: 88.60%, Time: 38.25s \nEpoch: 10\nIter: 1800, Train Loss: 0.1654, Train Acc: 96.88%, Val Loss: 0.7548, Val Acc: 89.39%, Time: 4.20s \nIter: 1830, Train Loss: 0.1317, Train Acc: 98.44%, Val Loss: 0.6798, Val Acc: 89.53%, Time: 10.15s \nIter: 1860, Train Loss: 0.1024, Train Acc: 100.00%, Val Loss: 0.7395, Val Acc: 89.32%, Time: 16.14s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.72      0.97      0.83       120\n                   Filter       0.88      0.68      0.77       155\n    Compute Derived Value       0.79      0.85      0.82       150\n            Find Extremum       0.95      0.93      0.94       174\n                     Sort       0.96      0.87      0.91       123\n          Determine Range       0.85      0.85      0.85       117\nCharacterize Distribution       0.97      0.90      0.93       163\n           Find Anomalies       0.92      0.85      0.88       136\n                  Cluster       0.83      0.98      0.90       125\n                Correlate       0.91      0.91      0.91       141\n\n                micro avg       0.88      0.88      0.88      1404\n                macro avg       0.88      0.88      0.87      1404\n             weighted avg       0.88      0.88      0.88      1404\n\nConfusion Matrix...\n[[116   0   4   0   0   0   0   0   0   0]\n [ 15 106  13   1   0  10   0   3   4   3]\n [ 14   2 128   1   1   0   0   0   2   2]\n [  4   1   3 161   3   0   1   0   0   1]\n [  1   0   0   3 107   1   0   0  11   0]\n [  4   3   5   2   1  99   1   0   2   0]\n [  4   1   4   0   0   3 146   2   2   1]\n [  3   7   2   1   0   3   0 115   1   4]\n [  0   0   0   0   0   0   0   1 123   1]\n [  0   0   3   0   0   0   2   4   3 129]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.435, Train Acc: 17.19%, Val Loss:   4.47, Val Acc: 14.40%, Time: 7.30s *\nIter: 60, Train Loss:  3.902, Train Acc: 32.81%, Val Loss:  3.956, Val Acc: 34.43%, Time: 13.32s *\nIter: 90, Train Loss:  3.181, Train Acc: 50.00%, Val Loss:  3.377, Val Acc: 43.05%, Time: 19.31s *\nIter: 120, Train Loss:  2.684, Train Acc: 60.94%, Val Loss:  2.833, Val Acc: 56.24%, Time: 25.33s *\nIter: 150, Train Loss:    2.7, Train Acc: 62.50%, Val Loss:  2.374, Val Acc: 62.94%, Time: 31.37s *\nIter: 180, Train Loss:  1.991, Train Acc: 75.00%, Val Loss:  2.072, Val Acc: 69.14%, Time: 37.38s *\nEpoch: 2\nIter: 210, Train Loss:  1.368, Train Acc: 81.25%, Val Loss:  1.945, Val Acc: 67.71%, Time: 3.27s \nIter: 240, Train Loss:  1.465, Train Acc: 76.56%, Val Loss:  1.726, Val Acc: 72.84%, Time: 9.20s *\nIter: 270, Train Loss:  1.224, Train Acc: 84.38%, Val Loss:   1.47, Val Acc: 76.12%, Time: 15.21s *\nIter: 300, Train Loss:  1.347, Train Acc: 78.12%, Val Loss:  1.416, Val Acc: 77.19%, Time: 21.09s *\nIter: 330, Train Loss:  1.725, Train Acc: 71.88%, Val Loss:  1.331, Val Acc: 78.26%, Time: 27.03s *\nIter: 360, Train Loss: 0.9394, Train Acc: 84.38%, Val Loss:  1.249, Val Acc: 80.04%, Time: 32.99s *\nIter: 390, Train Loss: 0.9071, Train Acc: 87.50%, Val Loss:  1.211, Val Acc: 80.90%, Time: 38.92s *\nEpoch: 3\nIter: 420, Train Loss: 0.9932, Train Acc: 84.38%, Val Loss:  1.171, Val Acc: 81.97%, Time: 5.11s *\nIter: 450, Train Loss:  1.241, Train Acc: 82.81%, Val Loss:  1.128, Val Acc: 82.97%, Time: 11.01s *\nIter: 480, Train Loss: 0.6489, Train Acc: 89.06%, Val Loss:  1.112, Val Acc: 83.11%, Time: 17.11s *\nIter: 510, Train Loss:   1.24, Train Acc: 78.12%, Val Loss:  1.023, Val Acc: 83.04%, Time: 23.11s \nIter: 540, Train Loss: 0.9584, Train Acc: 92.19%, Val Loss: 0.9792, Val Acc: 84.96%, Time: 29.04s *\nIter: 570, Train Loss: 0.4224, Train Acc: 93.75%, Val Loss: 0.9961, Val Acc: 84.68%, Time: 34.95s \nEpoch: 4\nIter: 600, Train Loss: 0.6153, Train Acc: 89.06%, Val Loss: 0.9435, Val Acc: 85.03%, Time: 2.36s *\nIter: 630, Train Loss: 0.7984, Train Acc: 87.50%, Val Loss: 0.9113, Val Acc: 86.32%, Time: 8.34s *\nIter: 660, Train Loss: 0.5457, Train Acc: 92.19%, Val Loss: 0.9953, Val Acc: 84.39%, Time: 14.32s \nIter: 690, Train Loss:    0.5, Train Acc: 92.19%, Val Loss: 0.9093, Val Acc: 85.46%, Time: 20.29s \nIter: 720, Train Loss: 0.4604, Train Acc: 93.75%, Val Loss: 0.8815, Val Acc: 86.46%, Time: 26.32s *\nIter: 750, Train Loss: 0.5294, Train Acc: 93.75%, Val Loss: 0.8031, Val Acc: 87.46%, Time: 32.20s *\nIter: 780, Train Loss:  1.039, Train Acc: 89.06%, Val Loss: 0.8454, Val Acc: 85.67%, Time: 38.17s \nEpoch: 5\nIter: 810, Train Loss: 0.8244, Train Acc: 84.38%, Val Loss: 0.8603, Val Acc: 86.32%, Time: 4.22s \nIter: 840, Train Loss: 0.4632, Train Acc: 92.19%, Val Loss: 0.8384, Val Acc: 86.67%, Time: 10.17s \nIter: 870, Train Loss: 0.6595, Train Acc: 87.50%, Val Loss: 0.7517, Val Acc: 88.24%, Time: 16.15s *\nIter: 900, Train Loss:  0.282, Train Acc: 96.88%, Val Loss: 0.7628, Val Acc: 88.45%, Time: 22.12s *\nIter: 930, Train Loss: 0.3255, Train Acc: 95.31%, Val Loss: 0.7128, Val Acc: 88.88%, Time: 28.09s *\nIter: 960, Train Loss:  0.501, Train Acc: 92.19%, Val Loss: 0.7325, Val Acc: 88.74%, Time: 34.16s \nIter: 990, Train Loss: 0.4971, Train Acc: 91.67%, Val Loss: 0.7311, Val Acc: 89.02%, Time: 40.08s *\nEpoch: 6\nIter: 1020, Train Loss: 0.2567, Train Acc: 96.88%, Val Loss: 0.7279, Val Acc: 89.45%, Time: 5.97s *\nIter: 1050, Train Loss: 0.1733, Train Acc: 98.44%, Val Loss: 0.8454, Val Acc: 87.60%, Time: 11.97s \nIter: 1080, Train Loss: 0.3008, Train Acc: 96.88%, Val Loss: 0.7032, Val Acc: 89.52%, Time: 18.01s *\nIter: 1110, Train Loss: 0.3215, Train Acc: 93.75%, Val Loss: 0.7342, Val Acc: 88.52%, Time: 24.04s \nIter: 1140, Train Loss: 0.7159, Train Acc: 90.62%, Val Loss: 0.8052, Val Acc: 87.60%, Time: 30.04s \nIter: 1170, Train Loss: 0.1469, Train Acc: 98.44%, Val Loss: 0.8318, Val Acc: 88.24%, Time: 35.98s \nEpoch: 7\nIter: 1200, Train Loss:  0.477, Train Acc: 95.31%, Val Loss: 0.7979, Val Acc: 88.45%, Time: 3.22s \nIter: 1230, Train Loss: 0.1536, Train Acc: 98.44%, Val Loss: 0.6999, Val Acc: 89.02%, Time: 9.31s \nIter: 1260, Train Loss: 0.08663, Train Acc: 100.00%, Val Loss: 0.7536, Val Acc: 89.88%, Time: 15.40s *\nIter: 1290, Train Loss: 0.4067, Train Acc: 96.88%, Val Loss:  0.678, Val Acc: 90.45%, Time: 21.32s *\nIter: 1320, Train Loss: 0.1148, Train Acc: 96.88%, Val Loss: 0.6268, Val Acc: 90.24%, Time: 27.25s \nIter: 1350, Train Loss:  0.119, Train Acc: 98.44%, Val Loss: 0.6816, Val Acc: 90.09%, Time: 33.21s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1380, Train Loss: 0.1662, Train Acc: 98.44%, Val Loss: 0.7165, Val Acc: 89.31%, Time: 39.14s \nEpoch: 8\nIter: 1410, Train Loss: 0.1798, Train Acc: 98.44%, Val Loss: 0.6538, Val Acc: 90.38%, Time: 5.09s \nIter: 1440, Train Loss: 0.2197, Train Acc: 96.88%, Val Loss: 0.6457, Val Acc: 90.45%, Time: 11.04s *\nIter: 1470, Train Loss: 0.1659, Train Acc: 98.44%, Val Loss: 0.7132, Val Acc: 89.09%, Time: 17.00s \nIter: 1500, Train Loss: 0.1963, Train Acc: 98.44%, Val Loss: 0.7191, Val Acc: 89.74%, Time: 22.95s \nIter: 1530, Train Loss: 0.1737, Train Acc: 98.44%, Val Loss: 0.7371, Val Acc: 90.02%, Time: 28.93s \nIter: 1560, Train Loss: 0.1362, Train Acc: 98.44%, Val Loss: 0.7367, Val Acc: 89.38%, Time: 34.91s \nEpoch: 9\nIter: 1590, Train Loss: 0.2415, Train Acc: 96.88%, Val Loss: 0.6481, Val Acc: 90.95%, Time: 2.35s *\nIter: 1620, Train Loss: 0.2477, Train Acc: 96.88%, Val Loss: 0.6728, Val Acc: 90.02%, Time: 8.28s \nIter: 1650, Train Loss: 0.05162, Train Acc: 100.00%, Val Loss:  0.629, Val Acc: 91.59%, Time: 14.20s *\nIter: 1680, Train Loss: 0.2134, Train Acc: 96.88%, Val Loss: 0.6975, Val Acc: 89.52%, Time: 20.22s \nIter: 1710, Train Loss: 0.07851, Train Acc: 100.00%, Val Loss: 0.6246, Val Acc: 91.38%, Time: 26.31s \nIter: 1740, Train Loss: 0.5007, Train Acc: 92.19%, Val Loss: 0.6775, Val Acc: 90.24%, Time: 32.37s \nIter: 1770, Train Loss:   0.12, Train Acc: 100.00%, Val Loss: 0.7858, Val Acc: 88.45%, Time: 38.32s \nEpoch: 10\nIter: 1800, Train Loss: 0.02975, Train Acc: 100.00%, Val Loss: 0.7333, Val Acc: 90.31%, Time: 4.14s \nIter: 1830, Train Loss: 0.0656, Train Acc: 100.00%, Val Loss: 0.6863, Val Acc: 90.95%, Time: 10.06s \nIter: 1860, Train Loss: 0.06104, Train Acc: 100.00%, Val Loss: 0.6073, Val Acc: 91.23%, Time: 16.08s \nIter: 1890, Train Loss: 0.07333, Train Acc: 100.00%, Val Loss: 0.6164, Val Acc: 91.16%, Time: 21.99s \nIter: 1920, Train Loss: 0.1998, Train Acc: 98.44%, Val Loss: 0.6426, Val Acc: 90.52%, Time: 27.94s \nIter: 1950, Train Loss: 0.05246, Train Acc: 100.00%, Val Loss: 0.7349, Val Acc: 90.31%, Time: 33.89s \nIter: 1980, Train Loss: 0.08859, Train Acc: 100.00%, Val Loss: 0.6536, Val Acc: 90.95%, Time: 39.87s \nEpoch: 11\nIter: 2010, Train Loss: 0.06635, Train Acc: 98.44%, Val Loss:  0.685, Val Acc: 91.38%, Time: 5.96s \nIter: 2040, Train Loss: 0.2239, Train Acc: 95.31%, Val Loss: 0.7166, Val Acc: 90.16%, Time: 11.92s \nIter: 2070, Train Loss: 0.04902, Train Acc: 100.00%, Val Loss: 0.6842, Val Acc: 90.24%, Time: 17.86s \nIter: 2100, Train Loss: 0.3828, Train Acc: 96.88%, Val Loss: 0.7101, Val Acc: 90.38%, Time: 23.77s \nIter: 2130, Train Loss: 0.01418, Train Acc: 100.00%, Val Loss: 0.6463, Val Acc: 91.52%, Time: 29.75s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.78      0.97      0.86       133\n                   Filter       0.93      0.82      0.88       120\n    Compute Derived Value       0.88      0.88      0.88       152\n            Find Extremum       0.93      0.95      0.94       162\n                     Sort       0.96      0.95      0.96       138\n          Determine Range       0.91      0.87      0.89       138\nCharacterize Distribution       0.96      0.90      0.93       126\n           Find Anomalies       0.96      0.93      0.94       161\n                  Cluster       0.90      0.95      0.92       116\n                Correlate       0.94      0.90      0.92       157\n\n                micro avg       0.91      0.91      0.91      1403\n                macro avg       0.91      0.91      0.91      1403\n             weighted avg       0.92      0.91      0.91      1403\n\nConfusion Matrix...\n[[129   0   2   0   0   1   0   0   1   0]\n [  9  99   3   1   0   2   0   3   2   1]\n [ 12   1 133   2   0   1   0   2   0   1]\n [  2   1   2 154   0   2   0   1   0   0]\n [  0   0   0   3 131   2   0   0   2   0]\n [  5   2   1   4   3 120   0   0   2   1]\n [  1   0   4   0   1   4 113   0   1   2]\n [  1   3   1   1   0   0   0 149   2   4]\n [  3   0   0   0   1   0   1   1 110   0]\n [  4   0   5   1   0   0   4   0   2 141]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.632, Train Acc:  9.38%, Val Loss:   4.54, Val Acc: 13.40%, Time: 7.26s *\nIter: 60, Train Loss:  4.127, Train Acc: 40.62%, Val Loss:  4.176, Val Acc: 31.22%, Time: 13.36s *\nIter: 90, Train Loss:  3.289, Train Acc: 51.56%, Val Loss:   3.39, Val Acc: 46.69%, Time: 19.45s *\nIter: 120, Train Loss:  2.676, Train Acc: 54.69%, Val Loss:   2.94, Val Acc: 53.24%, Time: 25.41s *\nIter: 150, Train Loss:  2.287, Train Acc: 62.50%, Val Loss:  2.379, Val Acc: 61.23%, Time: 31.47s *\nIter: 180, Train Loss:  2.132, Train Acc: 71.88%, Val Loss:  2.247, Val Acc: 64.43%, Time: 37.44s *\nEpoch: 2\nIter: 210, Train Loss:  1.828, Train Acc: 76.56%, Val Loss:  1.852, Val Acc: 71.70%, Time: 3.25s *\nIter: 240, Train Loss:   1.58, Train Acc: 78.12%, Val Loss:  1.779, Val Acc: 71.85%, Time: 9.20s *\nIter: 270, Train Loss:  1.317, Train Acc: 81.25%, Val Loss:  1.567, Val Acc: 74.70%, Time: 15.19s *\nIter: 300, Train Loss:  1.533, Train Acc: 73.44%, Val Loss:  1.694, Val Acc: 72.77%, Time: 21.21s \nIter: 330, Train Loss:  1.261, Train Acc: 76.56%, Val Loss:  1.426, Val Acc: 78.12%, Time: 27.13s *\nIter: 360, Train Loss:  1.335, Train Acc: 75.00%, Val Loss:  1.346, Val Acc: 78.33%, Time: 33.05s *\nIter: 390, Train Loss: 0.9935, Train Acc: 79.69%, Val Loss:   1.35, Val Acc: 78.76%, Time: 38.93s *\nEpoch: 3\nIter: 420, Train Loss:  1.044, Train Acc: 82.81%, Val Loss:  1.229, Val Acc: 80.68%, Time: 4.99s *\nIter: 450, Train Loss: 0.7579, Train Acc: 87.50%, Val Loss:  1.199, Val Acc: 81.11%, Time: 10.90s *\nIter: 480, Train Loss: 0.8607, Train Acc: 84.38%, Val Loss:  1.195, Val Acc: 81.75%, Time: 16.86s *\nIter: 510, Train Loss:  1.588, Train Acc: 76.56%, Val Loss:   1.21, Val Acc: 81.11%, Time: 22.79s \nIter: 540, Train Loss: 0.5367, Train Acc: 93.75%, Val Loss:  1.145, Val Acc: 82.39%, Time: 28.69s *\nIter: 570, Train Loss: 0.7853, Train Acc: 87.50%, Val Loss:  1.093, Val Acc: 84.25%, Time: 34.64s *\nEpoch: 4\nIter: 600, Train Loss: 0.2478, Train Acc: 98.44%, Val Loss:  1.107, Val Acc: 83.32%, Time: 2.43s \nIter: 630, Train Loss: 0.6516, Train Acc: 89.06%, Val Loss:  1.039, Val Acc: 83.68%, Time: 8.38s \nIter: 660, Train Loss: 0.8541, Train Acc: 87.50%, Val Loss: 0.9327, Val Acc: 86.10%, Time: 14.21s *\nIter: 690, Train Loss: 0.1303, Train Acc: 100.00%, Val Loss:  1.066, Val Acc: 84.03%, Time: 20.14s \nIter: 720, Train Loss: 0.4741, Train Acc: 95.31%, Val Loss:   1.05, Val Acc: 85.60%, Time: 26.12s \nIter: 750, Train Loss: 0.7099, Train Acc: 93.75%, Val Loss:  1.044, Val Acc: 84.46%, Time: 32.07s \nIter: 780, Train Loss:  0.528, Train Acc: 90.62%, Val Loss: 0.9871, Val Acc: 85.74%, Time: 38.22s \nEpoch: 5\nIter: 810, Train Loss: 0.3963, Train Acc: 95.31%, Val Loss: 0.9756, Val Acc: 86.24%, Time: 4.19s *\nIter: 840, Train Loss:  0.307, Train Acc: 96.88%, Val Loss:   0.97, Val Acc: 86.39%, Time: 10.11s *\nIter: 870, Train Loss: 0.3115, Train Acc: 96.88%, Val Loss: 0.9062, Val Acc: 87.60%, Time: 16.06s *\nIter: 900, Train Loss: 0.2481, Train Acc: 93.75%, Val Loss:   0.94, Val Acc: 87.24%, Time: 22.09s \nIter: 930, Train Loss: 0.6703, Train Acc: 90.62%, Val Loss: 0.8141, Val Acc: 88.38%, Time: 28.10s *\nIter: 960, Train Loss: 0.4885, Train Acc: 92.19%, Val Loss: 0.8775, Val Acc: 87.03%, Time: 34.11s \nIter: 990, Train Loss: 0.6255, Train Acc: 91.67%, Val Loss:  0.948, Val Acc: 86.81%, Time: 40.07s \nEpoch: 6\nIter: 1020, Train Loss: 0.1489, Train Acc: 98.44%, Val Loss: 0.9221, Val Acc: 87.67%, Time: 6.04s \nIter: 1050, Train Loss: 0.0832, Train Acc: 100.00%, Val Loss:    1.0, Val Acc: 86.67%, Time: 12.08s \nIter: 1080, Train Loss: 0.3645, Train Acc: 93.75%, Val Loss: 0.9605, Val Acc: 87.81%, Time: 18.30s \nIter: 1110, Train Loss: 0.2841, Train Acc: 96.88%, Val Loss: 0.9925, Val Acc: 85.67%, Time: 24.40s \nIter: 1140, Train Loss:  0.158, Train Acc: 98.44%, Val Loss: 0.8831, Val Acc: 88.38%, Time: 30.38s \nIter: 1170, Train Loss: 0.2527, Train Acc: 96.88%, Val Loss: 0.8668, Val Acc: 88.52%, Time: 36.36s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2069, Train Acc: 95.31%, Val Loss: 0.9728, Val Acc: 87.17%, Time: 3.30s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1230, Train Loss:  0.382, Train Acc: 96.88%, Val Loss: 0.8846, Val Acc: 88.95%, Time: 9.20s *\nIter: 1260, Train Loss: 0.1928, Train Acc: 98.44%, Val Loss: 0.8644, Val Acc: 88.45%, Time: 15.18s \nIter: 1290, Train Loss: 0.3984, Train Acc: 96.88%, Val Loss: 0.9287, Val Acc: 88.10%, Time: 21.18s \nIter: 1320, Train Loss: 0.4103, Train Acc: 93.75%, Val Loss: 0.9065, Val Acc: 89.17%, Time: 27.15s *\nIter: 1350, Train Loss: 0.04992, Train Acc: 100.00%, Val Loss: 0.9703, Val Acc: 87.95%, Time: 33.15s \nIter: 1380, Train Loss: 0.2489, Train Acc: 98.44%, Val Loss: 0.9754, Val Acc: 86.89%, Time: 39.18s \nEpoch: 8\nIter: 1410, Train Loss: 0.08565, Train Acc: 100.00%, Val Loss: 0.9107, Val Acc: 88.88%, Time: 5.05s \nIter: 1440, Train Loss: 0.05256, Train Acc: 100.00%, Val Loss: 0.9768, Val Acc: 87.53%, Time: 11.00s \nIter: 1470, Train Loss: 0.1347, Train Acc: 98.44%, Val Loss: 0.8717, Val Acc: 89.38%, Time: 17.00s *\nIter: 1500, Train Loss: 0.1127, Train Acc: 98.44%, Val Loss: 0.9162, Val Acc: 88.74%, Time: 23.02s \nIter: 1530, Train Loss: 0.1267, Train Acc: 98.44%, Val Loss: 0.8842, Val Acc: 89.17%, Time: 29.04s \nIter: 1560, Train Loss: 0.2275, Train Acc: 96.88%, Val Loss: 0.9122, Val Acc: 88.17%, Time: 34.96s \nEpoch: 9\nIter: 1590, Train Loss: 0.2684, Train Acc: 96.88%, Val Loss: 0.9086, Val Acc: 88.95%, Time: 2.35s \nIter: 1620, Train Loss: 0.1608, Train Acc: 98.44%, Val Loss: 0.9407, Val Acc: 88.45%, Time: 8.28s \nIter: 1650, Train Loss: 0.1803, Train Acc: 98.44%, Val Loss: 0.9242, Val Acc: 88.81%, Time: 14.24s \nIter: 1680, Train Loss: 0.08241, Train Acc: 98.44%, Val Loss: 0.9195, Val Acc: 89.24%, Time: 20.27s \nIter: 1710, Train Loss: 0.3277, Train Acc: 96.88%, Val Loss: 0.9486, Val Acc: 88.88%, Time: 26.24s \nIter: 1740, Train Loss: 0.09547, Train Acc: 100.00%, Val Loss: 0.9689, Val Acc: 88.03%, Time: 32.20s \nIter: 1770, Train Loss: 0.0837, Train Acc: 98.44%, Val Loss: 0.9579, Val Acc: 88.67%, Time: 38.23s \nEpoch: 10\nIter: 1800, Train Loss: 0.03374, Train Acc: 100.00%, Val Loss: 0.9235, Val Acc: 89.52%, Time: 4.27s *\nIter: 1830, Train Loss: 0.03034, Train Acc: 100.00%, Val Loss: 0.8443, Val Acc: 90.02%, Time: 10.24s *\nIter: 1860, Train Loss: 0.2318, Train Acc: 98.44%, Val Loss:   1.01, Val Acc: 88.88%, Time: 16.22s \nIter: 1890, Train Loss: 0.05477, Train Acc: 100.00%, Val Loss:  1.053, Val Acc: 87.17%, Time: 22.25s \nIter: 1920, Train Loss: 0.3033, Train Acc: 96.88%, Val Loss:  1.017, Val Acc: 88.03%, Time: 28.17s \nIter: 1950, Train Loss: 0.1241, Train Acc: 98.44%, Val Loss: 0.9041, Val Acc: 89.45%, Time: 34.12s \nIter: 1980, Train Loss: 0.02295, Train Acc: 100.00%, Val Loss:  1.012, Val Acc: 88.03%, Time: 40.12s \nEpoch: 11\nIter: 2010, Train Loss: 0.02477, Train Acc: 100.00%, Val Loss: 0.9549, Val Acc: 89.88%, Time: 5.95s \nIter: 2040, Train Loss: 0.0303, Train Acc: 100.00%, Val Loss:  1.079, Val Acc: 89.09%, Time: 11.84s \nIter: 2070, Train Loss: 0.03511, Train Acc: 100.00%, Val Loss:  1.054, Val Acc: 88.60%, Time: 17.90s \nIter: 2100, Train Loss: 0.03524, Train Acc: 100.00%, Val Loss:  0.944, Val Acc: 89.38%, Time: 24.06s \nIter: 2130, Train Loss: 0.06451, Train Acc: 100.00%, Val Loss:  1.065, Val Acc: 88.38%, Time: 30.07s \nIter: 2160, Train Loss: 0.09234, Train Acc: 98.44%, Val Loss:  1.005, Val Acc: 88.95%, Time: 36.02s \nEpoch: 12\nIter: 2190, Train Loss:  0.043, Train Acc: 100.00%, Val Loss:  1.017, Val Acc: 88.67%, Time: 3.24s \nIter: 2220, Train Loss: 0.04677, Train Acc: 100.00%, Val Loss:  1.018, Val Acc: 88.45%, Time: 9.24s \nIter: 2250, Train Loss: 0.02559, Train Acc: 100.00%, Val Loss: 0.9767, Val Acc: 89.17%, Time: 15.27s \nIter: 2280, Train Loss: 0.03917, Train Acc: 100.00%, Val Loss: 0.9469, Val Acc: 89.59%, Time: 21.32s \nIter: 2310, Train Loss: 0.07437, Train Acc: 98.44%, Val Loss: 0.9383, Val Acc: 89.67%, Time: 27.37s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.81      0.92      0.86       114\n                   Filter       0.89      0.84      0.86       140\n    Compute Derived Value       0.85      0.88      0.86       144\n            Find Extremum       0.95      0.93      0.94       168\n                     Sort       0.96      0.92      0.94       103\n          Determine Range       0.86      0.87      0.87       124\nCharacterize Distribution       0.92      0.84      0.88       153\n           Find Anomalies       0.90      0.89      0.89       138\n                  Cluster       0.92      0.95      0.93       161\n                Correlate       0.92      0.92      0.92       158\n\n                micro avg       0.90      0.90      0.90      1403\n                macro avg       0.90      0.90      0.90      1403\n             weighted avg       0.90      0.90      0.90      1403\n\nConfusion Matrix...\n[[105   1   5   1   0   1   0   0   0   1]\n [ 10 117   3   1   0   0   0   7   0   2]\n [  3   1 127   1   1   5   1   1   2   2]\n [  2   2   2 157   0   1   2   1   1   0]\n [  0   0   1   1  95   0   0   2   3   1]\n [  2   3   2   3   1 108   1   1   2   1]\n [  2   2   8   1   1   7 129   0   1   2]\n [  5   5   0   1   0   1   2 123   0   1]\n [  0   0   0   0   1   1   3   0 153   3]\n [  1   0   2   0   0   1   2   2   5 145]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.566, Train Acc:  7.81%, Val Loss:  4.516, Val Acc: 11.26%, Time: 7.16s *\nIter: 60, Train Loss:   3.93, Train Acc: 42.19%, Val Loss:  4.098, Val Acc: 34.85%, Time: 13.17s *\nIter: 90, Train Loss:  3.056, Train Acc: 54.69%, Val Loss:  3.317, Val Acc: 46.61%, Time: 19.14s *\nIter: 120, Train Loss:  2.469, Train Acc: 59.38%, Val Loss:  2.714, Val Acc: 58.30%, Time: 25.16s *\nIter: 150, Train Loss:   2.13, Train Acc: 64.06%, Val Loss:  2.422, Val Acc: 62.79%, Time: 31.10s *\nIter: 180, Train Loss:   1.92, Train Acc: 75.00%, Val Loss:  2.114, Val Acc: 68.21%, Time: 37.02s *\nEpoch: 2\nIter: 210, Train Loss:  2.441, Train Acc: 59.38%, Val Loss:   1.88, Val Acc: 70.92%, Time: 3.26s *\nIter: 240, Train Loss:  2.032, Train Acc: 62.50%, Val Loss:  1.704, Val Acc: 73.98%, Time: 9.25s *\nIter: 270, Train Loss:  1.459, Train Acc: 79.69%, Val Loss:   1.58, Val Acc: 74.91%, Time: 15.35s *\nIter: 300, Train Loss:  1.348, Train Acc: 81.25%, Val Loss:   1.53, Val Acc: 76.91%, Time: 21.28s *\nIter: 330, Train Loss:  1.851, Train Acc: 76.56%, Val Loss:  1.391, Val Acc: 78.47%, Time: 27.21s *\nIter: 360, Train Loss:  1.341, Train Acc: 79.69%, Val Loss:  1.304, Val Acc: 80.33%, Time: 33.22s *\nIter: 390, Train Loss: 0.7902, Train Acc: 87.50%, Val Loss:  1.334, Val Acc: 79.69%, Time: 39.23s \nEpoch: 3\nIter: 420, Train Loss: 0.8751, Train Acc: 89.06%, Val Loss:  1.161, Val Acc: 82.32%, Time: 5.10s *\nIter: 450, Train Loss:  0.873, Train Acc: 90.62%, Val Loss:  1.205, Val Acc: 82.04%, Time: 11.06s \nIter: 480, Train Loss: 0.6025, Train Acc: 90.62%, Val Loss:  1.169, Val Acc: 82.82%, Time: 16.98s *\nIter: 510, Train Loss: 0.9951, Train Acc: 84.38%, Val Loss:   1.08, Val Acc: 83.75%, Time: 22.97s *\nIter: 540, Train Loss: 0.9785, Train Acc: 85.94%, Val Loss:  1.048, Val Acc: 84.46%, Time: 28.94s *\nIter: 570, Train Loss: 0.6014, Train Acc: 93.75%, Val Loss:  1.119, Val Acc: 83.25%, Time: 34.93s \nEpoch: 4\nIter: 600, Train Loss: 0.5629, Train Acc: 90.62%, Val Loss: 0.9871, Val Acc: 85.17%, Time: 2.38s *\nIter: 630, Train Loss: 0.5897, Train Acc: 93.75%, Val Loss: 0.9698, Val Acc: 85.53%, Time: 8.38s *\nIter: 660, Train Loss: 0.4859, Train Acc: 95.31%, Val Loss:  0.964, Val Acc: 86.53%, Time: 14.44s *\nIter: 690, Train Loss: 0.3114, Train Acc: 96.88%, Val Loss: 0.9256, Val Acc: 86.96%, Time: 20.36s *\nIter: 720, Train Loss: 0.6482, Train Acc: 90.62%, Val Loss: 0.9236, Val Acc: 86.46%, Time: 26.38s \nIter: 750, Train Loss: 0.6371, Train Acc: 84.38%, Val Loss: 0.9141, Val Acc: 87.17%, Time: 32.45s *\nIter: 780, Train Loss:  0.416, Train Acc: 95.31%, Val Loss: 0.8415, Val Acc: 87.53%, Time: 38.42s *\nEpoch: 5\nIter: 810, Train Loss: 0.6161, Train Acc: 90.62%, Val Loss:  0.931, Val Acc: 87.03%, Time: 4.12s \nIter: 840, Train Loss: 0.6786, Train Acc: 92.19%, Val Loss:  0.888, Val Acc: 87.10%, Time: 10.11s \nIter: 870, Train Loss:  0.466, Train Acc: 95.31%, Val Loss: 0.8709, Val Acc: 87.38%, Time: 16.15s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 900, Train Loss: 0.4502, Train Acc: 95.31%, Val Loss:  0.861, Val Acc: 87.46%, Time: 22.10s \nIter: 930, Train Loss: 0.3762, Train Acc: 95.31%, Val Loss: 0.8408, Val Acc: 88.17%, Time: 28.06s *\nIter: 960, Train Loss: 0.2721, Train Acc: 95.31%, Val Loss: 0.8233, Val Acc: 87.81%, Time: 34.03s \nIter: 990, Train Loss: 0.4155, Train Acc: 91.67%, Val Loss: 0.9033, Val Acc: 86.32%, Time: 39.97s \nEpoch: 6\nIter: 1020, Train Loss: 0.3542, Train Acc: 93.75%, Val Loss: 0.8996, Val Acc: 86.96%, Time: 5.96s \nIter: 1050, Train Loss: 0.1925, Train Acc: 98.44%, Val Loss: 0.8522, Val Acc: 88.03%, Time: 11.88s \nIter: 1080, Train Loss: 0.09637, Train Acc: 100.00%, Val Loss: 0.7976, Val Acc: 88.45%, Time: 17.85s *\nIter: 1110, Train Loss: 0.6106, Train Acc: 95.31%, Val Loss: 0.7593, Val Acc: 89.17%, Time: 23.83s *\nIter: 1140, Train Loss: 0.1712, Train Acc: 100.00%, Val Loss: 0.7978, Val Acc: 89.45%, Time: 29.83s *\nIter: 1170, Train Loss: 0.2174, Train Acc: 98.44%, Val Loss: 0.7753, Val Acc: 89.52%, Time: 35.79s *\nEpoch: 7\nIter: 1200, Train Loss: 0.1013, Train Acc: 98.44%, Val Loss: 0.7936, Val Acc: 89.67%, Time: 3.29s *\nIter: 1230, Train Loss: 0.2616, Train Acc: 95.31%, Val Loss: 0.8243, Val Acc: 89.02%, Time: 9.31s \nIter: 1260, Train Loss: 0.03627, Train Acc: 100.00%, Val Loss: 0.8139, Val Acc: 89.45%, Time: 15.38s \nIter: 1290, Train Loss: 0.1851, Train Acc: 98.44%, Val Loss: 0.7688, Val Acc: 89.81%, Time: 21.30s *\nIter: 1320, Train Loss: 0.4301, Train Acc: 95.31%, Val Loss: 0.7436, Val Acc: 89.95%, Time: 27.25s *\nIter: 1350, Train Loss: 0.1594, Train Acc: 96.88%, Val Loss:  0.853, Val Acc: 88.17%, Time: 33.17s \nIter: 1380, Train Loss: 0.1403, Train Acc: 98.44%, Val Loss: 0.7092, Val Acc: 90.16%, Time: 39.18s *\nEpoch: 8\nIter: 1410, Train Loss: 0.04812, Train Acc: 100.00%, Val Loss: 0.8253, Val Acc: 89.24%, Time: 5.08s \nIter: 1440, Train Loss: 0.1853, Train Acc: 96.88%, Val Loss: 0.8168, Val Acc: 89.88%, Time: 11.02s \nIter: 1470, Train Loss: 0.3971, Train Acc: 93.75%, Val Loss:  0.741, Val Acc: 90.16%, Time: 17.02s \nIter: 1500, Train Loss: 0.1402, Train Acc: 98.44%, Val Loss: 0.7663, Val Acc: 89.59%, Time: 22.99s \nIter: 1530, Train Loss: 0.2558, Train Acc: 95.31%, Val Loss: 0.8024, Val Acc: 89.52%, Time: 28.94s \nIter: 1560, Train Loss: 0.2037, Train Acc: 96.88%, Val Loss: 0.8724, Val Acc: 88.45%, Time: 35.00s \nEpoch: 9\nIter: 1590, Train Loss: 0.1779, Train Acc: 98.44%, Val Loss: 0.8841, Val Acc: 89.17%, Time: 2.38s \nIter: 1620, Train Loss: 0.0419, Train Acc: 100.00%, Val Loss: 0.8359, Val Acc: 89.17%, Time: 8.35s \nIter: 1650, Train Loss: 0.2147, Train Acc: 98.44%, Val Loss: 0.7835, Val Acc: 89.74%, Time: 14.25s \nIter: 1680, Train Loss: 0.1535, Train Acc: 98.44%, Val Loss: 0.8017, Val Acc: 90.52%, Time: 20.18s *\nIter: 1710, Train Loss: 0.0756, Train Acc: 98.44%, Val Loss: 0.8257, Val Acc: 89.38%, Time: 26.34s \nIter: 1740, Train Loss: 0.2686, Train Acc: 96.88%, Val Loss: 0.8115, Val Acc: 89.95%, Time: 32.34s \nIter: 1770, Train Loss: 0.1571, Train Acc: 96.88%, Val Loss: 0.8202, Val Acc: 89.52%, Time: 38.25s \nEpoch: 10\nIter: 1800, Train Loss: 0.1867, Train Acc: 98.44%, Val Loss: 0.8336, Val Acc: 89.17%, Time: 4.19s \nIter: 1830, Train Loss: 0.03931, Train Acc: 100.00%, Val Loss: 0.8585, Val Acc: 89.67%, Time: 10.16s \nIter: 1860, Train Loss: 0.0864, Train Acc: 98.44%, Val Loss: 0.7895, Val Acc: 90.52%, Time: 16.15s \nIter: 1890, Train Loss: 0.2611, Train Acc: 98.44%, Val Loss: 0.8753, Val Acc: 89.38%, Time: 22.12s \nIter: 1920, Train Loss: 0.04897, Train Acc: 100.00%, Val Loss: 0.8169, Val Acc: 89.95%, Time: 27.99s \nIter: 1950, Train Loss: 0.1571, Train Acc: 98.44%, Val Loss:  0.879, Val Acc: 89.52%, Time: 33.92s \nIter: 1980, Train Loss: 0.04038, Train Acc: 100.00%, Val Loss: 0.8604, Val Acc: 89.81%, Time: 39.79s \nEpoch: 11\nIter: 2010, Train Loss: 0.04246, Train Acc: 100.00%, Val Loss: 0.9101, Val Acc: 90.38%, Time: 5.98s \nIter: 2040, Train Loss: 0.02613, Train Acc: 100.00%, Val Loss:  1.046, Val Acc: 89.38%, Time: 11.95s \nIter: 2070, Train Loss: 0.1291, Train Acc: 100.00%, Val Loss: 0.8583, Val Acc: 89.17%, Time: 17.90s \nIter: 2100, Train Loss: 0.06864, Train Acc: 98.44%, Val Loss: 0.7603, Val Acc: 89.81%, Time: 23.91s \nIter: 2130, Train Loss: 0.01483, Train Acc: 100.00%, Val Loss: 0.7933, Val Acc: 89.95%, Time: 29.91s \nIter: 2160, Train Loss: 0.03631, Train Acc: 100.00%, Val Loss:  0.865, Val Acc: 90.45%, Time: 35.95s \nEpoch: 12\nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.92      0.95      0.94       146\n                   Filter       0.73      0.96      0.83       138\n    Compute Derived Value       0.93      0.81      0.86       158\n            Find Extremum       0.93      0.88      0.91       161\n                     Sort       0.94      0.97      0.96       117\n          Determine Range       0.94      0.75      0.84       146\nCharacterize Distribution       0.93      0.89      0.91       128\n           Find Anomalies       0.86      0.87      0.87       139\n                  Cluster       0.93      0.91      0.92       125\n                Correlate       0.85      0.92      0.89       145\n\n                micro avg       0.89      0.89      0.89      1403\n                macro avg       0.90      0.89      0.89      1403\n             weighted avg       0.90      0.89      0.89      1403\n\nConfusion Matrix...\n[[139   3   1   1   1   0   1   0   0   0]\n [  2 132   0   0   0   0   1   3   0   0]\n [  1   9 128   4   1   1   3   3   2   6]\n [  2   6   2 142   0   2   1   3   0   3]\n [  0   1   0   0 114   1   0   0   1   0]\n [  5  14   3   3   2 110   1   4   1   3]\n [  1   1   1   0   1   3 114   1   1   5]\n [  0   9   1   1   0   0   0 121   2   5]\n [  0   3   0   0   2   0   2   3 114   1]\n [  1   3   2   1   0   0   0   2   2 134]]\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.373, Train Acc: 26.56%, Val Loss:   4.42, Val Acc: 19.03%, Time: 7.13s *\nIter: 60, Train Loss:  3.791, Train Acc: 39.06%, Val Loss:  3.836, Val Acc: 36.35%, Time: 13.11s *\nIter: 90, Train Loss:  3.227, Train Acc: 37.50%, Val Loss:  3.139, Val Acc: 50.25%, Time: 19.08s *\nIter: 120, Train Loss:  2.262, Train Acc: 65.62%, Val Loss:  2.549, Val Acc: 61.01%, Time: 25.05s *\nIter: 150, Train Loss:  2.422, Train Acc: 60.94%, Val Loss:  2.292, Val Acc: 64.15%, Time: 31.08s *\nIter: 180, Train Loss:   1.65, Train Acc: 76.56%, Val Loss:  1.931, Val Acc: 70.42%, Time: 37.01s *\nEpoch: 2\nIter: 210, Train Loss:  1.302, Train Acc: 85.94%, Val Loss:  1.709, Val Acc: 74.13%, Time: 3.27s *\nIter: 240, Train Loss:  1.767, Train Acc: 68.75%, Val Loss:  1.654, Val Acc: 75.62%, Time: 9.22s *\nIter: 270, Train Loss:  1.371, Train Acc: 73.44%, Val Loss:  1.562, Val Acc: 75.55%, Time: 15.19s \nIter: 300, Train Loss:  1.435, Train Acc: 75.00%, Val Loss:  1.438, Val Acc: 77.48%, Time: 21.14s *\nIter: 330, Train Loss:  1.116, Train Acc: 84.38%, Val Loss:  1.325, Val Acc: 79.69%, Time: 27.05s *\nIter: 360, Train Loss:  1.469, Train Acc: 76.56%, Val Loss:  1.291, Val Acc: 80.61%, Time: 33.03s *\nIter: 390, Train Loss:  1.375, Train Acc: 79.69%, Val Loss:  1.322, Val Acc: 79.97%, Time: 39.02s \nEpoch: 3\nIter: 420, Train Loss: 0.8884, Train Acc: 87.50%, Val Loss:  1.209, Val Acc: 81.54%, Time: 5.00s *\nIter: 450, Train Loss: 0.9747, Train Acc: 87.50%, Val Loss:   1.17, Val Acc: 82.04%, Time: 10.99s *\nIter: 480, Train Loss: 0.9409, Train Acc: 87.50%, Val Loss:  1.207, Val Acc: 80.97%, Time: 17.02s \nIter: 510, Train Loss: 0.5027, Train Acc: 89.06%, Val Loss:  1.017, Val Acc: 85.39%, Time: 22.98s *\nIter: 540, Train Loss:  1.194, Train Acc: 81.25%, Val Loss:  1.092, Val Acc: 83.68%, Time: 28.96s \nIter: 570, Train Loss: 0.7556, Train Acc: 90.62%, Val Loss:  1.025, Val Acc: 85.46%, Time: 34.92s *\nEpoch: 4\nIter: 600, Train Loss:  1.175, Train Acc: 82.81%, Val Loss: 0.9604, Val Acc: 84.39%, Time: 2.37s \nIter: 630, Train Loss: 0.5109, Train Acc: 89.06%, Val Loss: 0.9134, Val Acc: 86.67%, Time: 8.32s *\nIter: 660, Train Loss: 0.6143, Train Acc: 89.06%, Val Loss: 0.8684, Val Acc: 86.74%, Time: 14.30s *\nIter: 690, Train Loss: 0.5833, Train Acc: 89.06%, Val Loss: 0.8962, Val Acc: 86.24%, Time: 20.26s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 720, Train Loss:  0.833, Train Acc: 84.38%, Val Loss: 0.8346, Val Acc: 86.81%, Time: 26.23s *\nIter: 750, Train Loss: 0.3465, Train Acc: 95.31%, Val Loss: 0.8515, Val Acc: 87.38%, Time: 32.14s *\nIter: 780, Train Loss: 0.2371, Train Acc: 98.44%, Val Loss: 0.8218, Val Acc: 87.81%, Time: 38.13s *\nEpoch: 5\nIter: 810, Train Loss: 0.5729, Train Acc: 93.75%, Val Loss: 0.8731, Val Acc: 86.32%, Time: 4.16s \nIter: 840, Train Loss: 0.2017, Train Acc: 96.88%, Val Loss: 0.7987, Val Acc: 87.81%, Time: 10.01s \nIter: 870, Train Loss: 0.6085, Train Acc: 96.88%, Val Loss: 0.7932, Val Acc: 88.38%, Time: 15.92s *\nIter: 900, Train Loss: 0.2812, Train Acc: 93.75%, Val Loss:  0.764, Val Acc: 88.74%, Time: 21.93s *\nIter: 930, Train Loss: 0.2893, Train Acc: 96.88%, Val Loss: 0.8319, Val Acc: 88.10%, Time: 27.90s \nIter: 960, Train Loss: 0.3849, Train Acc: 95.31%, Val Loss: 0.8143, Val Acc: 87.24%, Time: 33.85s \nIter: 990, Train Loss: 0.2255, Train Acc: 95.83%, Val Loss: 0.7962, Val Acc: 87.81%, Time: 39.77s \nEpoch: 6\nIter: 1020, Train Loss: 0.5319, Train Acc: 93.75%, Val Loss: 0.8034, Val Acc: 88.67%, Time: 5.97s \nIter: 1050, Train Loss: 0.3347, Train Acc: 95.31%, Val Loss:  0.806, Val Acc: 88.10%, Time: 11.94s \nIter: 1080, Train Loss: 0.2988, Train Acc: 95.31%, Val Loss: 0.8171, Val Acc: 88.74%, Time: 17.91s *\nIter: 1110, Train Loss: 0.3296, Train Acc: 95.31%, Val Loss:  0.795, Val Acc: 88.24%, Time: 23.84s \nIter: 1140, Train Loss: 0.3518, Train Acc: 95.31%, Val Loss: 0.7731, Val Acc: 88.88%, Time: 29.84s *\nIter: 1170, Train Loss: 0.2804, Train Acc: 96.88%, Val Loss: 0.7009, Val Acc: 89.81%, Time: 35.81s *\nEpoch: 7\nIter: 1200, Train Loss: 0.2922, Train Acc: 96.88%, Val Loss: 0.7201, Val Acc: 89.45%, Time: 3.26s \nIter: 1230, Train Loss: 0.1195, Train Acc: 100.00%, Val Loss: 0.7463, Val Acc: 89.52%, Time: 9.25s \nIter: 1260, Train Loss: 0.1201, Train Acc: 98.44%, Val Loss: 0.7818, Val Acc: 88.52%, Time: 15.25s \nIter: 1290, Train Loss: 0.0481, Train Acc: 100.00%, Val Loss: 0.6966, Val Acc: 90.38%, Time: 21.24s *\nIter: 1320, Train Loss: 0.1281, Train Acc: 96.88%, Val Loss: 0.7706, Val Acc: 89.45%, Time: 27.26s \nIter: 1350, Train Loss: 0.2934, Train Acc: 95.31%, Val Loss: 0.8502, Val Acc: 87.88%, Time: 33.27s \nIter: 1380, Train Loss: 0.1361, Train Acc: 96.88%, Val Loss: 0.7393, Val Acc: 89.67%, Time: 39.22s \nEpoch: 8\nIter: 1410, Train Loss: 0.08494, Train Acc: 100.00%, Val Loss: 0.8033, Val Acc: 88.95%, Time: 5.04s \nIter: 1440, Train Loss: 0.2616, Train Acc: 98.44%, Val Loss: 0.8524, Val Acc: 88.52%, Time: 11.09s \nIter: 1470, Train Loss: 0.3845, Train Acc: 93.75%, Val Loss: 0.8058, Val Acc: 89.02%, Time: 17.12s \nIter: 1500, Train Loss:  0.243, Train Acc: 93.75%, Val Loss: 0.7445, Val Acc: 89.02%, Time: 23.07s \nIter: 1530, Train Loss:  0.105, Train Acc: 98.44%, Val Loss: 0.7555, Val Acc: 90.02%, Time: 29.03s \nIter: 1560, Train Loss: 0.09042, Train Acc: 98.44%, Val Loss: 0.7673, Val Acc: 89.95%, Time: 34.95s \nEpoch: 9\nIter: 1590, Train Loss:  0.235, Train Acc: 95.31%, Val Loss: 0.7552, Val Acc: 89.02%, Time: 2.34s \nIter: 1620, Train Loss: 0.06327, Train Acc: 98.44%, Val Loss: 0.7923, Val Acc: 89.81%, Time: 8.30s \nIter: 1650, Train Loss: 0.3193, Train Acc: 95.31%, Val Loss: 0.8393, Val Acc: 89.59%, Time: 14.28s \nIter: 1680, Train Loss: 0.1015, Train Acc: 98.44%, Val Loss:  0.817, Val Acc: 89.02%, Time: 20.20s \nIter: 1710, Train Loss: 0.2482, Train Acc: 95.31%, Val Loss:  0.774, Val Acc: 90.38%, Time: 26.15s *\nIter: 1740, Train Loss: 0.08838, Train Acc: 98.44%, Val Loss: 0.7597, Val Acc: 90.09%, Time: 32.13s \nIter: 1770, Train Loss: 0.3195, Train Acc: 95.31%, Val Loss: 0.7875, Val Acc: 90.31%, Time: 38.13s \nEpoch: 10\nIter: 1800, Train Loss: 0.1288, Train Acc: 98.44%, Val Loss: 0.9506, Val Acc: 88.88%, Time: 4.16s \nIter: 1830, Train Loss: 0.09851, Train Acc: 98.44%, Val Loss: 0.8837, Val Acc: 88.52%, Time: 10.04s \nIter: 1860, Train Loss: 0.1481, Train Acc: 96.88%, Val Loss: 0.8456, Val Acc: 89.45%, Time: 15.97s \nIter: 1890, Train Loss: 0.06769, Train Acc: 100.00%, Val Loss: 0.8151, Val Acc: 89.38%, Time: 21.93s \nIter: 1920, Train Loss: 0.1296, Train Acc: 98.44%, Val Loss: 0.8022, Val Acc: 89.31%, Time: 27.88s \nIter: 1950, Train Loss: 0.04003, Train Acc: 100.00%, Val Loss: 0.8558, Val Acc: 89.67%, Time: 33.89s \nIter: 1980, Train Loss: 0.02247, Train Acc: 100.00%, Val Loss: 0.8799, Val Acc: 89.45%, Time: 39.83s \nEpoch: 11\nIter: 2010, Train Loss:  0.068, Train Acc: 98.44%, Val Loss: 0.8684, Val Acc: 89.88%, Time: 5.98s \nIter: 2040, Train Loss: 0.1584, Train Acc: 96.88%, Val Loss: 0.9018, Val Acc: 89.38%, Time: 11.94s \nIter: 2070, Train Loss: 0.02126, Train Acc: 100.00%, Val Loss: 0.8103, Val Acc: 90.16%, Time: 17.93s \nIter: 2100, Train Loss: 0.06307, Train Acc: 98.44%, Val Loss: 0.8815, Val Acc: 88.45%, Time: 23.99s \nIter: 2130, Train Loss: 0.05594, Train Acc: 100.00%, Val Loss: 0.8532, Val Acc: 89.17%, Time: 29.93s \nIter: 2160, Train Loss: 0.09167, Train Acc: 98.44%, Val Loss: 0.8047, Val Acc: 89.59%, Time: 35.83s \nEpoch: 12\nIter: 2190, Train Loss: 0.03877, Train Acc: 98.44%, Val Loss: 0.8623, Val Acc: 89.59%, Time: 3.29s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.83      0.94      0.88       149\n                   Filter       0.88      0.75      0.81       154\n    Compute Derived Value       0.86      0.86      0.86       155\n            Find Extremum       0.91      0.87      0.89       158\n                     Sort       0.97      0.88      0.92       128\n          Determine Range       0.74      0.92      0.82       137\nCharacterize Distribution       0.91      0.91      0.91       117\n           Find Anomalies       0.91      0.87      0.89       125\n                  Cluster       0.85      0.96      0.90       120\n                Correlate       0.96      0.83      0.89       160\n\n                micro avg       0.88      0.88      0.88      1403\n                macro avg       0.88      0.88      0.88      1403\n             weighted avg       0.88      0.88      0.88      1403\n\nConfusion Matrix...\n[[140   1   2   0   0   4   1   0   1   0]\n [  6 116   2   4   0  17   2   3   2   2]\n [  7   1 134   2   1   7   2   1   0   0]\n [  2   2   4 137   0   9   1   1   1   1]\n [  3   3   2   2 112   2   0   0   4   0]\n [  4   0   1   4   1 126   0   0   1   0]\n [  1   0   3   0   1   2 106   2   2   0]\n [  2   5   2   1   0   2   0 109   2   2]\n [  0   0   2   1   0   1   1   0 115   0]\n [  3   4   4   0   0   0   4   4   8 133]]\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.326, Train Acc: 20.31%, Val Loss:  4.416, Val Acc: 20.03%, Time: 7.11s *\nIter: 60, Train Loss:  3.782, Train Acc: 39.06%, Val Loss:  3.762, Val Acc: 39.27%, Time: 13.02s *\nIter: 90, Train Loss:  3.115, Train Acc: 48.44%, Val Loss:  2.946, Val Acc: 52.82%, Time: 18.99s *\nIter: 120, Train Loss:  2.622, Train Acc: 64.06%, Val Loss:  2.563, Val Acc: 59.52%, Time: 24.98s *\nIter: 150, Train Loss:  2.209, Train Acc: 62.50%, Val Loss:  2.121, Val Acc: 67.50%, Time: 30.86s *\nIter: 180, Train Loss:  1.876, Train Acc: 73.44%, Val Loss:  1.926, Val Acc: 70.21%, Time: 36.97s *\nEpoch: 2\nIter: 210, Train Loss:  1.759, Train Acc: 73.44%, Val Loss:  1.724, Val Acc: 73.20%, Time: 3.30s *\nIter: 240, Train Loss:  1.431, Train Acc: 78.12%, Val Loss:  1.601, Val Acc: 74.27%, Time: 9.22s *\nIter: 270, Train Loss:  1.278, Train Acc: 85.94%, Val Loss:  1.439, Val Acc: 77.83%, Time: 15.23s *\nIter: 300, Train Loss:  1.551, Train Acc: 78.12%, Val Loss:    1.3, Val Acc: 80.40%, Time: 21.22s *\nIter: 330, Train Loss: 0.9417, Train Acc: 85.94%, Val Loss:   1.28, Val Acc: 78.83%, Time: 27.16s \nIter: 360, Train Loss: 0.8697, Train Acc: 85.94%, Val Loss:  1.202, Val Acc: 80.26%, Time: 33.10s \nIter: 390, Train Loss: 0.8916, Train Acc: 85.94%, Val Loss:   1.12, Val Acc: 82.39%, Time: 39.01s *\nEpoch: 3\nIter: 420, Train Loss:  1.008, Train Acc: 84.38%, Val Loss:  1.091, Val Acc: 82.89%, Time: 5.13s *\nIter: 450, Train Loss: 0.8717, Train Acc: 87.50%, Val Loss:  1.044, Val Acc: 84.60%, Time: 11.07s *\nIter: 480, Train Loss: 0.6852, Train Acc: 89.06%, Val Loss: 0.9661, Val Acc: 85.32%, Time: 17.08s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 510, Train Loss: 0.9797, Train Acc: 84.38%, Val Loss:  0.978, Val Acc: 84.03%, Time: 23.02s \nIter: 540, Train Loss:  1.061, Train Acc: 87.50%, Val Loss:  1.022, Val Acc: 84.25%, Time: 29.01s \nIter: 570, Train Loss:   0.59, Train Acc: 95.31%, Val Loss: 0.9228, Val Acc: 86.81%, Time: 35.02s *\nEpoch: 4\nIter: 600, Train Loss: 0.7836, Train Acc: 87.50%, Val Loss: 0.8598, Val Acc: 87.31%, Time: 2.37s *\nIter: 630, Train Loss: 0.4582, Train Acc: 93.75%, Val Loss: 0.8552, Val Acc: 87.17%, Time: 8.40s \nIter: 660, Train Loss: 0.6575, Train Acc: 92.19%, Val Loss: 0.9249, Val Acc: 85.89%, Time: 14.38s \nIter: 690, Train Loss: 0.7339, Train Acc: 92.19%, Val Loss: 0.8173, Val Acc: 88.03%, Time: 20.39s *\nIter: 720, Train Loss: 0.4471, Train Acc: 93.75%, Val Loss: 0.8838, Val Acc: 86.32%, Time: 26.34s \nIter: 750, Train Loss: 0.7073, Train Acc: 89.06%, Val Loss: 0.8295, Val Acc: 87.24%, Time: 32.30s \nIter: 780, Train Loss: 0.6425, Train Acc: 92.19%, Val Loss: 0.8136, Val Acc: 87.95%, Time: 38.42s \nEpoch: 5\nIter: 810, Train Loss: 0.5519, Train Acc: 93.75%, Val Loss: 0.7738, Val Acc: 88.74%, Time: 4.18s *\nIter: 840, Train Loss:  0.509, Train Acc: 92.19%, Val Loss: 0.7503, Val Acc: 88.60%, Time: 10.25s \nIter: 870, Train Loss: 0.3355, Train Acc: 95.31%, Val Loss: 0.8516, Val Acc: 87.24%, Time: 16.21s \nIter: 900, Train Loss: 0.2326, Train Acc: 96.88%, Val Loss: 0.7708, Val Acc: 88.88%, Time: 22.18s *\nIter: 930, Train Loss: 0.2093, Train Acc: 96.88%, Val Loss: 0.8652, Val Acc: 87.10%, Time: 28.17s \nIter: 960, Train Loss: 0.3669, Train Acc: 92.19%, Val Loss:  0.759, Val Acc: 87.81%, Time: 34.13s \nIter: 990, Train Loss: 0.3227, Train Acc: 95.83%, Val Loss: 0.7329, Val Acc: 89.24%, Time: 40.05s *\nEpoch: 6\nIter: 1020, Train Loss: 0.2278, Train Acc: 96.88%, Val Loss: 0.7188, Val Acc: 89.17%, Time: 5.92s \nIter: 1050, Train Loss: 0.2797, Train Acc: 96.88%, Val Loss: 0.8168, Val Acc: 88.31%, Time: 11.89s \nIter: 1080, Train Loss: 0.2595, Train Acc: 95.31%, Val Loss: 0.7068, Val Acc: 89.24%, Time: 17.96s \nIter: 1110, Train Loss: 0.2203, Train Acc: 98.44%, Val Loss: 0.7673, Val Acc: 88.31%, Time: 23.86s \nIter: 1140, Train Loss: 0.2393, Train Acc: 95.31%, Val Loss: 0.6918, Val Acc: 90.16%, Time: 29.88s *\nIter: 1170, Train Loss: 0.1418, Train Acc: 96.88%, Val Loss: 0.7559, Val Acc: 89.38%, Time: 35.99s \nEpoch: 7\nIter: 1200, Train Loss: 0.08821, Train Acc: 98.44%, Val Loss: 0.7571, Val Acc: 90.09%, Time: 3.26s \nIter: 1230, Train Loss: 0.5159, Train Acc: 92.19%, Val Loss: 0.8632, Val Acc: 87.53%, Time: 9.25s \nIter: 1260, Train Loss: 0.09234, Train Acc: 100.00%, Val Loss: 0.7149, Val Acc: 89.31%, Time: 15.20s \nIter: 1290, Train Loss: 0.4846, Train Acc: 92.19%, Val Loss: 0.7376, Val Acc: 89.17%, Time: 21.17s \nIter: 1320, Train Loss: 0.2296, Train Acc: 96.88%, Val Loss: 0.7703, Val Acc: 89.81%, Time: 27.15s \nIter: 1350, Train Loss: 0.1913, Train Acc: 96.88%, Val Loss: 0.7383, Val Acc: 89.24%, Time: 33.16s \nIter: 1380, Train Loss: 0.2053, Train Acc: 96.88%, Val Loss: 0.7356, Val Acc: 89.45%, Time: 39.17s \nEpoch: 8\nIter: 1410, Train Loss: 0.2447, Train Acc: 98.44%, Val Loss: 0.6831, Val Acc: 90.66%, Time: 5.08s *\nIter: 1440, Train Loss: 0.06404, Train Acc: 100.00%, Val Loss: 0.7726, Val Acc: 89.31%, Time: 11.07s \nIter: 1470, Train Loss: 0.4335, Train Acc: 95.31%, Val Loss:  0.876, Val Acc: 89.17%, Time: 17.00s \nIter: 1500, Train Loss: 0.2341, Train Acc: 98.44%, Val Loss: 0.7923, Val Acc: 89.09%, Time: 22.95s \nIter: 1530, Train Loss: 0.2109, Train Acc: 93.75%, Val Loss: 0.8404, Val Acc: 89.24%, Time: 28.88s \nIter: 1560, Train Loss: 0.07723, Train Acc: 98.44%, Val Loss: 0.6912, Val Acc: 91.16%, Time: 34.80s *\nEpoch: 9\nIter: 1590, Train Loss: 0.06214, Train Acc: 100.00%, Val Loss: 0.7448, Val Acc: 90.73%, Time: 2.34s \nIter: 1620, Train Loss: 0.2361, Train Acc: 96.88%, Val Loss: 0.6812, Val Acc: 90.88%, Time: 8.26s \nIter: 1650, Train Loss: 0.07411, Train Acc: 98.44%, Val Loss: 0.6925, Val Acc: 90.95%, Time: 14.24s \nIter: 1680, Train Loss: 0.02842, Train Acc: 100.00%, Val Loss: 0.7974, Val Acc: 90.52%, Time: 20.25s \nIter: 1710, Train Loss: 0.2002, Train Acc: 95.31%, Val Loss: 0.7531, Val Acc: 89.81%, Time: 26.17s \nIter: 1740, Train Loss: 0.1458, Train Acc: 98.44%, Val Loss: 0.7672, Val Acc: 90.24%, Time: 32.06s \nIter: 1770, Train Loss: 0.04739, Train Acc: 100.00%, Val Loss: 0.7399, Val Acc: 89.59%, Time: 38.04s \nEpoch: 10\nIter: 1800, Train Loss: 0.04486, Train Acc: 100.00%, Val Loss: 0.8485, Val Acc: 89.74%, Time: 4.19s \nIter: 1830, Train Loss: 0.1476, Train Acc: 98.44%, Val Loss:  0.753, Val Acc: 90.38%, Time: 10.20s \nIter: 1860, Train Loss: 0.07439, Train Acc: 98.44%, Val Loss: 0.7507, Val Acc: 90.31%, Time: 16.09s \nIter: 1890, Train Loss: 0.08447, Train Acc: 98.44%, Val Loss:  0.757, Val Acc: 90.16%, Time: 22.04s \nIter: 1920, Train Loss: 0.1885, Train Acc: 96.88%, Val Loss: 0.7577, Val Acc: 90.38%, Time: 27.99s \nIter: 1950, Train Loss: 0.0984, Train Acc: 98.44%, Val Loss: 0.7396, Val Acc: 89.88%, Time: 33.98s \nIter: 1980, Train Loss:  0.137, Train Acc: 100.00%, Val Loss: 0.7225, Val Acc: 90.45%, Time: 40.04s \nEpoch: 11\nIter: 2010, Train Loss: 0.05658, Train Acc: 98.44%, Val Loss: 0.8242, Val Acc: 89.31%, Time: 5.97s \nIter: 2040, Train Loss: 0.1119, Train Acc: 98.44%, Val Loss: 0.8909, Val Acc: 88.60%, Time: 11.99s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.84      0.95      0.89       133\n                   Filter       0.80      0.92      0.85       147\n    Compute Derived Value       0.92      0.82      0.86       173\n            Find Extremum       0.93      0.95      0.94       157\n                     Sort       0.94      0.94      0.94       124\n          Determine Range       0.91      0.92      0.91       145\nCharacterize Distribution       0.91      0.91      0.91       116\n           Find Anomalies       0.95      0.81      0.87       133\n                  Cluster       0.96      0.92      0.94       131\n                Correlate       0.93      0.93      0.93       144\n\n                micro avg       0.90      0.90      0.90      1403\n                macro avg       0.91      0.91      0.90      1403\n             weighted avg       0.91      0.90      0.90      1403\n\nConfusion Matrix...\n[[126   2   3   0   0   1   0   0   0   1]\n [  3 135   2   1   0   3   0   3   0   0]\n [ 14   5 141   5   1   2   3   1   0   1]\n [  1   1   1 149   2   2   0   0   0   1]\n [  3   0   0   2 116   1   1   0   1   0]\n [  2   5   1   2   0 134   1   0   0   0]\n [  1   4   0   0   1   1 105   0   3   1]\n [  0  14   1   1   0   3   2 108   1   3]\n [  0   0   1   1   3   1   1   1 120   3]\n [  0   3   4   0   0   0   2   1   0 134]]\n[0.9223646723646723, 0.9024216524216524, 0.905982905982906, 0.8853276353276354, 0.8760683760683761, 0.9116179615110478, 0.8973627940128297, 0.8895224518888097, 0.8752672843905915, 0.9037776193870278]\n0.896971335335555, 0.014545787968274725, 0.015332606780540386, 0.00021157994761800575\nexpert\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:   4.63, Train Acc: 10.94%, Val Loss:  4.528, Val Acc: 15.15%, Time: 7.25s *\nIter: 60, Train Loss:  4.162, Train Acc: 23.44%, Val Loss:  4.119, Val Acc: 28.15%, Time: 13.20s *\nIter: 90, Train Loss:  3.693, Train Acc: 32.81%, Val Loss:    3.5, Val Acc: 42.90%, Time: 18.99s *\nIter: 120, Train Loss:  2.756, Train Acc: 54.69%, Val Loss:  3.007, Val Acc: 48.53%, Time: 24.73s *\nIter: 150, Train Loss:  2.188, Train Acc: 64.06%, Val Loss:  2.546, Val Acc: 55.35%, Time: 30.53s *\nIter: 180, Train Loss:  1.972, Train Acc: 68.75%, Val Loss:  2.364, Val Acc: 59.71%, Time: 36.40s *\nEpoch: 2\nIter: 210, Train Loss:  1.907, Train Acc: 65.62%, Val Loss:  2.159, Val Acc: 64.31%, Time: 2.86s *\nIter: 240, Train Loss:  1.319, Train Acc: 78.12%, Val Loss:  2.012, Val Acc: 67.09%, Time: 8.76s *\nIter: 270, Train Loss:  1.839, Train Acc: 73.44%, Val Loss:  1.956, Val Acc: 68.28%, Time: 14.57s *\nIter: 300, Train Loss:  1.668, Train Acc: 67.19%, Val Loss:  1.967, Val Acc: 68.75%, Time: 20.41s *\nIter: 330, Train Loss:  1.058, Train Acc: 82.81%, Val Loss:  1.814, Val Acc: 69.47%, Time: 26.24s *\nIter: 360, Train Loss:  1.038, Train Acc: 89.06%, Val Loss:  1.823, Val Acc: 70.58%, Time: 32.09s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 390, Train Loss: 0.9915, Train Acc: 79.69%, Val Loss:  1.765, Val Acc: 69.94%, Time: 37.91s \nEpoch: 3\nIter: 420, Train Loss: 0.7218, Train Acc: 90.62%, Val Loss:   2.06, Val Acc: 68.83%, Time: 4.31s \nIter: 450, Train Loss: 0.8646, Train Acc: 89.06%, Val Loss:  1.834, Val Acc: 70.66%, Time: 10.12s *\nIter: 480, Train Loss: 0.4589, Train Acc: 98.44%, Val Loss:  2.056, Val Acc: 68.12%, Time: 15.95s \nIter: 510, Train Loss: 0.6045, Train Acc: 93.75%, Val Loss:  1.918, Val Acc: 68.99%, Time: 21.84s \nIter: 540, Train Loss:  1.033, Train Acc: 82.81%, Val Loss:  1.766, Val Acc: 72.01%, Time: 27.71s *\nIter: 570, Train Loss: 0.8466, Train Acc: 89.06%, Val Loss:  1.812, Val Acc: 72.64%, Time: 33.58s *\nIter: 600, Train Loss: 0.2887, Train Acc: 100.00%, Val Loss:   1.78, Val Acc: 72.32%, Time: 39.42s \nEpoch: 4\nIter: 630, Train Loss: 0.3208, Train Acc: 96.88%, Val Loss:  1.931, Val Acc: 70.90%, Time: 5.82s \nIter: 660, Train Loss: 0.7496, Train Acc: 85.94%, Val Loss:  1.882, Val Acc: 72.40%, Time: 11.66s \nIter: 690, Train Loss: 0.7871, Train Acc: 89.06%, Val Loss:  1.859, Val Acc: 71.45%, Time: 17.51s \nIter: 720, Train Loss: 0.5837, Train Acc: 89.06%, Val Loss:  1.959, Val Acc: 70.98%, Time: 23.33s \nIter: 750, Train Loss: 0.2805, Train Acc: 96.88%, Val Loss:  2.025, Val Acc: 71.93%, Time: 29.20s \nIter: 780, Train Loss: 0.4731, Train Acc: 92.19%, Val Loss:  1.996, Val Acc: 71.37%, Time: 35.05s \nEpoch: 5\nIter: 810, Train Loss: 0.6635, Train Acc: 93.75%, Val Loss:  1.997, Val Acc: 72.48%, Time: 2.93s \nIter: 840, Train Loss: 0.1635, Train Acc: 100.00%, Val Loss:  1.833, Val Acc: 73.59%, Time: 8.80s *\nIter: 870, Train Loss: 0.3747, Train Acc: 95.31%, Val Loss:  1.932, Val Acc: 73.59%, Time: 14.61s *\nIter: 900, Train Loss: 0.3373, Train Acc: 95.31%, Val Loss:   2.18, Val Acc: 71.45%, Time: 20.51s \nIter: 930, Train Loss: 0.1611, Train Acc: 98.44%, Val Loss:  2.305, Val Acc: 69.15%, Time: 26.34s \nIter: 960, Train Loss: 0.4396, Train Acc: 93.75%, Val Loss:  2.176, Val Acc: 71.05%, Time: 32.20s \nIter: 990, Train Loss:  1.052, Train Acc: 89.06%, Val Loss:  2.144, Val Acc: 73.28%, Time: 37.91s \nEpoch: 6\nIter: 1020, Train Loss: 0.4991, Train Acc: 93.75%, Val Loss:  2.075, Val Acc: 73.12%, Time: 4.39s \nIter: 1050, Train Loss: 0.2944, Train Acc: 96.88%, Val Loss:  2.093, Val Acc: 73.75%, Time: 10.29s *\nIter: 1080, Train Loss: 0.3908, Train Acc: 93.75%, Val Loss:  2.152, Val Acc: 73.75%, Time: 16.14s \nIter: 1110, Train Loss: 0.06743, Train Acc: 100.00%, Val Loss:  2.069, Val Acc: 73.59%, Time: 21.96s \nIter: 1140, Train Loss: 0.1252, Train Acc: 98.44%, Val Loss:  2.101, Val Acc: 72.88%, Time: 27.77s \nIter: 1170, Train Loss: 0.3264, Train Acc: 96.88%, Val Loss:  2.158, Val Acc: 73.35%, Time: 33.62s \nIter: 1200, Train Loss: 0.4034, Train Acc: 92.11%, Val Loss:  1.962, Val Acc: 74.39%, Time: 39.45s *\nEpoch: 7\nIter: 1230, Train Loss:  0.528, Train Acc: 95.31%, Val Loss:  2.052, Val Acc: 73.67%, Time: 5.84s \nIter: 1260, Train Loss: 0.2177, Train Acc: 98.44%, Val Loss:  2.362, Val Acc: 71.61%, Time: 11.76s \nIter: 1290, Train Loss: 0.2329, Train Acc: 98.44%, Val Loss:  2.282, Val Acc: 71.85%, Time: 17.66s \nIter: 1320, Train Loss: 0.2529, Train Acc: 96.88%, Val Loss:  2.183, Val Acc: 72.32%, Time: 23.47s \nIter: 1350, Train Loss: 0.1576, Train Acc: 95.31%, Val Loss:  2.055, Val Acc: 73.99%, Time: 29.28s \nIter: 1380, Train Loss: 0.2133, Train Acc: 96.88%, Val Loss:  2.073, Val Acc: 74.86%, Time: 35.15s *\nEpoch: 8\nIter: 1410, Train Loss: 0.3405, Train Acc: 96.88%, Val Loss:  2.318, Val Acc: 72.09%, Time: 2.81s \nIter: 1440, Train Loss: 0.2212, Train Acc: 96.88%, Val Loss:  2.223, Val Acc: 72.48%, Time: 8.78s \nIter: 1470, Train Loss: 0.1038, Train Acc: 100.00%, Val Loss:  2.118, Val Acc: 74.54%, Time: 14.62s \nIter: 1500, Train Loss: 0.2539, Train Acc: 96.88%, Val Loss:  2.183, Val Acc: 73.43%, Time: 20.46s \nIter: 1530, Train Loss: 0.1957, Train Acc: 98.44%, Val Loss:  2.226, Val Acc: 74.46%, Time: 26.28s \nIter: 1560, Train Loss: 0.04448, Train Acc: 98.44%, Val Loss:  2.338, Val Acc: 72.32%, Time: 32.09s \nIter: 1590, Train Loss: 0.2025, Train Acc: 96.88%, Val Loss:   2.33, Val Acc: 71.69%, Time: 37.90s \nEpoch: 9\nIter: 1620, Train Loss: 0.2006, Train Acc: 98.44%, Val Loss:  2.293, Val Acc: 74.54%, Time: 4.30s \nIter: 1650, Train Loss: 0.07225, Train Acc: 100.00%, Val Loss:  2.278, Val Acc: 73.51%, Time: 10.19s \nIter: 1680, Train Loss:  0.277, Train Acc: 96.88%, Val Loss:  2.364, Val Acc: 74.46%, Time: 15.98s \nIter: 1710, Train Loss: 0.2217, Train Acc: 96.88%, Val Loss:  2.406, Val Acc: 74.15%, Time: 21.83s \nIter: 1740, Train Loss: 0.08591, Train Acc: 98.44%, Val Loss:  2.257, Val Acc: 75.26%, Time: 27.83s *\nIter: 1770, Train Loss:  0.226, Train Acc: 98.44%, Val Loss:   2.51, Val Acc: 74.70%, Time: 33.66s \nIter: 1800, Train Loss: 0.1626, Train Acc: 94.74%, Val Loss:  2.125, Val Acc: 74.54%, Time: 39.56s \nEpoch: 10\nIter: 1830, Train Loss: 0.01876, Train Acc: 100.00%, Val Loss:  2.344, Val Acc: 74.86%, Time: 5.85s \nIter: 1860, Train Loss: 0.02006, Train Acc: 100.00%, Val Loss:  2.495, Val Acc: 73.83%, Time: 11.67s \nIter: 1890, Train Loss: 0.2652, Train Acc: 95.31%, Val Loss:  2.337, Val Acc: 71.61%, Time: 17.54s \nIter: 1920, Train Loss: 0.1345, Train Acc: 98.44%, Val Loss:  2.234, Val Acc: 73.59%, Time: 23.38s \nIter: 1950, Train Loss: 0.02039, Train Acc: 100.00%, Val Loss:  2.401, Val Acc: 74.23%, Time: 29.14s \nIter: 1980, Train Loss: 0.06355, Train Acc: 100.00%, Val Loss:  2.202, Val Acc: 74.46%, Time: 34.96s \nEpoch: 11\nIter: 2010, Train Loss: 0.06009, Train Acc: 100.00%, Val Loss:   2.23, Val Acc: 74.62%, Time: 2.91s \nIter: 2040, Train Loss: 0.2235, Train Acc: 96.88%, Val Loss:  2.517, Val Acc: 73.59%, Time: 8.93s \nIter: 2070, Train Loss:  0.109, Train Acc: 98.44%, Val Loss:  2.566, Val Acc: 72.64%, Time: 14.84s \nIter: 2100, Train Loss: 0.04972, Train Acc: 100.00%, Val Loss:  2.502, Val Acc: 73.83%, Time: 20.74s \nIter: 2130, Train Loss: 0.02697, Train Acc: 100.00%, Val Loss:  2.548, Val Acc: 73.28%, Time: 26.58s \nIter: 2160, Train Loss: 0.05401, Train Acc: 100.00%, Val Loss:  2.353, Val Acc: 74.07%, Time: 32.34s \nIter: 2190, Train Loss: 0.07971, Train Acc: 98.44%, Val Loss:  2.399, Val Acc: 73.67%, Time: 38.21s \nEpoch: 12\nIter: 2220, Train Loss: 0.01181, Train Acc: 100.00%, Val Loss:  2.304, Val Acc: 74.86%, Time: 4.38s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.59      0.72      0.65       132\n                   Filter       0.60      0.74      0.66       132\n    Compute Derived Value       0.76      0.44      0.56       134\n            Find Extremum       0.88      0.87      0.87       127\n                     Sort       0.91      0.87      0.89       119\n          Determine Range       0.60      0.54      0.57       124\nCharacterize Distribution       0.72      0.92      0.81       127\n           Find Anomalies       0.67      0.73      0.70       116\n                  Cluster       0.87      0.75      0.80       120\n                Correlate       0.89      0.78      0.84       130\n\n                micro avg       0.73      0.73      0.73      1261\n                macro avg       0.75      0.74      0.73      1261\n             weighted avg       0.75      0.73      0.73      1261\n\nConfusion Matrix...\n[[ 95   5  12   3   0  13   1   0   3   0]\n [  6  98   0   4   0   1   1  21   0   1]\n [ 40   3  59   2   0  11  17   0   2   0]\n [  2   2   0 110   2   9   0   0   1   1]\n [  0   3   2   3 103   5   1   1   1   0]\n [  5  35   1   2   2  67   6   1   3   2]\n [  2   0   2   1   1   3 117   0   0   1]\n [  5  12   2   0   0   0  10  85   0   2]\n [  0   3   0   0   5   1   6  10  90   5]\n [  7   3   0   0   0   1   4   9   4 102]]\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.595, Train Acc: 14.06%, Val Loss:  4.547, Val Acc: 10.06%, Time: 7.34s *\nIter: 60, Train Loss:  4.085, Train Acc: 42.19%, Val Loss:  4.187, Val Acc: 36.92%, Time: 13.47s *\nIter: 90, Train Loss:  3.179, Train Acc: 51.56%, Val Loss:  3.416, Val Acc: 43.16%, Time: 19.66s *\nIter: 120, Train Loss:  2.638, Train Acc: 56.25%, Val Loss:  2.946, Val Acc: 53.72%, Time: 25.88s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 150, Train Loss:  2.131, Train Acc: 68.75%, Val Loss:  2.825, Val Acc: 51.24%, Time: 31.97s \nIter: 180, Train Loss:   2.24, Train Acc: 70.31%, Val Loss:  2.367, Val Acc: 63.84%, Time: 38.05s *\nEpoch: 2\nIter: 210, Train Loss:  1.582, Train Acc: 73.44%, Val Loss:  2.461, Val Acc: 60.15%, Time: 3.91s \nIter: 240, Train Loss:  1.703, Train Acc: 70.31%, Val Loss:  2.381, Val Acc: 62.13%, Time: 10.11s \nIter: 270, Train Loss:  1.552, Train Acc: 76.56%, Val Loss:  2.258, Val Acc: 63.78%, Time: 16.28s \nIter: 300, Train Loss:  1.422, Train Acc: 76.56%, Val Loss:  2.315, Val Acc: 63.53%, Time: 22.45s \nIter: 330, Train Loss:  1.436, Train Acc: 76.56%, Val Loss:  2.163, Val Acc: 64.93%, Time: 28.67s *\nIter: 360, Train Loss:  1.283, Train Acc: 81.25%, Val Loss:  1.976, Val Acc: 68.94%, Time: 34.84s *\nIter: 390, Train Loss:   1.22, Train Acc: 83.33%, Val Loss:  1.888, Val Acc: 71.99%, Time: 41.07s *\nEpoch: 3\nIter: 420, Train Loss: 0.7609, Train Acc: 89.06%, Val Loss:  1.957, Val Acc: 69.64%, Time: 6.20s \nIter: 450, Train Loss: 0.8334, Train Acc: 93.75%, Val Loss:  2.065, Val Acc: 68.11%, Time: 12.45s \nIter: 480, Train Loss:   1.12, Train Acc: 85.94%, Val Loss:  2.026, Val Acc: 70.85%, Time: 18.59s \nIter: 510, Train Loss:   1.39, Train Acc: 85.94%, Val Loss:  1.859, Val Acc: 71.99%, Time: 24.74s \nIter: 540, Train Loss: 0.6732, Train Acc: 89.06%, Val Loss:  1.966, Val Acc: 72.69%, Time: 30.90s *\nIter: 570, Train Loss: 0.5326, Train Acc: 96.88%, Val Loss:  2.024, Val Acc: 70.02%, Time: 37.12s \nEpoch: 4\nIter: 600, Train Loss: 0.4258, Train Acc: 93.75%, Val Loss:  1.774, Val Acc: 73.71%, Time: 3.97s *\nIter: 630, Train Loss: 0.6763, Train Acc: 92.19%, Val Loss:  2.254, Val Acc: 68.30%, Time: 10.11s \nIter: 660, Train Loss: 0.6721, Train Acc: 89.06%, Val Loss:  1.972, Val Acc: 72.82%, Time: 16.32s \nIter: 690, Train Loss: 0.6127, Train Acc: 92.19%, Val Loss:  1.909, Val Acc: 71.99%, Time: 22.49s \nIter: 720, Train Loss: 0.6818, Train Acc: 95.31%, Val Loss:  2.026, Val Acc: 71.93%, Time: 28.75s \nIter: 750, Train Loss: 0.4945, Train Acc: 92.19%, Val Loss:  1.856, Val Acc: 74.16%, Time: 35.11s *\nIter: 780, Train Loss: 0.5785, Train Acc: 93.75%, Val Loss:  1.911, Val Acc: 74.54%, Time: 41.35s *\nEpoch: 5\nIter: 810, Train Loss: 0.3306, Train Acc: 93.75%, Val Loss:   1.97, Val Acc: 72.69%, Time: 6.17s \nIter: 840, Train Loss: 0.4511, Train Acc: 92.19%, Val Loss:  2.023, Val Acc: 72.63%, Time: 12.38s \nIter: 870, Train Loss: 0.5425, Train Acc: 95.31%, Val Loss:  2.249, Val Acc: 71.74%, Time: 18.54s \nIter: 900, Train Loss: 0.3916, Train Acc: 96.88%, Val Loss:  2.208, Val Acc: 71.67%, Time: 24.63s \nIter: 930, Train Loss: 0.5574, Train Acc: 93.75%, Val Loss:  1.965, Val Acc: 73.58%, Time: 30.85s \nIter: 960, Train Loss:   0.23, Train Acc: 98.44%, Val Loss:  2.097, Val Acc: 73.97%, Time: 37.03s \nEpoch: 6\nIter: 990, Train Loss: 0.2167, Train Acc: 96.88%, Val Loss:  2.051, Val Acc: 73.84%, Time: 3.96s \nIter: 1020, Train Loss: 0.4835, Train Acc: 92.19%, Val Loss:  2.121, Val Acc: 74.28%, Time: 10.16s \nIter: 1050, Train Loss: 0.1344, Train Acc: 96.88%, Val Loss:  2.274, Val Acc: 72.44%, Time: 16.33s \nIter: 1080, Train Loss: 0.5379, Train Acc: 93.75%, Val Loss:  2.594, Val Acc: 69.96%, Time: 22.46s \nIter: 1110, Train Loss: 0.2834, Train Acc: 95.31%, Val Loss:  2.101, Val Acc: 73.58%, Time: 28.64s \nIter: 1140, Train Loss: 0.2505, Train Acc: 95.31%, Val Loss:  2.324, Val Acc: 69.06%, Time: 34.76s \nIter: 1170, Train Loss: 0.2856, Train Acc: 95.83%, Val Loss:  2.141, Val Acc: 73.39%, Time: 40.96s \nEpoch: 7\nIter: 1200, Train Loss: 0.3585, Train Acc: 95.31%, Val Loss:  2.148, Val Acc: 73.52%, Time: 6.12s \nIter: 1230, Train Loss: 0.1517, Train Acc: 96.88%, Val Loss:  2.461, Val Acc: 70.85%, Time: 12.30s \nIter: 1260, Train Loss: 0.4664, Train Acc: 96.88%, Val Loss:  2.157, Val Acc: 73.65%, Time: 18.55s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.53      0.87      0.66       135\n                   Filter       0.74      0.61      0.67       183\n    Compute Derived Value       0.58      0.80      0.67       162\n            Find Extremum       0.90      0.80      0.85       215\n                     Sort       0.76      0.67      0.71       135\n          Determine Range       0.79      0.82      0.80       130\nCharacterize Distribution       0.88      0.78      0.82       135\n           Find Anomalies       0.80      0.71      0.75       137\n                  Cluster       0.90      0.81      0.85       159\n                Correlate       0.71      0.58      0.64       180\n\n                micro avg       0.74      0.74      0.74      1571\n                macro avg       0.76      0.74      0.74      1571\n             weighted avg       0.76      0.74      0.74      1571\n\nConfusion Matrix...\n[[117   2   8   3   0   2   0   2   0   1]\n [ 21 112  33   2   0   4   0   0   2   9]\n [ 24   2 130   2   0   2   0   1   0   1]\n [  7   8   1 172  11  14   1   1   0   0]\n [ 23  10   3   3  90   1   0   0   4   1]\n [  5   0   4   6   1 106   0   4   1   3]\n [  2   1   7   1   2   0 105   0   3  14]\n [  6  11  12   1   0   0   0  97   1   9]\n [  3   3   2   1  15   0   0   2 128   5]\n [ 11   3  25   0   0   6  14  14   3 104]]\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.581, Train Acc: 10.94%, Val Loss:  4.528, Val Acc: 18.00%, Time: 7.08s *\nIter: 60, Train Loss:  3.913, Train Acc: 39.06%, Val Loss:  3.984, Val Acc: 36.64%, Time: 12.97s *\nIter: 90, Train Loss:  2.618, Train Acc: 68.75%, Val Loss:  3.289, Val Acc: 46.73%, Time: 18.85s *\nIter: 120, Train Loss:  2.776, Train Acc: 56.25%, Val Loss:  2.852, Val Acc: 54.80%, Time: 24.71s *\nIter: 150, Train Loss:  2.082, Train Acc: 71.88%, Val Loss:  2.712, Val Acc: 55.13%, Time: 30.53s *\nIter: 180, Train Loss:   1.79, Train Acc: 73.44%, Val Loss:   2.38, Val Acc: 60.94%, Time: 36.36s *\nEpoch: 2\nIter: 210, Train Loss:  1.505, Train Acc: 78.12%, Val Loss:  2.129, Val Acc: 65.62%, Time: 2.81s *\nIter: 240, Train Loss:  1.432, Train Acc: 79.69%, Val Loss:  2.204, Val Acc: 64.73%, Time: 8.62s \nIter: 270, Train Loss:   1.26, Train Acc: 82.81%, Val Loss:  2.062, Val Acc: 67.64%, Time: 14.50s *\nIter: 300, Train Loss:   1.46, Train Acc: 71.88%, Val Loss:  1.992, Val Acc: 68.20%, Time: 20.40s *\nIter: 330, Train Loss:  1.089, Train Acc: 87.50%, Val Loss:  1.852, Val Acc: 71.27%, Time: 26.26s *\nIter: 360, Train Loss:  1.056, Train Acc: 84.38%, Val Loss:  1.835, Val Acc: 70.78%, Time: 32.06s \nIter: 390, Train Loss:  1.368, Train Acc: 78.12%, Val Loss:  1.763, Val Acc: 71.27%, Time: 38.05s \nEpoch: 3\nIter: 420, Train Loss:   1.05, Train Acc: 81.25%, Val Loss:   1.62, Val Acc: 74.01%, Time: 4.43s *\nIter: 450, Train Loss: 0.9324, Train Acc: 87.50%, Val Loss:  1.665, Val Acc: 73.77%, Time: 10.30s \nIter: 480, Train Loss: 0.6672, Train Acc: 92.19%, Val Loss:  1.712, Val Acc: 73.77%, Time: 16.11s \nIter: 510, Train Loss:  1.129, Train Acc: 82.81%, Val Loss:  1.492, Val Acc: 75.46%, Time: 21.94s *\nIter: 540, Train Loss:  1.036, Train Acc: 85.94%, Val Loss:  1.626, Val Acc: 75.30%, Time: 27.71s \nIter: 570, Train Loss: 0.5605, Train Acc: 90.62%, Val Loss:  1.538, Val Acc: 76.27%, Time: 33.58s *\nIter: 600, Train Loss: 0.8692, Train Acc: 85.00%, Val Loss:  1.506, Val Acc: 76.76%, Time: 39.41s *\nEpoch: 4\nIter: 630, Train Loss: 0.4637, Train Acc: 92.19%, Val Loss:  1.647, Val Acc: 73.61%, Time: 5.83s \nIter: 660, Train Loss: 0.3493, Train Acc: 93.75%, Val Loss:  1.751, Val Acc: 75.06%, Time: 11.64s \nIter: 690, Train Loss: 0.6547, Train Acc: 92.19%, Val Loss:  1.648, Val Acc: 76.59%, Time: 17.49s \nIter: 720, Train Loss: 0.5974, Train Acc: 90.62%, Val Loss:  1.516, Val Acc: 77.32%, Time: 23.32s *\nIter: 750, Train Loss:  1.237, Train Acc: 81.25%, Val Loss:  1.589, Val Acc: 74.17%, Time: 29.15s \nIter: 780, Train Loss: 0.2516, Train Acc: 96.88%, Val Loss:   1.55, Val Acc: 77.08%, Time: 34.97s \nEpoch: 5\nIter: 810, Train Loss: 0.1581, Train Acc: 98.44%, Val Loss:  1.632, Val Acc: 75.14%, Time: 2.88s \nIter: 840, Train Loss: 0.4281, Train Acc: 96.88%, Val Loss:  1.566, Val Acc: 75.95%, Time: 8.69s \nIter: 870, Train Loss: 0.4648, Train Acc: 93.75%, Val Loss:  1.676, Val Acc: 75.87%, Time: 14.52s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 900, Train Loss: 0.4939, Train Acc: 95.31%, Val Loss:  1.691, Val Acc: 75.95%, Time: 20.35s \nIter: 930, Train Loss: 0.3376, Train Acc: 93.75%, Val Loss:  1.566, Val Acc: 76.19%, Time: 26.18s \nIter: 960, Train Loss: 0.2566, Train Acc: 98.44%, Val Loss:  1.521, Val Acc: 77.72%, Time: 32.02s *\nIter: 990, Train Loss: 0.2578, Train Acc: 95.31%, Val Loss:  1.537, Val Acc: 77.89%, Time: 37.75s *\nEpoch: 6\nIter: 1020, Train Loss: 0.3832, Train Acc: 92.19%, Val Loss:  1.712, Val Acc: 75.54%, Time: 4.32s \nIter: 1050, Train Loss: 0.2281, Train Acc: 95.31%, Val Loss:  1.672, Val Acc: 76.92%, Time: 10.19s \nIter: 1080, Train Loss: 0.3194, Train Acc: 96.88%, Val Loss:   1.84, Val Acc: 75.54%, Time: 16.01s \nIter: 1110, Train Loss: 0.2017, Train Acc: 98.44%, Val Loss:   1.63, Val Acc: 76.84%, Time: 21.81s \nIter: 1140, Train Loss: 0.2653, Train Acc: 95.31%, Val Loss:  1.543, Val Acc: 77.08%, Time: 27.60s \nIter: 1170, Train Loss: 0.2229, Train Acc: 96.88%, Val Loss:  1.612, Val Acc: 78.45%, Time: 33.44s *\nIter: 1200, Train Loss: 0.5662, Train Acc: 93.33%, Val Loss:  1.734, Val Acc: 77.08%, Time: 39.32s \nEpoch: 7\nIter: 1230, Train Loss: 0.1221, Train Acc: 98.44%, Val Loss:  1.839, Val Acc: 75.38%, Time: 5.77s \nIter: 1260, Train Loss: 0.3625, Train Acc: 93.75%, Val Loss:  1.745, Val Acc: 78.45%, Time: 11.64s *\nIter: 1290, Train Loss: 0.2394, Train Acc: 98.44%, Val Loss:  2.007, Val Acc: 74.01%, Time: 17.43s \nIter: 1320, Train Loss: 0.4134, Train Acc: 92.19%, Val Loss:  1.982, Val Acc: 74.58%, Time: 23.23s \nIter: 1350, Train Loss: 0.2048, Train Acc: 96.88%, Val Loss:  1.829, Val Acc: 77.32%, Time: 29.05s \nIter: 1380, Train Loss: 0.2455, Train Acc: 96.88%, Val Loss:  1.853, Val Acc: 75.95%, Time: 35.01s \nEpoch: 8\nIter: 1410, Train Loss: 0.3766, Train Acc: 96.88%, Val Loss:  1.989, Val Acc: 74.74%, Time: 2.86s \nIter: 1440, Train Loss: 0.2249, Train Acc: 96.88%, Val Loss:  1.887, Val Acc: 75.38%, Time: 8.68s \nIter: 1470, Train Loss: 0.1094, Train Acc: 98.44%, Val Loss:  1.717, Val Acc: 77.48%, Time: 14.46s \nIter: 1500, Train Loss: 0.1308, Train Acc: 98.44%, Val Loss:  1.944, Val Acc: 74.90%, Time: 20.33s \nIter: 1530, Train Loss: 0.1861, Train Acc: 95.31%, Val Loss:  1.897, Val Acc: 77.89%, Time: 26.15s \nIter: 1560, Train Loss: 0.03506, Train Acc: 100.00%, Val Loss:  1.962, Val Acc: 75.71%, Time: 32.08s \nIter: 1590, Train Loss: 0.1012, Train Acc: 98.44%, Val Loss:  1.987, Val Acc: 75.14%, Time: 37.88s \nEpoch: 9\nIter: 1620, Train Loss: 0.1142, Train Acc: 98.44%, Val Loss:  2.131, Val Acc: 73.20%, Time: 4.36s \nIter: 1650, Train Loss: 0.1292, Train Acc: 98.44%, Val Loss:  1.954, Val Acc: 76.03%, Time: 10.23s \nIter: 1680, Train Loss: 0.1162, Train Acc: 98.44%, Val Loss:  1.935, Val Acc: 76.51%, Time: 16.07s \nIter: 1710, Train Loss: 0.07599, Train Acc: 98.44%, Val Loss:  1.875, Val Acc: 77.97%, Time: 21.88s \nIter: 1740, Train Loss: 0.08233, Train Acc: 100.00%, Val Loss:  1.862, Val Acc: 75.95%, Time: 27.72s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.77      0.77      0.77       123\n                   Filter       0.75      0.59      0.66       128\n    Compute Derived Value       0.55      0.70      0.62       122\n            Find Extremum       0.90      0.66      0.76       128\n                     Sort       0.82      0.71      0.76       112\n          Determine Range       0.79      0.85      0.82       122\nCharacterize Distribution       0.73      0.90      0.80       128\n           Find Anomalies       0.83      0.73      0.77       118\n                  Cluster       0.82      0.89      0.86       127\n                Correlate       0.76      0.79      0.78       131\n\n                micro avg       0.76      0.76      0.76      1239\n                macro avg       0.77      0.76      0.76      1239\n             weighted avg       0.77      0.76      0.76      1239\n\nConfusion Matrix...\n[[ 95   0  12   1   0   4   6   1   1   3]\n [  7  76   9   0   1   6   9   7  11   2]\n [  9   1  86   0   5   3   9   0   3   6]\n [  5   0  27  84   1  10   0   1   0   0]\n [  5   2   0   8  79   0   7   5   3   3]\n [  1   0   4   0   3 104   7   0   0   3]\n [  1   0   2   0   1   4 115   1   2   2]\n [  0  16   2   0   0   1   1  86   1  11]\n [  1   0   2   0   6   0   2   1 113   2]\n [  0   7  13   0   0   0   2   2   3 104]]\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.485, Train Acc: 26.56%, Val Loss:   4.39, Val Acc: 29.95%, Time: 7.39s *\nIter: 60, Train Loss:  3.852, Train Acc: 37.50%, Val Loss:  3.905, Val Acc: 36.92%, Time: 13.54s *\nIter: 90, Train Loss:  3.094, Train Acc: 53.12%, Val Loss:  3.237, Val Acc: 50.03%, Time: 19.81s *\nIter: 120, Train Loss:  2.297, Train Acc: 62.50%, Val Loss:  2.903, Val Acc: 54.12%, Time: 25.95s *\nIter: 150, Train Loss:  2.295, Train Acc: 65.62%, Val Loss:   2.57, Val Acc: 61.70%, Time: 32.09s *\nIter: 180, Train Loss:  1.922, Train Acc: 65.62%, Val Loss:  2.484, Val Acc: 62.00%, Time: 38.39s *\nEpoch: 2\nIter: 210, Train Loss:   1.97, Train Acc: 65.62%, Val Loss:  2.383, Val Acc: 66.51%, Time: 4.13s *\nIter: 240, Train Loss:  1.491, Train Acc: 78.12%, Val Loss:  2.373, Val Acc: 65.18%, Time: 10.37s \nIter: 270, Train Loss: 0.9689, Train Acc: 84.38%, Val Loss:  2.254, Val Acc: 67.41%, Time: 16.59s *\nIter: 300, Train Loss: 0.9831, Train Acc: 89.06%, Val Loss:  2.369, Val Acc: 67.17%, Time: 22.86s \nIter: 330, Train Loss:  1.493, Train Acc: 68.75%, Val Loss:  2.221, Val Acc: 68.61%, Time: 29.14s *\nIter: 360, Train Loss: 0.9957, Train Acc: 85.94%, Val Loss:   2.28, Val Acc: 68.01%, Time: 35.36s \nEpoch: 3\nIter: 390, Train Loss: 0.8955, Train Acc: 85.94%, Val Loss:  2.251, Val Acc: 69.51%, Time: 2.04s *\nIter: 420, Train Loss:  0.553, Train Acc: 90.62%, Val Loss:  2.305, Val Acc: 67.89%, Time: 8.24s \nIter: 450, Train Loss: 0.7522, Train Acc: 89.06%, Val Loss:   2.43, Val Acc: 68.07%, Time: 14.45s \nIter: 480, Train Loss:   1.02, Train Acc: 85.94%, Val Loss:  2.404, Val Acc: 69.27%, Time: 20.65s \nIter: 510, Train Loss: 0.5991, Train Acc: 85.94%, Val Loss:  2.446, Val Acc: 68.91%, Time: 26.79s \nIter: 540, Train Loss: 0.8403, Train Acc: 90.62%, Val Loss:  2.498, Val Acc: 68.55%, Time: 32.98s \nIter: 570, Train Loss: 0.7688, Train Acc: 87.50%, Val Loss:   2.33, Val Acc: 69.45%, Time: 39.27s \nEpoch: 4\nIter: 600, Train Loss: 0.6256, Train Acc: 90.62%, Val Loss:  2.561, Val Acc: 69.45%, Time: 4.45s \nIter: 630, Train Loss: 0.6519, Train Acc: 90.62%, Val Loss:  2.469, Val Acc: 68.73%, Time: 10.74s \nIter: 660, Train Loss: 0.6679, Train Acc: 90.62%, Val Loss:  2.579, Val Acc: 69.69%, Time: 16.90s *\nIter: 690, Train Loss: 0.5867, Train Acc: 89.06%, Val Loss:  2.486, Val Acc: 70.60%, Time: 23.13s *\nIter: 720, Train Loss: 0.6324, Train Acc: 92.19%, Val Loss:  2.337, Val Acc: 70.84%, Time: 29.32s *\nIter: 750, Train Loss:  0.368, Train Acc: 96.88%, Val Loss:  2.487, Val Acc: 68.31%, Time: 35.51s \nEpoch: 5\nIter: 780, Train Loss: 0.3877, Train Acc: 96.88%, Val Loss:  2.802, Val Acc: 68.61%, Time: 2.32s \nIter: 810, Train Loss: 0.6178, Train Acc: 93.75%, Val Loss:  2.639, Val Acc: 70.23%, Time: 8.49s \nIter: 840, Train Loss: 0.6497, Train Acc: 89.06%, Val Loss:    2.7, Val Acc: 70.11%, Time: 14.67s \nIter: 870, Train Loss: 0.4114, Train Acc: 93.75%, Val Loss:  2.809, Val Acc: 67.71%, Time: 20.83s \nIter: 900, Train Loss: 0.3235, Train Acc: 95.31%, Val Loss:  2.829, Val Acc: 69.69%, Time: 27.10s \nIter: 930, Train Loss: 0.3725, Train Acc: 92.19%, Val Loss:  2.654, Val Acc: 70.48%, Time: 33.27s \nIter: 960, Train Loss: 0.2358, Train Acc: 96.88%, Val Loss:  2.655, Val Acc: 70.48%, Time: 39.45s \nEpoch: 6\nIter: 990, Train Loss: 0.1137, Train Acc: 98.44%, Val Loss:  2.841, Val Acc: 68.13%, Time: 4.73s \nIter: 1020, Train Loss: 0.5395, Train Acc: 93.75%, Val Loss:  2.868, Val Acc: 69.03%, Time: 10.87s \nIter: 1050, Train Loss: 0.3438, Train Acc: 95.31%, Val Loss:  2.818, Val Acc: 69.93%, Time: 17.06s \nIter: 1080, Train Loss: 0.1718, Train Acc: 98.44%, Val Loss:  2.768, Val Acc: 70.48%, Time: 23.26s \nIter: 1110, Train Loss: 0.3468, Train Acc: 95.31%, Val Loss:  2.897, Val Acc: 68.73%, Time: 29.47s \nIter: 1140, Train Loss: 0.2168, Train Acc: 96.88%, Val Loss:  2.931, Val Acc: 66.63%, Time: 35.69s \n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 7\nIter: 1170, Train Loss: 0.08347, Train Acc: 98.44%, Val Loss:  2.857, Val Acc: 69.33%, Time: 2.63s \nIter: 1200, Train Loss: 0.2152, Train Acc: 96.88%, Val Loss:  3.109, Val Acc: 68.37%, Time: 8.85s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.60      0.64      0.62       112\n                   Filter       0.68      0.61      0.65       127\n    Compute Derived Value       0.80      0.55      0.65       208\n            Find Extremum       0.78      0.83      0.80       329\n                     Sort       0.84      0.72      0.78       120\n          Determine Range       0.44      0.47      0.46       118\nCharacterize Distribution       0.61      0.69      0.65       180\n           Find Anomalies       0.64      0.63      0.64       145\n                  Cluster       0.71      0.66      0.68       141\n                Correlate       0.66      0.82      0.73       183\n\n                micro avg       0.68      0.68      0.68      1663\n                macro avg       0.68      0.66      0.67      1663\n             weighted avg       0.69      0.68      0.68      1663\n\nConfusion Matrix...\n[[ 72   3  17   1   0   0   8   6   2   3]\n [  3  78   1   3   0  17   2  15   7   1]\n [ 15  16 114  11   5   9  13   6   4  15]\n [ 16   1   6 272   1  24   6   0   3   0]\n [  4   0   2  13  87   1  10   1   2   0]\n [  2  13   1  29  10  56   5   0   2   0]\n [  6   0   0   0   0  10 124   3   3  34]\n [  0   1   1  20   0   1  11  92   3  16]\n [  1   0   1   0   0  10  18  10  93   8]\n [  2   2   0   0   0   0   6  11  12 150]]\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.518, Train Acc: 17.19%, Val Loss:  4.525, Val Acc: 12.22%, Time: 7.05s *\nIter: 60, Train Loss:  3.893, Train Acc: 39.06%, Val Loss:   4.09, Val Acc: 32.36%, Time: 12.87s *\nIter: 90, Train Loss:  3.319, Train Acc: 50.00%, Val Loss:  3.571, Val Acc: 42.96%, Time: 18.64s *\nIter: 120, Train Loss:  2.635, Train Acc: 62.50%, Val Loss:  3.373, Val Acc: 43.85%, Time: 24.51s *\nIter: 150, Train Loss:  2.456, Train Acc: 59.38%, Val Loss:  2.879, Val Acc: 53.48%, Time: 30.38s *\nIter: 180, Train Loss:  1.719, Train Acc: 75.00%, Val Loss:  2.559, Val Acc: 57.36%, Time: 36.25s *\nEpoch: 2\nIter: 210, Train Loss:  1.836, Train Acc: 79.69%, Val Loss:  2.509, Val Acc: 59.39%, Time: 2.78s *\nIter: 240, Train Loss:  1.501, Train Acc: 79.69%, Val Loss:  2.352, Val Acc: 62.38%, Time: 8.57s *\nIter: 270, Train Loss:  1.379, Train Acc: 76.56%, Val Loss:   2.31, Val Acc: 62.94%, Time: 14.60s *\nIter: 300, Train Loss:  1.049, Train Acc: 85.94%, Val Loss:  2.245, Val Acc: 62.94%, Time: 20.54s \nIter: 330, Train Loss:  1.623, Train Acc: 73.44%, Val Loss:  2.242, Val Acc: 65.05%, Time: 26.40s *\nIter: 360, Train Loss:  1.186, Train Acc: 78.12%, Val Loss:  2.333, Val Acc: 61.81%, Time: 32.25s \nIter: 390, Train Loss:  1.128, Train Acc: 82.81%, Val Loss:  2.367, Val Acc: 63.27%, Time: 38.05s \nEpoch: 3\nIter: 420, Train Loss:  1.075, Train Acc: 82.81%, Val Loss:  2.158, Val Acc: 66.34%, Time: 4.32s *\nIter: 450, Train Loss: 0.9768, Train Acc: 82.81%, Val Loss:   2.47, Val Acc: 63.51%, Time: 10.11s \nIter: 480, Train Loss:  1.151, Train Acc: 84.38%, Val Loss:  2.421, Val Acc: 62.54%, Time: 15.94s \nIter: 510, Train Loss: 0.7533, Train Acc: 87.50%, Val Loss:  2.147, Val Acc: 67.56%, Time: 21.77s *\nIter: 540, Train Loss: 0.6462, Train Acc: 89.06%, Val Loss:  2.272, Val Acc: 65.78%, Time: 27.57s \nIter: 570, Train Loss: 0.5455, Train Acc: 89.06%, Val Loss:   2.38, Val Acc: 66.34%, Time: 33.41s \nIter: 600, Train Loss: 0.5009, Train Acc: 92.06%, Val Loss:  2.207, Val Acc: 66.67%, Time: 39.17s \nEpoch: 4\nIter: 630, Train Loss: 0.2531, Train Acc: 98.44%, Val Loss:  2.406, Val Acc: 66.83%, Time: 5.84s \nIter: 660, Train Loss: 0.4024, Train Acc: 96.88%, Val Loss:  2.265, Val Acc: 68.20%, Time: 11.66s *\nIter: 690, Train Loss: 0.6005, Train Acc: 90.62%, Val Loss:  2.312, Val Acc: 66.99%, Time: 17.49s \nIter: 720, Train Loss: 0.4363, Train Acc: 93.75%, Val Loss:  2.306, Val Acc: 66.99%, Time: 23.32s \nIter: 750, Train Loss: 0.4629, Train Acc: 93.75%, Val Loss:  2.312, Val Acc: 66.50%, Time: 29.13s \nIter: 780, Train Loss: 0.4561, Train Acc: 95.31%, Val Loss:  2.258, Val Acc: 68.53%, Time: 34.99s *\nEpoch: 5\nIter: 810, Train Loss: 0.4377, Train Acc: 95.31%, Val Loss:  2.617, Val Acc: 65.45%, Time: 2.81s \nIter: 840, Train Loss: 0.2372, Train Acc: 96.88%, Val Loss:  2.508, Val Acc: 65.53%, Time: 8.60s \nIter: 870, Train Loss: 0.2374, Train Acc: 96.88%, Val Loss:  2.443, Val Acc: 66.99%, Time: 14.42s \nIter: 900, Train Loss: 0.5117, Train Acc: 90.62%, Val Loss:  2.454, Val Acc: 68.61%, Time: 20.28s *\nIter: 930, Train Loss: 0.2625, Train Acc: 96.88%, Val Loss:   2.39, Val Acc: 69.09%, Time: 26.08s *\nIter: 960, Train Loss: 0.3173, Train Acc: 96.88%, Val Loss:  2.495, Val Acc: 67.31%, Time: 31.85s \nIter: 990, Train Loss: 0.2466, Train Acc: 96.88%, Val Loss:  2.462, Val Acc: 67.72%, Time: 37.64s \nEpoch: 6\nIter: 1020, Train Loss: 0.2474, Train Acc: 95.31%, Val Loss:  2.989, Val Acc: 64.72%, Time: 4.32s \nIter: 1050, Train Loss: 0.2209, Train Acc: 98.44%, Val Loss:  2.691, Val Acc: 67.56%, Time: 10.08s \nIter: 1080, Train Loss: 0.1825, Train Acc: 95.31%, Val Loss:  2.895, Val Acc: 66.83%, Time: 15.89s \nIter: 1110, Train Loss: 0.2919, Train Acc: 98.44%, Val Loss:  2.848, Val Acc: 67.88%, Time: 21.71s \nIter: 1140, Train Loss: 0.5812, Train Acc: 93.75%, Val Loss:   2.65, Val Acc: 66.42%, Time: 27.51s \nIter: 1170, Train Loss: 0.07595, Train Acc: 100.00%, Val Loss:  2.637, Val Acc: 68.20%, Time: 33.29s \nIter: 1200, Train Loss: 0.2275, Train Acc: 96.83%, Val Loss:  2.543, Val Acc: 67.72%, Time: 39.18s \nEpoch: 7\nIter: 1230, Train Loss: 0.0853, Train Acc: 100.00%, Val Loss:  2.772, Val Acc: 67.80%, Time: 5.81s \nIter: 1260, Train Loss: 0.2443, Train Acc: 96.88%, Val Loss:  2.808, Val Acc: 67.56%, Time: 11.63s \nIter: 1290, Train Loss: 0.3915, Train Acc: 93.75%, Val Loss:  2.833, Val Acc: 66.34%, Time: 17.64s \nIter: 1320, Train Loss: 0.1723, Train Acc: 98.44%, Val Loss:   2.92, Val Acc: 68.61%, Time: 23.48s \nIter: 1350, Train Loss: 0.09607, Train Acc: 98.44%, Val Loss:  2.917, Val Acc: 67.31%, Time: 29.28s \nIter: 1380, Train Loss: 0.1335, Train Acc: 100.00%, Val Loss:  2.613, Val Acc: 67.56%, Time: 35.06s \nEpoch: 8\nIter: 1410, Train Loss: 0.1939, Train Acc: 98.44%, Val Loss:  2.863, Val Acc: 66.18%, Time: 2.84s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.45      0.71      0.55       109\n                   Filter       0.60      0.50      0.54       119\n    Compute Derived Value       0.60      0.38      0.47       130\n            Find Extremum       0.61      0.83      0.70       123\n                     Sort       0.70      0.78      0.74       110\n          Determine Range       0.68      0.60      0.63       126\nCharacterize Distribution       0.73      0.61      0.66       123\n           Find Anomalies       0.83      0.77      0.79       124\n                  Cluster       0.71      0.91      0.80       118\n                Correlate       0.89      0.66      0.76       154\n\n                micro avg       0.67      0.67      0.67      1236\n                macro avg       0.68      0.67      0.67      1236\n             weighted avg       0.69      0.67      0.67      1236\n\nConfusion Matrix...\n[[ 77   3  10   6   7   5   0   0   1   0]\n [ 16  59   1  16   9  16   1   1   0   0]\n [ 35  14  50  24   0   6   0   0   1   0]\n [  7   6   3 102   2   2   0   0   1   0]\n [  1   1   0  15  86   0   2   1   4   0]\n [  7   7   8   2  11  75   1   0  14   1]\n [ 12   7   4   0   3   5  75   8   9   0]\n [  2   1   1   3   1   1   6  95   5   9]\n [  2   0   0   0   4   0   0   3 107   2]\n [ 11   0   7   0   0   1  18   7   8 102]]\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.383, Train Acc: 15.62%, Val Loss:   4.49, Val Acc:  9.93%, Time: 7.58s *\nIter: 60, Train Loss:  3.931, Train Acc: 34.38%, Val Loss:  3.991, Val Acc: 30.48%, Time: 14.01s *\nIter: 90, Train Loss:  3.469, Train Acc: 45.31%, Val Loss:  3.315, Val Acc: 48.17%, Time: 20.37s *\n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 120, Train Loss:  2.626, Train Acc: 60.94%, Val Loss:  2.885, Val Acc: 55.71%, Time: 26.75s *\nIter: 150, Train Loss:  1.953, Train Acc: 73.44%, Val Loss:  2.544, Val Acc: 59.08%, Time: 33.15s *\nIter: 180, Train Loss:   2.62, Train Acc: 56.25%, Val Loss:   2.34, Val Acc: 63.18%, Time: 39.48s *\nEpoch: 2\nIter: 210, Train Loss:   1.34, Train Acc: 78.12%, Val Loss:  2.088, Val Acc: 66.72%, Time: 4.58s *\nIter: 240, Train Loss:  1.504, Train Acc: 75.00%, Val Loss:    2.0, Val Acc: 67.75%, Time: 10.88s *\nIter: 270, Train Loss:  1.229, Train Acc: 78.12%, Val Loss:  2.039, Val Acc: 68.38%, Time: 17.16s *\nIter: 300, Train Loss:  0.831, Train Acc: 87.50%, Val Loss:  2.034, Val Acc: 68.66%, Time: 23.57s *\nIter: 330, Train Loss:  1.281, Train Acc: 78.12%, Val Loss:  1.949, Val Acc: 70.26%, Time: 29.92s *\nIter: 360, Train Loss: 0.9772, Train Acc: 85.94%, Val Loss:   1.86, Val Acc: 69.98%, Time: 36.31s \nEpoch: 3\nIter: 390, Train Loss: 0.9694, Train Acc: 85.94%, Val Loss:  1.851, Val Acc: 71.06%, Time: 2.75s *\nIter: 420, Train Loss:  1.031, Train Acc: 87.50%, Val Loss:  1.865, Val Acc: 70.66%, Time: 9.13s \nIter: 450, Train Loss:  1.561, Train Acc: 76.56%, Val Loss:  1.907, Val Acc: 70.32%, Time: 15.50s \nIter: 480, Train Loss:   0.71, Train Acc: 90.62%, Val Loss:  1.967, Val Acc: 69.24%, Time: 21.84s \nIter: 510, Train Loss: 0.9858, Train Acc: 81.25%, Val Loss:  1.952, Val Acc: 69.92%, Time: 28.15s \nIter: 540, Train Loss:  1.015, Train Acc: 87.50%, Val Loss:  1.729, Val Acc: 72.32%, Time: 34.52s *\nIter: 570, Train Loss: 0.7936, Train Acc: 89.06%, Val Loss:  1.921, Val Acc: 70.38%, Time: 40.75s \nEpoch: 4\nIter: 600, Train Loss: 0.4714, Train Acc: 90.62%, Val Loss:  1.937, Val Acc: 71.12%, Time: 5.44s \nIter: 630, Train Loss: 0.7356, Train Acc: 92.19%, Val Loss:  2.069, Val Acc: 70.55%, Time: 11.89s \nIter: 660, Train Loss: 0.5134, Train Acc: 93.75%, Val Loss:  1.915, Val Acc: 72.09%, Time: 18.23s \nIter: 690, Train Loss: 0.2344, Train Acc: 96.88%, Val Loss:  1.978, Val Acc: 72.03%, Time: 24.61s \nIter: 720, Train Loss: 0.7166, Train Acc: 87.50%, Val Loss:  1.936, Val Acc: 71.35%, Time: 30.96s \nIter: 750, Train Loss: 0.3026, Train Acc: 98.44%, Val Loss:   2.06, Val Acc: 71.46%, Time: 37.31s \nEpoch: 5\nIter: 780, Train Loss: 0.5583, Train Acc: 93.75%, Val Loss:  2.076, Val Acc: 70.55%, Time: 3.72s \nIter: 810, Train Loss:   0.52, Train Acc: 85.94%, Val Loss:  2.133, Val Acc: 71.29%, Time: 10.01s \nIter: 840, Train Loss: 0.2534, Train Acc: 96.88%, Val Loss:  2.092, Val Acc: 71.23%, Time: 16.36s \nIter: 870, Train Loss:  0.284, Train Acc: 95.31%, Val Loss:  2.227, Val Acc: 69.01%, Time: 22.63s \nIter: 900, Train Loss: 0.3679, Train Acc: 93.75%, Val Loss:  2.457, Val Acc: 70.03%, Time: 29.10s \nIter: 930, Train Loss: 0.2461, Train Acc: 96.88%, Val Loss:  2.211, Val Acc: 70.95%, Time: 35.40s \nIter: 960, Train Loss: 0.4043, Train Acc: 94.92%, Val Loss:  2.458, Val Acc: 69.75%, Time: 41.73s \nEpoch: 6\nIter: 990, Train Loss: 0.3162, Train Acc: 95.31%, Val Loss:  2.478, Val Acc: 69.63%, Time: 6.32s \nIter: 1020, Train Loss:  0.275, Train Acc: 96.88%, Val Loss:  2.374, Val Acc: 70.15%, Time: 12.67s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.55      0.61      0.57       226\n                   Filter       0.59      0.45      0.51       216\n    Compute Derived Value       0.57      0.51      0.54       170\n            Find Extremum       0.86      0.70      0.77       199\n                     Sort       0.88      0.82      0.85       123\n          Determine Range       0.71      0.62      0.66       130\nCharacterize Distribution       0.87      0.86      0.86       156\n           Find Anomalies       0.52      0.72      0.60       204\n                  Cluster       0.74      0.81      0.78       123\n                Correlate       0.83      0.91      0.87       205\n\n                micro avg       0.69      0.69      0.69      1752\n                macro avg       0.71      0.70      0.70      1752\n             weighted avg       0.70      0.69      0.69      1752\n\nConfusion Matrix...\n[[137   2  31   0   0   2   1  50   2   1]\n [ 19  97  24  12   0  19   1  38   5   1]\n [ 27  15  87   0   0   2  13  16   0  10]\n [ 15   6   4 139  11   3   0  20   1   0]\n [ 10   0   1   1 101   0   0   0   6   4]\n [ 16   8   3   7   0  81   4   1   6   4]\n [  1   1   1   1   2   4 134   2  10   0]\n [  9  22   1   0   1   3   1 146   4  17]\n [ 16   3   0   1   0   0   0   2 100   1]\n [  1  11   1   1   0   0   0   4   1 186]]\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:  4.421, Train Acc: 32.81%, Val Loss:  4.509, Val Acc: 20.18%, Time: 6.94s *\nIter: 60, Train Loss:  3.921, Train Acc: 39.06%, Val Loss:  4.057, Val Acc: 32.39%, Time: 12.69s *\nIter: 90, Train Loss:  3.094, Train Acc: 45.31%, Val Loss:  3.481, Val Acc: 39.27%, Time: 18.37s *\nIter: 120, Train Loss:  2.248, Train Acc: 70.31%, Val Loss:  3.165, Val Acc: 48.44%, Time: 24.04s *\nIter: 150, Train Loss:  2.365, Train Acc: 65.62%, Val Loss:   2.93, Val Acc: 51.10%, Time: 29.83s *\nIter: 180, Train Loss:  2.159, Train Acc: 70.31%, Val Loss:  2.793, Val Acc: 56.42%, Time: 35.54s *\nEpoch: 2\nIter: 210, Train Loss:  1.604, Train Acc: 71.88%, Val Loss:  2.594, Val Acc: 57.52%, Time: 2.23s *\nIter: 240, Train Loss:  1.205, Train Acc: 84.38%, Val Loss:  2.633, Val Acc: 58.35%, Time: 7.87s *\nIter: 270, Train Loss:  1.736, Train Acc: 73.44%, Val Loss:  2.623, Val Acc: 59.63%, Time: 13.57s *\nIter: 300, Train Loss:  1.342, Train Acc: 78.12%, Val Loss:  2.517, Val Acc: 60.18%, Time: 19.32s *\nIter: 330, Train Loss:  1.482, Train Acc: 78.12%, Val Loss:  2.501, Val Acc: 60.46%, Time: 25.02s *\nIter: 360, Train Loss:  1.264, Train Acc: 85.94%, Val Loss:  2.562, Val Acc: 59.08%, Time: 30.69s \nIter: 390, Train Loss:  1.599, Train Acc: 79.69%, Val Loss:  2.643, Val Acc: 59.91%, Time: 36.38s \nEpoch: 3\nIter: 420, Train Loss: 0.8319, Train Acc: 85.94%, Val Loss:  2.702, Val Acc: 59.27%, Time: 3.26s \nIter: 450, Train Loss: 0.7357, Train Acc: 87.50%, Val Loss:  2.568, Val Acc: 62.02%, Time: 8.96s *\nIter: 480, Train Loss:  1.321, Train Acc: 84.38%, Val Loss:   2.55, Val Acc: 61.47%, Time: 14.70s \nIter: 510, Train Loss:  0.986, Train Acc: 82.81%, Val Loss:  2.598, Val Acc: 61.10%, Time: 20.43s \nIter: 540, Train Loss: 0.5704, Train Acc: 93.75%, Val Loss:  2.773, Val Acc: 61.93%, Time: 26.09s \nIter: 570, Train Loss: 0.8612, Train Acc: 82.81%, Val Loss:  2.746, Val Acc: 60.64%, Time: 31.76s \nIter: 600, Train Loss: 0.5927, Train Acc: 92.19%, Val Loss:  2.655, Val Acc: 59.54%, Time: 37.49s \nEpoch: 4\nIter: 630, Train Loss: 0.2587, Train Acc: 96.88%, Val Loss:    2.7, Val Acc: 62.94%, Time: 4.37s *\nIter: 660, Train Loss: 0.2379, Train Acc: 98.44%, Val Loss:  2.928, Val Acc: 60.18%, Time: 9.95s \nIter: 690, Train Loss: 0.4406, Train Acc: 96.88%, Val Loss:  2.784, Val Acc: 63.12%, Time: 15.72s *\nIter: 720, Train Loss: 0.6774, Train Acc: 92.19%, Val Loss:  3.055, Val Acc: 60.92%, Time: 21.52s \nIter: 750, Train Loss: 0.8801, Train Acc: 90.62%, Val Loss:  2.692, Val Acc: 64.13%, Time: 27.30s *\nIter: 780, Train Loss: 0.6774, Train Acc: 87.50%, Val Loss:  2.848, Val Acc: 62.84%, Time: 33.03s \nIter: 810, Train Loss: 0.6884, Train Acc: 92.19%, Val Loss:  2.809, Val Acc: 61.01%, Time: 38.70s \nEpoch: 5\nIter: 840, Train Loss: 0.2871, Train Acc: 96.88%, Val Loss:  2.897, Val Acc: 61.56%, Time: 5.33s \nIter: 870, Train Loss: 0.2717, Train Acc: 96.88%, Val Loss:  2.916, Val Acc: 63.30%, Time: 10.98s \nIter: 900, Train Loss: 0.2875, Train Acc: 96.88%, Val Loss:  2.896, Val Acc: 60.64%, Time: 16.65s \nIter: 930, Train Loss: 0.6223, Train Acc: 93.75%, Val Loss:  2.868, Val Acc: 62.66%, Time: 22.32s \nIter: 960, Train Loss: 0.6642, Train Acc: 92.19%, Val Loss:  2.938, Val Acc: 62.66%, Time: 28.03s \nIter: 990, Train Loss: 0.2321, Train Acc: 96.88%, Val Loss:   2.93, Val Acc: 62.75%, Time: 33.78s \nEpoch: 6\nIter: 1020, Train Loss: 0.2243, Train Acc: 93.75%, Val Loss:  3.001, Val Acc: 61.65%, Time: 1.98s \nIter: 1050, Train Loss: 0.1581, Train Acc: 98.44%, Val Loss:  3.307, Val Acc: 62.29%, Time: 7.74s \nIter: 1080, Train Loss:  0.188, Train Acc: 96.88%, Val Loss:  3.283, Val Acc: 61.38%, Time: 13.43s \n", "name": "stdout"}, {"output_type": "stream", "text": "Iter: 1110, Train Loss: 0.3052, Train Acc: 96.88%, Val Loss:   3.01, Val Acc: 62.29%, Time: 19.17s \nIter: 1140, Train Loss: 0.07053, Train Acc: 98.44%, Val Loss:  3.415, Val Acc: 59.91%, Time: 24.80s \nIter: 1170, Train Loss: 0.4058, Train Acc: 95.31%, Val Loss:  2.992, Val Acc: 62.66%, Time: 30.44s \nIter: 1200, Train Loss: 0.3213, Train Acc: 96.88%, Val Loss:   3.09, Val Acc: 60.46%, Time: 36.16s \nEpoch: 7\nIter: 1230, Train Loss: 0.3383, Train Acc: 93.75%, Val Loss:   3.15, Val Acc: 61.47%, Time: 3.06s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.52      0.47      0.49       118\n                   Filter       0.51      0.36      0.43       129\n    Compute Derived Value       0.58      0.69      0.63       124\n            Find Extremum       0.50      0.79      0.61       114\n                     Sort       0.89      0.93      0.91       120\n          Determine Range       0.73      0.55      0.63       100\nCharacterize Distribution       0.56      0.61      0.58        90\n           Find Anomalies       0.68      0.61      0.64        93\n                  Cluster       0.57      0.46      0.51       100\n                Correlate       0.63      0.65      0.64       102\n\n                micro avg       0.61      0.61      0.61      1090\n                macro avg       0.62      0.61      0.61      1090\n             weighted avg       0.62      0.61      0.61      1090\n\nConfusion Matrix...\n[[ 55  13  22   1   7   0  14   0   1   5]\n [ 17  47  17  22   1   9   4  11   1   0]\n [  6   4  85  22   0   0   1   6   0   0]\n [ 13   3   0  90   0   0   0   4   1   3]\n [  0   1   0   1 111   0   0   0   7   0]\n [  4   1  11  11   1  55  15   1   1   0]\n [  0   0   3  15   2   2  55   1  12   0]\n [  8   1   2   1   0   1   7  57  10   6]\n [  1  12   3   0   1   8   2   3  46  24]\n [  1  10   3  17   2   0   1   1   1  66]]\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter: 30, Train Loss:   4.53, Train Acc: 14.06%, Val Loss:  4.446, Val Acc: 25.32%, Time: 7.52s *\nIter: 60, Train Loss:  3.688, Train Acc: 37.50%, Val Loss:  3.975, Val Acc: 34.48%, Time: 13.87s *\nIter: 90, Train Loss:  2.713, Train Acc: 60.94%, Val Loss:  3.532, Val Acc: 43.36%, Time: 20.26s *\nIter: 120, Train Loss:  2.562, Train Acc: 56.25%, Val Loss:  3.042, Val Acc: 47.54%, Time: 26.62s *\nIter: 150, Train Loss:  2.167, Train Acc: 70.31%, Val Loss:  2.982, Val Acc: 51.26%, Time: 33.05s *\nIter: 180, Train Loss:  1.889, Train Acc: 75.00%, Val Loss:  2.843, Val Acc: 53.67%, Time: 39.41s *\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 19, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-9-07130408a9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest_acc_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-3-39a028c39de0>\u001b[0m in \u001b[0;36mtrain_split_data\u001b[0;34m(model, train_data, test_data, split_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-6-2c493f70eb2b>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, x_val, y_val, categories)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# \u8fd0\u884c\u4f18\u5316\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-c7d86422cf4f>\", line 14, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-7-a79079a13536>\", line 19, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-7-a79079a13536>\", line 101, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\t [[Node: Adam/update/_46 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3036_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"]}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}