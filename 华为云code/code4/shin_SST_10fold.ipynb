{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf\nimport sys\nimport time\nfrom datetime import timedelta\nimport tensorflow.contrib.keras as kr\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/SST.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    train_data = []\n    seq_length = 0\n    with open(filePath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.strip().split()\n            label = int(word[0])\n            content = word[1:]\n            train_data.append([content,label])\n            \n            if (len(content) > seq_length):\n                seq_length = len(content)\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data), seq_length\n\n\ndef loadGloVe(filename):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i])  # label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\nprint(len(embedding),embedding_dim,vocab_size)\n\ntrainData, seq_length = readfile(trainDataPath)\nprint(len(trainData), seq_length)\n\n# seq_length = 37", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n400000 100 400000\n11748 56\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def train_10_fold(train_data, categories):\n    \n    tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n    print(len(tx),len(tx[0]),len(tx[1]))\n    \n    fold_id = 0\n    test_acc = []\n    \n    kf = KFold(n_splits=10)\n    for train_i, test_i in kf.split(tx):\n        fold_id += 1\n        print(\"Fold: \", fold_id)\n        test_acc.append(classifier.train(\n            X_train=tx[train_i],\n            y_train=ty[train_i],\n            X_eval=tx[test_i],\n            y_eval=ty[test_i],\n            categories=categories,\n            epochs=30\n        ))\n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class Classifier:\n\n    def __init__(self, model, input_length, output_length):\n        self.model = model\n        self.input_length = input_length\n        self.output_length = output_length\n\n    def compile(self, batch_size=32):\n        self._ds_x = tf.placeholder(tf.float32, [None, self.input_length])\n        self._ds_y = tf.placeholder(tf.float32, [None, self.output_length])\n\n        ds = tf.data.Dataset.from_tensor_slices((self._ds_x, self._ds_y))\n        ds = ds.batch(batch_size)\n\n        self._ds_it = ds.make_initializable_iterator()\n        self._input, self._labels = self._ds_it.get_next()\n\n        self._features = self.model(self._input)\n        self._output = _create_dense_layer(self._features, self.output_length)\n\n        self._create_acc_computations()\n        self._create_backpropagation()\n\n    def _create_acc_computations(self):\n        self._predictions = tf.argmax(self._output, 1)\n        labels = tf.argmax(self._labels, 1)\n        self._accuracy = tf.reduce_mean(\n            tf.cast(tf.equal(self._predictions, labels), 'float32'))\n\n    def _create_backpropagation(self):\n        losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self._output,\n            labels=self._labels)\n        self._loss = tf.reduce_mean(losses)\n\n        optimizer = tf.train.AdamOptimizer(0.001)\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n        grads_and_vars = optimizer.compute_gradients(self._loss)\n\n        self._train_op = optimizer.apply_gradients(\n            grads_and_vars, global_step=global_step)\n\n    def summary(self):\n        print('input:', self._input.shape)\n        self.model.summary()\n        print('output:', self._output.shape)\n\n    def train(self, X_train, y_train, X_eval, y_eval, categories, epochs=20, require_improve=3):\n        \n        session = tf.Session()\n        session.run(tf.global_variables_initializer())\n        session.run(tf.local_variables_initializer())\n        \n        best_vac_acc = 0.0\n        last_improved = 0\n        \n        for e in range(epochs):\n            start_time = time.time()\n            loss, acc = self._train(X_train, y_train, session)\n            duration = time.time() - start_time\n\n            val_loss, val_acc = self._eval(X_eval, y_eval, session)\n            \n            if val_acc > best_vac_acc:\n                best_vac_acc = val_acc\n                last_improved = e\n                improved_str = '*'\n            else:\n                improved_str = ''\n            \n            output = 'Epoch: {:>1}, Train Loss: {:>6.4}, Train Acc: {:>6.2%}, Val Loss: {:>6.4}, Val Acc: {:>6.2%}, Time: {:.2f}s {}'\n            print(output.format(e + 1, loss, acc, val_loss, val_acc, duration, improved_str))\n            \n            if e - last_improved > require_improve:\n                print(\"No optimization for a long time, auto-stopping...\")\n                \n                y_test_cls = np.argmax(y_eval, 1)  # \u83b7\u5f97\u7c7b\u522b\n                y_test_pred_cls = np.argmax(self.predict(X_eval, session), 1)\n                accuracy_score = metrics.accuracy_score(y_test_cls, y_test_pred_cls)\n                \n                # evaluate\n                print(\"Precision, Recall and F1-Score...\")\n                print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n                '''\n                sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n                    y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n                    y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n                    labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n                    target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n                    \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n                '''\n\n                print(\"Confusion Matrix...\")\n                print(metrics.confusion_matrix(y_test_cls, y_test_pred_cls))\n                '''\n                \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n                \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n                https://blog.csdn.net/u011734144/article/details/80277225\n                '''\n                break\n        # endfor\n        session.close()\n        return accuracy_score\n\n    def _train(self, X_train, y_train, session):\n        import numpy as np\n\n        session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_train,\n                self._ds_y: y_train\n            })\n        loss, acc, = [], []\n        while True:\n            try:\n                _, vloss, vacc = session.run(\n                    fetches=[self._train_op, self._loss, self._accuracy])\n\n                loss.append(vloss)\n                acc.append(vacc)\n            except tf.errors.OutOfRangeError:\n                break\n        # endwhile\n\n        loss, acc = np.mean(loss), np.mean(acc)\n        return loss, acc\n\n    def _eval(self, X_val, y_val, session):\n        session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_val,\n                self._ds_y: y_val\n            })\n\n        loss, acc, = 0, 0\n        while True:\n            try:\n                l, vloss, vacc = session.run(\n                    fetches=[self._labels, self._loss, self._accuracy])\n\n                loss += vloss * len(l)\n                acc += vacc * len(l)\n            except tf.errors.OutOfRangeError:\n                break\n\n        return loss / len(X_val), acc / len(X_val)\n\n    def predict(self, X, session):\n        session.run(self._ds_it.initializer,\n                         feed_dict={\n                             self._ds_x: X,\n                             self._ds_y: np.empty((len(X), self.output_length))\n                         }\n                         )\n\n        pred = list()\n        while True:\n            try:\n                ppred = session.run(tf.nn.softmax(self._output))\n\n                pred.extend(map(lambda l: l.tolist(), ppred))\n            except tf.errors.OutOfRangeError:\n                break\n\n        return pred\n\ndef _create_dense_layer(x, output_length):\n    '''Creates a dense layer\n    '''\n    input_size = x.shape[1].value\n    W = tf.Variable(\n        initial_value=tf.truncated_normal(\n            shape=[input_size, output_length],\n            stddev=0.1))\n    b = tf.Variable(\n        initial_value=tf.truncated_normal(\n            shape=[output_length]))\n\n    dense = tf.nn.xw_plus_b(x, W, b)\n\n    return dense", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class ShinContextualModel:\n    '''\n    Implementation proposal of: http://milab.snu.ac.kr/pub/BigComp2018.pdf\n    '''\n    def __init__(self, embedding, conv_size=3, num_layers=2):\n        '''Constructor.\n        # Parameters:\n        embedding: numpy array representing the embedding.\n        conv_size: Size of the convolutions. Number of words that takes each\n            convolution step.\n        num_layers: Number of recurrent convolutions.\n        '''\n        self._embedding = embedding\n        self._conv_size = conv_size\n        self._num_layers = num_layers\n\n    def __call__(self, input):\n        self._embedding_tf = self._create_embedding_layer(\n            self._embedding, input)\n        self._convolution_tf = self._create_convolutional_layers(\n            self._conv_size, self._num_layers, self._embedding_tf)\n        self._pooling_tf = self._create_maxpooling_layer(self._convolution_tf)\n\n        return self._pooling_tf\n\n    def summary(self):\n        print('embedding:', str(self._embedding_tf.shape))\n        print('conv:', str(self._convolution_tf.shape))\n        print('pool:', str(self._pooling_tf.shape))\n\n    def _create_embedding_layer(self, embedding, input_x):\n        embedding = tf.Variable(initial_value=embedding)\n\n        embedded_chars = tf.nn.embedding_lookup(\n            embedding, tf.cast(input_x, 'int32'))\n\n        return embedded_chars\n\n    def _create_convolutional_layers(self, conv_size, num_layers, embedding):\n        filter_height = conv_size\n        filter_width = embedding.shape[2].value\n\n        filter_shape = [filter_height, filter_width, filter_width]\n\n        W = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=filter_shape,\n                stddev=0.1))\n        b = tf.Variable(\n            initial_value=tf.truncated_normal(\n                shape=[filter_width]))\n\n        z = embedding\n        for _ in range(num_layers):\n            conv = tf.nn.conv1d(\n                value=z,\n                filters=W,\n                stride=1,\n                padding='SAME')\n            bias = tf.nn.bias_add(conv, b)\n            c = tf.nn.relu(bias)\n\n            d = tf.nn.dropout(c, 0.75)\n            # Add BatchNormalization or LocalResponseNormalization\n            e = tf.expand_dims(d, 1)\n\n            z = tf.nn.local_response_normalization(\n                e,\n                depth_radius=5,\n                bias=1,\n                alpha=0.001,\n                beta=0.75\n            )\n            z = tf.squeeze(z, 1)\n        # endfor\n        return z\n\n    def _create_maxpooling_layer(self, convolution):\n        conv_size = convolution.shape[1].value\n        embedding_size = convolution.shape[2].value\n\n        convolution = tf.expand_dims(convolution, -1)\n        pooled = tf.nn.max_pool(\n            value=convolution,\n            ksize=[1, conv_size, 1, 1],\n            strides=[1, 1, 1, 1],\n            padding='VALID')\n\n        flat = tf.reshape(pooled, [-1, embedding_size])\n        return flat", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_vector = embedding.astype('float32')\nmodel = ShinContextualModel(embedding=word_vector)\n\nclassifier = Classifier(\n    model=model,\n    input_length=seq_length,\n    output_length=num_classes)\n\nclassifier.compile(batch_size=32)\nclassifier.summary()", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "input: (?, 56)\nembedding: (?, 56, 100)\nconv: (?, 56, 100)\npool: (?, 100)\noutput: (?, 5)\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_10_fold(trainData, categories)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "11748 56 56\nFold:  1\nEpoch: 1, Train Loss:  1.617, Train Acc: 28.62%, Val Loss:  1.493, Val Acc: 31.40%, Time: 31.01s *\nEpoch: 2, Train Loss:  1.412, Train Acc: 37.25%, Val Loss:  1.389, Val Acc: 41.19%, Time: 30.91s *\nEpoch: 3, Train Loss:  1.265, Train Acc: 44.52%, Val Loss:  1.333, Val Acc: 39.83%, Time: 31.04s \nEpoch: 4, Train Loss:  1.134, Train Acc: 50.99%, Val Loss:  1.305, Val Acc: 42.64%, Time: 30.96s *\nEpoch: 5, Train Loss:  1.011, Train Acc: 57.10%, Val Loss:  1.342, Val Acc: 44.09%, Time: 31.04s *\nEpoch: 6, Train Loss: 0.8881, Train Acc: 62.81%, Val Loss:  1.457, Val Acc: 43.15%, Time: 30.91s \nEpoch: 7, Train Loss: 0.7642, Train Acc: 69.23%, Val Loss:  1.563, Val Acc: 41.87%, Time: 31.02s \nEpoch: 8, Train Loss: 0.6526, Train Acc: 74.08%, Val Loss:  1.767, Val Acc: 41.87%, Time: 30.93s \nEpoch: 9, Train Loss: 0.5506, Train Acc: 78.44%, Val Loss:  2.009, Val Acc: 40.09%, Time: 30.94s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.39      0.22      0.29       138\n     negative       0.42      0.63      0.50       308\n      neutral       0.28      0.16      0.20       220\n     positive       0.46      0.48      0.47       330\nvery positive       0.46      0.42      0.44       179\n\n    micro avg       0.42      0.42      0.42      1175\n    macro avg       0.40      0.38      0.38      1175\n weighted avg       0.41      0.42      0.40      1175\n\nConfusion Matrix...\n[[ 31  87  11   9   0]\n [ 29 194  33  44   8]\n [ 14  98  35  56  17]\n [  3  72  36 157  62]\n [  2  16  10  76  75]]\nFold:  2\nEpoch: 1, Train Loss:  1.604, Train Acc: 28.07%, Val Loss:  1.455, Val Acc: 35.15%, Time: 31.00s *\nEpoch: 2, Train Loss:  1.374, Train Acc: 39.37%, Val Loss:  1.396, Val Acc: 38.04%, Time: 30.94s *\nEpoch: 3, Train Loss:  1.235, Train Acc: 46.07%, Val Loss:  1.374, Val Acc: 40.60%, Time: 30.94s *\nEpoch: 4, Train Loss:  1.111, Train Acc: 52.16%, Val Loss:  1.436, Val Acc: 41.02%, Time: 30.92s *\nEpoch: 5, Train Loss: 0.9932, Train Acc: 57.79%, Val Loss:  1.448, Val Acc: 40.17%, Time: 30.93s \nEpoch: 6, Train Loss: 0.8707, Train Acc: 63.74%, Val Loss:  1.586, Val Acc: 41.02%, Time: 30.96s \nEpoch: 7, Train Loss: 0.7514, Train Acc: 69.55%, Val Loss:  1.769, Val Acc: 42.38%, Time: 30.96s *\nEpoch: 8, Train Loss: 0.6475, Train Acc: 73.80%, Val Loss:  2.078, Val Acc: 39.40%, Time: 30.97s \nEpoch: 9, Train Loss: 0.5482, Train Acc: 78.47%, Val Loss:  2.373, Val Acc: 39.91%, Time: 30.95s \nEpoch: 10, Train Loss:  0.473, Train Acc: 82.19%, Val Loss:  2.455, Val Acc: 39.06%, Time: 31.00s \nEpoch: 11, Train Loss: 0.3892, Train Acc: 85.19%, Val Loss:  2.868, Val Acc: 37.79%, Time: 30.92s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.41      0.25      0.32       153\n     negative       0.42      0.40      0.41       305\n      neutral       0.24      0.21      0.22       232\n     positive       0.40      0.44      0.42       310\nvery positive       0.39      0.57      0.46       175\n\n    micro avg       0.38      0.38      0.38      1175\n    macro avg       0.37      0.37      0.37      1175\n weighted avg       0.37      0.38      0.37      1175\n\nConfusion Matrix...\n[[ 39  51  32  21  10]\n [ 32 121  72  59  21]\n [ 15  71  48  76  22]\n [  6  33  33 137 101]\n [  2  10  11  53  99]]\nFold:  3\nEpoch: 1, Train Loss:  1.624, Train Acc: 27.99%, Val Loss:  1.477, Val Acc: 35.91%, Time: 31.14s *\nEpoch: 2, Train Loss:  1.412, Train Acc: 37.70%, Val Loss:  1.374, Val Acc: 39.91%, Time: 31.03s *\nEpoch: 3, Train Loss:  1.265, Train Acc: 44.46%, Val Loss:  1.323, Val Acc: 42.64%, Time: 31.17s *\nEpoch: 4, Train Loss:  1.132, Train Acc: 50.73%, Val Loss:  1.346, Val Acc: 43.06%, Time: 31.11s *\nEpoch: 5, Train Loss:  1.007, Train Acc: 56.90%, Val Loss:  1.386, Val Acc: 42.72%, Time: 31.10s \nEpoch: 6, Train Loss: 0.8746, Train Acc: 63.66%, Val Loss:  1.481, Val Acc: 43.49%, Time: 31.19s *\nEpoch: 7, Train Loss: 0.7572, Train Acc: 69.34%, Val Loss:   1.64, Val Acc: 42.04%, Time: 31.22s \nEpoch: 8, Train Loss: 0.6408, Train Acc: 74.84%, Val Loss:  1.795, Val Acc: 42.30%, Time: 31.27s \nEpoch: 9, Train Loss: 0.5373, Train Acc: 79.15%, Val Loss:  2.043, Val Acc: 44.09%, Time: 31.13s *\nEpoch: 10, Train Loss: 0.4389, Train Acc: 82.82%, Val Loss:  2.236, Val Acc: 42.55%, Time: 31.12s \nEpoch: 11, Train Loss: 0.3479, Train Acc: 87.16%, Val Loss:  2.474, Val Acc: 41.62%, Time: 31.23s \nEpoch: 12, Train Loss: 0.2928, Train Acc: 89.09%, Val Loss:  2.847, Val Acc: 40.51%, Time: 31.32s \nEpoch: 13, Train Loss: 0.2452, Train Acc: 90.77%, Val Loss:  3.151, Val Acc: 39.06%, Time: 31.26s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.25      0.32      0.28       120\n     negative       0.43      0.46      0.44       324\n      neutral       0.32      0.18      0.23       237\n     positive       0.41      0.45      0.43       310\nvery positive       0.44      0.49      0.47       184\n\n    micro avg       0.39      0.39      0.39      1175\n    macro avg       0.37      0.38      0.37      1175\n weighted avg       0.39      0.39      0.38      1175\n\nConfusion Matrix...\n[[ 38  50  16  11   5]\n [ 66 149  34  61  14]\n [ 30  81  42  61  23]\n [ 14  52  30 140  74]\n [  5  15   8  65  91]]\nFold:  4\nEpoch: 1, Train Loss:  1.623, Train Acc: 28.09%, Val Loss:   1.52, Val Acc: 33.62%, Time: 31.39s *\nEpoch: 2, Train Loss:  1.403, Train Acc: 37.64%, Val Loss:  1.437, Val Acc: 39.66%, Time: 31.36s *\nEpoch: 3, Train Loss:  1.257, Train Acc: 44.46%, Val Loss:  1.383, Val Acc: 39.15%, Time: 31.34s \nEpoch: 4, Train Loss:  1.132, Train Acc: 50.50%, Val Loss:  1.381, Val Acc: 40.34%, Time: 31.38s *\nEpoch: 5, Train Loss:  1.004, Train Acc: 57.18%, Val Loss:  1.399, Val Acc: 42.98%, Time: 31.52s *\nEpoch: 6, Train Loss: 0.8836, Train Acc: 62.93%, Val Loss:  1.482, Val Acc: 39.57%, Time: 31.42s \nEpoch: 7, Train Loss: 0.7612, Train Acc: 69.44%, Val Loss:  1.643, Val Acc: 40.26%, Time: 31.40s \nEpoch: 8, Train Loss: 0.6508, Train Acc: 74.48%, Val Loss:  1.827, Val Acc: 40.94%, Time: 31.35s \nEpoch: 9, Train Loss: 0.5337, Train Acc: 79.40%, Val Loss:   1.99, Val Acc: 39.83%, Time: 31.35s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.38      0.23      0.29       140\n     negative       0.44      0.54      0.48       333\n      neutral       0.30      0.27      0.28       221\n     positive       0.41      0.46      0.43       291\nvery positive       0.51      0.41      0.45       190\n\n    micro avg       0.41      0.41      0.41      1175\n    macro avg       0.41      0.38      0.39      1175\n weighted avg       0.41      0.41      0.40      1175\n\nConfusion Matrix...\n[[ 32  75  20  11   2]\n [ 34 180  58  55   6]\n [  9  94  59  48  11]\n [  8  47  46 134  56]\n [  1  14  16  82  77]]\nFold:  5\nEpoch: 1, Train Loss:   1.59, Train Acc: 28.60%, Val Loss:  1.482, Val Acc: 33.70%, Time: 31.14s *\nEpoch: 2, Train Loss:  1.392, Train Acc: 38.07%, Val Loss:   1.36, Val Acc: 39.66%, Time: 31.07s *\nEpoch: 3, Train Loss:  1.252, Train Acc: 44.56%, Val Loss:  1.328, Val Acc: 42.13%, Time: 31.14s *\nEpoch: 4, Train Loss:  1.118, Train Acc: 50.90%, Val Loss:   1.32, Val Acc: 44.43%, Time: 31.27s *\nEpoch: 5, Train Loss: 0.9927, Train Acc: 57.34%, Val Loss:  1.389, Val Acc: 41.19%, Time: 31.10s \nEpoch: 6, Train Loss: 0.8765, Train Acc: 63.68%, Val Loss:  1.503, Val Acc: 41.11%, Time: 31.10s \nEpoch: 7, Train Loss: 0.7613, Train Acc: 69.53%, Val Loss:  1.626, Val Acc: 42.04%, Time: 31.14s \nEpoch: 8, Train Loss: 0.6561, Train Acc: 73.62%, Val Loss:  1.846, Val Acc: 42.38%, Time: 31.19s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.35      0.29      0.32       149\n     negative       0.41      0.60      0.49       315\n      neutral       0.26      0.17      0.20       221\n     positive       0.42      0.45      0.43       307\nvery positive       0.54      0.33      0.41       183\n\n    micro avg       0.40      0.40      0.40      1175\n    macro avg       0.40      0.37      0.37      1175\n weighted avg       0.40      0.40      0.39      1175\n\nConfusion Matrix...\n[[ 43  86  10   9   1]\n [ 45 190  42  36   2]\n [ 15 104  37  59   6]\n [ 15  72  38 139  43]\n [  4  11  17  90  61]]\nFold:  6\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch: 1, Train Loss:  1.613, Train Acc: 28.50%, Val Loss:  1.501, Val Acc: 32.51%, Time: 30.91s *\nEpoch: 2, Train Loss:  1.405, Train Acc: 37.26%, Val Loss:  1.425, Val Acc: 36.51%, Time: 30.89s *\nEpoch: 3, Train Loss:  1.258, Train Acc: 45.36%, Val Loss:  1.366, Val Acc: 39.32%, Time: 30.78s *\nEpoch: 4, Train Loss:  1.134, Train Acc: 50.95%, Val Loss:  1.356, Val Acc: 41.53%, Time: 30.87s *\nEpoch: 5, Train Loss:  1.014, Train Acc: 56.56%, Val Loss:  1.397, Val Acc: 42.72%, Time: 30.90s *\nEpoch: 6, Train Loss: 0.8896, Train Acc: 62.75%, Val Loss:  1.546, Val Acc: 39.40%, Time: 30.86s \nEpoch: 7, Train Loss: 0.7777, Train Acc: 68.80%, Val Loss:   1.73, Val Acc: 37.53%, Time: 30.75s \nEpoch: 8, Train Loss: 0.6547, Train Acc: 74.46%, Val Loss:  1.951, Val Acc: 36.85%, Time: 30.89s \nEpoch: 9, Train Loss: 0.5581, Train Acc: 78.38%, Val Loss:  2.075, Val Acc: 39.15%, Time: 30.79s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.43      0.28      0.34       159\n     negative       0.39      0.53      0.45       298\n      neutral       0.28      0.20      0.23       233\n     positive       0.38      0.39      0.38       303\nvery positive       0.46      0.48      0.47       182\n\n    micro avg       0.39      0.39      0.39      1175\n    macro avg       0.39      0.38      0.38      1175\n weighted avg       0.38      0.39      0.38      1175\n\nConfusion Matrix...\n[[ 45  78  18  16   2]\n [ 33 157  39  50  19]\n [ 18  97  47  57  14]\n [  8  59  52 117  67]\n [  1  11  13  69  88]]\nFold:  7\nEpoch: 1, Train Loss:  1.599, Train Acc: 28.65%, Val Loss:  1.518, Val Acc: 32.85%, Time: 31.42s *\nEpoch: 2, Train Loss:    1.4, Train Acc: 38.22%, Val Loss:  1.391, Val Acc: 39.83%, Time: 31.32s *\nEpoch: 3, Train Loss:  1.255, Train Acc: 45.09%, Val Loss:  1.305, Val Acc: 43.66%, Time: 31.39s *\nEpoch: 4, Train Loss:  1.132, Train Acc: 51.06%, Val Loss:  1.321, Val Acc: 44.26%, Time: 31.35s *\nEpoch: 5, Train Loss:  1.013, Train Acc: 56.03%, Val Loss:  1.364, Val Acc: 41.70%, Time: 31.48s \nEpoch: 6, Train Loss: 0.8821, Train Acc: 62.99%, Val Loss:  1.465, Val Acc: 42.72%, Time: 31.36s \nEpoch: 7, Train Loss: 0.7535, Train Acc: 69.87%, Val Loss:  1.612, Val Acc: 44.17%, Time: 31.35s \nEpoch: 8, Train Loss: 0.6426, Train Acc: 75.08%, Val Loss:  1.912, Val Acc: 41.11%, Time: 31.32s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.40      0.37      0.38       164\n     negative       0.39      0.66      0.49       295\n      neutral       0.28      0.21      0.24       216\n     positive       0.45      0.39      0.42       306\nvery positive       0.60      0.32      0.42       194\n\n    micro avg       0.41      0.41      0.41      1175\n    macro avg       0.43      0.39      0.39      1175\n weighted avg       0.42      0.41      0.40      1175\n\nConfusion Matrix...\n[[ 61  86   8   8   1]\n [ 56 194  32  12   1]\n [ 18 103  45  44   6]\n [ 11  86  56 120  33]\n [  7  26  18  81  62]]\nFold:  8\nEpoch: 1, Train Loss:  1.611, Train Acc: 28.07%, Val Loss:   1.51, Val Acc: 33.02%, Time: 31.47s *\nEpoch: 2, Train Loss:  1.413, Train Acc: 36.96%, Val Loss:  1.404, Val Acc: 35.83%, Time: 31.45s *\nEpoch: 3, Train Loss:  1.265, Train Acc: 44.20%, Val Loss:  1.355, Val Acc: 39.66%, Time: 31.49s *\nEpoch: 4, Train Loss:  1.135, Train Acc: 50.61%, Val Loss:  1.365, Val Acc: 43.15%, Time: 31.46s *\nEpoch: 5, Train Loss:  1.013, Train Acc: 56.61%, Val Loss:  1.394, Val Acc: 42.47%, Time: 31.42s \nEpoch: 6, Train Loss: 0.8938, Train Acc: 62.60%, Val Loss:  1.486, Val Acc: 42.47%, Time: 31.42s \nEpoch: 7, Train Loss: 0.7816, Train Acc: 68.70%, Val Loss:  1.619, Val Acc: 40.00%, Time: 31.46s \nEpoch: 8, Train Loss: 0.6586, Train Acc: 74.07%, Val Loss:  1.822, Val Acc: 41.70%, Time: 31.46s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.30      0.37      0.33       147\n     negative       0.39      0.52      0.45       314\n      neutral       0.28      0.19      0.22       230\n     positive       0.42      0.42      0.42       306\nvery positive       0.52      0.35      0.42       178\n\n    micro avg       0.38      0.38      0.38      1175\n    macro avg       0.38      0.37      0.37      1175\n weighted avg       0.38      0.38      0.38      1175\n\nConfusion Matrix...\n[[ 55  68  11  12   1]\n [ 78 162  36  34   4]\n [ 32  95  43  50  10]\n [ 12  72  52 128  42]\n [  5  16  12  83  62]]\nFold:  9\nEpoch: 1, Train Loss:  1.602, Train Acc: 28.20%, Val Loss:  1.493, Val Acc: 34.07%, Time: 31.04s *\nEpoch: 2, Train Loss:  1.405, Train Acc: 37.83%, Val Loss:   1.37, Val Acc: 42.33%, Time: 30.95s *\nEpoch: 3, Train Loss:  1.259, Train Acc: 44.85%, Val Loss:  1.331, Val Acc: 43.02%, Time: 30.90s *\nEpoch: 4, Train Loss:  1.128, Train Acc: 50.65%, Val Loss:  1.301, Val Acc: 44.29%, Time: 30.88s *\nEpoch: 5, Train Loss:  1.007, Train Acc: 57.44%, Val Loss:  1.356, Val Acc: 43.95%, Time: 30.90s \nEpoch: 6, Train Loss: 0.8801, Train Acc: 63.07%, Val Loss:  1.455, Val Acc: 44.80%, Time: 30.96s *\nEpoch: 7, Train Loss: 0.7723, Train Acc: 68.74%, Val Loss:  1.562, Val Acc: 44.04%, Time: 31.00s \nEpoch: 8, Train Loss: 0.6456, Train Acc: 74.00%, Val Loss:  1.809, Val Acc: 43.61%, Time: 30.90s \nEpoch: 9, Train Loss: 0.5473, Train Acc: 78.61%, Val Loss:  2.045, Val Acc: 41.31%, Time: 31.00s \nEpoch: 10, Train Loss: 0.4676, Train Acc: 82.32%, Val Loss:   2.35, Val Acc: 42.25%, Time: 30.94s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.36      0.35      0.35       168\n     negative       0.43      0.60      0.50       321\n      neutral       0.31      0.18      0.23       186\n     positive       0.46      0.50      0.48       309\nvery positive       0.54      0.32      0.40       190\n\n    micro avg       0.43      0.43      0.43      1174\n    macro avg       0.42      0.39      0.39      1174\n weighted avg       0.42      0.43      0.41      1174\n\nConfusion Matrix...\n[[ 58  90   9  11   0]\n [ 65 193  23  34   6]\n [ 21  81  34  44   6]\n [ 15  70  30 154  40]\n [  4  17  14  94  61]]\nFold:  10\nEpoch: 1, Train Loss:   1.58, Train Acc: 28.34%, Val Loss:  1.478, Val Acc: 33.56%, Time: 31.23s *\nEpoch: 2, Train Loss:  1.379, Train Acc: 39.10%, Val Loss:  1.361, Val Acc: 39.10%, Time: 31.18s *\nEpoch: 3, Train Loss:   1.24, Train Acc: 45.51%, Val Loss:  1.318, Val Acc: 41.82%, Time: 31.23s *\nEpoch: 4, Train Loss:  1.116, Train Acc: 51.82%, Val Loss:  1.327, Val Acc: 41.06%, Time: 31.10s \nEpoch: 5, Train Loss: 0.9969, Train Acc: 57.40%, Val Loss:  1.414, Val Acc: 40.89%, Time: 31.09s \nEpoch: 6, Train Loss: 0.8701, Train Acc: 64.01%, Val Loss:    1.5, Val Acc: 39.69%, Time: 30.96s \nEpoch: 7, Train Loss: 0.7511, Train Acc: 69.99%, Val Loss:  1.655, Val Acc: 40.12%, Time: 31.00s \nNo optimization for a long time, auto-stopping...\nPrecision, Recall and F1-Score...\n               precision    recall  f1-score   support\n\nvery negative       0.48      0.20      0.28       148\n     negative       0.42      0.49      0.45       294\n      neutral       0.26      0.33      0.29       233\n     positive       0.44      0.45      0.45       319\nvery positive       0.54      0.44      0.48       180\n\n    micro avg       0.40      0.40      0.40      1174\n    macro avg       0.43      0.38      0.39      1174\n weighted avg       0.42      0.40      0.40      1174\n\nConfusion Matrix...\n[[ 29  69  30  13   7]\n [ 20 143  95  34   2]\n [  9  77  78  60   9]\n [  3  46  78 143  49]\n [  0   6  23  72  79]]\n[0.4187234042553192, 0.3778723404255319, 0.39148936170212767, 0.4102127659574468, 0.4, 0.38638297872340427, 0.4102127659574468, 0.3829787234042553, 0.42589437819420783, 0.4020442930153322]\n0.4005811011635072, 0.01505888316630104, 0.015873456607959816, 0.00022676996221630483\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "[0.4187234042553192,\n 0.3778723404255319,\n 0.39148936170212767,\n 0.4102127659574468,\n 0.4,\n 0.38638297872340427,\n 0.4102127659574468,\n 0.3829787234042553,\n 0.42589437819420783,\n 0.4020442930153322]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "raw", "source": "classifier.train(\n    X_train=X_train,\n    y_train=y_train,\n    X_eval=X_eval,\n    y_eval=y_eval,\n    epochs=30\n)"}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}