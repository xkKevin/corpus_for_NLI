{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_5.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model/cnn_tt\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    train_data = []\n    with open(filePath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n\n    return np.asarray(train_data)\n\n\ndef readCategory():\n    \"\"\"\u8bfb\u53d6\u5206\u7c7b\u76ee\u5f55\uff0c\u56fa\u5b9aid\"\"\"\n    '''\n    Retrieve Value\n    Filter\n    Compute Derived Value\n    Find Extremum\n    Sort\n    Determine Range\n    Characterize Distribution\n    Find Anomalies\n    Cluster\n    Correlate\n    '''\n    categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n    cat_to_id = dict(zip(categories, range(1,len(categories)+1)))\n    id_to_cat = dict(zip(range(1,len(categories)+1), categories))\n    return id_to_cat, cat_to_id\n\n\ndef loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data = readfile(trainDataPath)", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(train_data[0]),len(train_data)", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "(2, 14035)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.random.shuffle(train_data)\nlen(train_data[:,0])", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "14035"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data[0]", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "array([list(['please', 'tell', 'me', 'the', 'average', 'ashley', 'populace', 'in', '2000']),\n       3], dtype=object)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length = 0\nfor content in train_data[:,0]:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 41", "execution_count": 8, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "41"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocab, embd = loadGloVe(vocabPath, 100)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id = dict(zip(vocab, range(vocab_size)))", "execution_count": 11, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(embedding),embedding_dim,vocab_size", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "(400000, 100, 400000)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n\nnum_classes = len(categories)\n\nprint(\"Loading training and validation and testing data...\")\nstart_time = time.time()\nx_train, y_train = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\ntime_dif = get_time_dif(start_time)\nprint(\"Loading data Time usage:\", time_dif)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "Loading training and validation and testing data...\nLoading data Time usage: 0:00:01\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "x_train[0], y_train[0]", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "(array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,  3832,  1361,   285,     0,\n          641,  9358, 20658,     6,   665], dtype=int32),\n array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]))"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(x_train)", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "14035"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id[\"arkansas\"]", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "4791"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "embedding[3]", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "array([-0.1529  , -0.24279 ,  0.89837 ,  0.16996 ,  0.53516 ,  0.48784 ,\n       -0.58826 , -0.17982 , -1.3581  ,  0.42541 ,  0.15377 ,  0.24215 ,\n        0.13474 ,  0.41193 ,  0.67043 , -0.56418 ,  0.42985 , -0.012183,\n       -0.11677 ,  0.31781 ,  0.054177, -0.054273,  0.35516 , -0.30241 ,\n        0.31434 , -0.33846 ,  0.71715 , -0.26855 , -0.15837 , -0.47467 ,\n        0.051581, -0.33252 ,  0.15003 , -0.1299  , -0.54617 , -0.37843 ,\n        0.64261 ,  0.82187 , -0.080006,  0.078479, -0.96976 , -0.57741 ,\n        0.56491 , -0.39873 , -0.057099,  0.19743 ,  0.065706, -0.48092 ,\n       -0.20125 , -0.40834 ,  0.39456 , -0.02642 , -0.11838 ,  1.012   ,\n       -0.53171 , -2.7474  , -0.042981, -0.74849 ,  1.7574  ,  0.59085 ,\n        0.04885 ,  0.78267 ,  0.38497 ,  0.42097 ,  0.67882 ,  0.10337 ,\n        0.6328  , -0.026595,  0.58647 , -0.44332 ,  0.33057 , -0.12022 ,\n       -0.55645 ,  0.073611,  0.20915 ,  0.43395 , -0.012761,  0.089874,\n       -1.7991  ,  0.084808,  0.77112 ,  0.63105 , -0.90685 ,  0.60326 ,\n       -1.7515  ,  0.18596 , -0.50687 , -0.70203 ,  0.66578 , -0.81304 ,\n        0.18712 , -0.018488, -0.26757 ,  0.727   , -0.59363 , -0.34839 ,\n       -0.56094 , -0.591   ,  1.0039  ,  0.20664 ])"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class rnn_cnn(object):\n    \n    def __init__(self, savePath, num_classes):\n        self.savePath = savePath\n        # \u8f93\u5165\u5185\u5bb9\u53ca\u5bf9\u5e94\u7684\u6807\u7b7e\n        self.input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n        # dropout\u7684\u635f\u5931\u7387\n        self.keep_prob = tf.placeholder(tf.float64, name='keep_prob')\n        # \u8bcd\u5411\u91cf\u6620\u5c04;\u5b9e\u9645\u4e0a\u6b64\u5904\u7684\u8bcd\u5411\u91cf\u5e76\u4e0d\u662f\u7528\u7684\u9884\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u800c\u662f\u672a\u7ecf\u4efb\u4f55\u8bad\u7ec3\u76f4\u63a5\u751f\u6210\u4e86\u4e00\u4e2a\u77e9\u9635\uff0c\u5c06\u6b64\u77e9\u9635\u4f5c\u4e3a\u8bcd\u5411\u91cf\u77e9\u9635\u4f7f\u7528\uff0c\u6548\u679c\u4e5f\u8fd8\u4e0d\u9519\u3002\n        # \u82e5\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u6216\u8bb8\u8bad\u7ec3\u6b64\u6b21\u6587\u672c\u5206\u7c7b\u7684\u6a21\u578b\u65f6\u4f1a\u66f4\u5feb\uff0c\u66f4\u597d\u3002\n        # embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])\n        embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n\n        num_filters = 256\n        kernel_size = 5\n        hidden_dim = 128\n        learning_rate = 1e-3\n\n        # CNN layer\n        conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 \u8fd9\u662f\u4e2a\u5565\n        ''' https://blog.csdn.net/khy19940520/article/details/89934335\n        tf.layers.conv1d\uff1a\u4e00\u7ef4\u5377\u79ef\u4e00\u822c\u7528\u4e8e\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u5e38\u7528\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u8f93\u5165\u4e00\u822c\u662f\u6587\u672c\u7ecf\u8fc7embedding\u7684\u4e8c\u7ef4\u6570\u636e\u3002\n            inputs\uff1a \u8f93\u5165tensor\uff0c \u7ef4\u5ea6(batch_size, seq_length, embedding_dim) \u662f\u4e00\u4e2a\u4e09\u7ef4\u7684tensor\uff1b\u5176\u4e2d\uff0c\n                batch_size\u6307\u6bcf\u6b21\u8f93\u5165\u7684\u6587\u672c\u6570\u91cf\uff1b\n                seq_length\u6307\u6bcf\u4e2a\u6587\u672c\u7684\u8bcd\u8bed\u6570\u6216\u8005\u5355\u5b57\u6570\uff1b\n                embedding_dim\u6307\u6bcf\u4e2a\u8bcd\u8bed\u6216\u8005\u6bcf\u4e2a\u5b57\u7684\u5411\u91cf\u957f\u5ea6\uff1b\n                \u4f8b\u5982\u6bcf\u6b21\u8bad\u7ec3\u8f93\u51652\u7bc7\u6587\u672c\uff0c\u6bcf\u7bc7\u6587\u672c\u6709100\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u957f\u5ea6\u4e3a20\uff0c\u90a3input\u7ef4\u5ea6\u5373\u4e3a(2, 100, 20)\u3002\n            filters\uff1a\u8fc7\u6ee4\u5668\uff08\u5377\u79ef\u6838\uff09\u7684\u6570\u76ee\n            kernel_size\uff1a\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u5377\u79ef\u6838\u672c\u8eab\u5e94\u8be5\u662f\u4e8c\u7ef4\u7684\uff0c\u8fd9\u91cc\u53ea\u9700\u8981\u6307\u5b9a\u4e00\u7ef4\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5373\u957f\u5ea6\u4e0e\u8bcd\u5411\u91cf\u7684\u957f\u5ea6\u4e00\u81f4\uff0c\u5377\u79ef\u6838\u53ea\u80fd\u4ece\u4e0a\u5f80\u4e0b\u8d70\uff0c\u4e0d\u80fd\u4ece\u5de6\u5f80\u53f3\u8d70\uff0c\u5373\u53ea\u80fd\u6309\u7167\u6587\u672c\u4e2d\u8bcd\u7684\u987a\u5e8f\uff0c\u4e5f\u662f\u5217\u7684\u987a\u5e8f\u3002\n        '''\n        # global max pooling layer\n        gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n\n        # \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b\n        fc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim\uff1a128\n        ''' https://blog.csdn.net/yangfengling1023/article/details/81774580\n        dense \uff1a\u5168\u8fde\u63a5\u5c42  inputs\uff1a\u8f93\u5165\u8be5\u7f51\u7edc\u5c42\u7684\u6570\u636e\uff1bunits\uff1a\u8f93\u51fa\u7684\u7ef4\u5ea6\u5927\u5c0f\uff0c\u6539\u53d8inputs\u7684\u6700\u540e\u4e00\u7ef4\n        '''\n        fc = tf.nn.dropout(fc, self.keep_prob)\n        fc = tf.nn.relu(fc)\n\n        # \u5206\u7c7b\u5668\n        logits = tf.layers.dense(fc, num_classes, name='fc2')\n        self.y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n\n        # \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y)\n        self.loss = tf.reduce_mean(cross_entropy)\n        # \u4f18\u5316\u5668\n        self.optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        self.saver = tf.train.Saver()\n        \n    \n    def train(self, x_train, y_train, split_type, fold_id, num_epochs=20, dropout_keep_prob=0.5, print_per_batch=30, batch_size=64):\n        \n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        # \u521b\u5efasession\n        session = tf.Session()\n        session.run(tf.global_variables_initializer())\n        \n        print('Training and evaluating...')\n        start_time = time.time()\n        total_batch = 0  # \u603b\u6279\u6b21\n        best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n        last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n        require_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n        flag = False\n\n        for epoch in range(num_epochs):  # 20\n            print('Epoch:', epoch + 1)\n            batch_train = rnn_cnn.batch_iter(x_train, y_train, batch_size)\n            for x_batch, y_batch in batch_train:\n                feed_dict = {self.input_x: x_batch, self.input_y: y_batch, self.keep_prob: dropout_keep_prob}\n                session.run(self.optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n                total_batch += 1\n\n                if total_batch % print_per_batch == 0:\n                    # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                    feed_dict[self.keep_prob] = 1.0\n                    loss_train, acc_train = session.run([self.loss, self.acc], feed_dict=feed_dict)\n                    # loss_val, acc_val = evaluate(session, x_dev, y_dev, loss, acc)\n                    if acc_train > best_acc_train:\n                        # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                        best_acc_train = acc_train\n                        last_improved = total_batch\n                        self.saver.save(sess=session, save_path=savePath)\n                        improved_str = '*'\n                    else:\n                        improved_str = ''\n\n                    time_dif = get_time_dif(start_time)\n                    msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, Time: {3} {4}'\n                    print(msg.format(total_batch, loss_train, acc_train, time_dif, improved_str))\n\n                if total_batch - last_improved > require_improvement:\n                    # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                    print(\"No optimization for a long time, auto-stopping...\")\n                    flag = True\n                    break  # \u8df3\u51fa\u5faa\u73af\n            if flag:  # \u540c\u4e0a\n                break\n                \n        session.close()\n        return best_acc_train\n        \n        \n    def evaluate_model(self, x_test, y_test, split_type, fold_id, categories, batch_size=64):\n        \n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n        session = tf.Session()\n        self.saver.restore(sess=session, save_path=savePath)\n        start_time = time.time()\n        print('Testing...')\n        loss_test, acc_test = self.evaluate(session, x_test, y_test, self.loss, self.acc, batch_size)\n        msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n        print(msg.format(loss_test, acc_test))\n\n        test_data_len = len(x_test)\n        test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n        y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n        y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n        for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n            start_id = i * batch_size\n            end_id = min((i + 1) * batch_size, test_data_len)\n            feed_dict = {\n                self.input_x: x_test[start_id:end_id],\n                self.keep_prob: 1.0\n            }\n            y_test_pred_cls[start_id:end_id] = session.run(self.y_pred_cls, feed_dict=feed_dict)\n\n        # \u8bc4\u4f30\n        print(\"Precision, Recall and F1-Score...\")\n        print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n        '''\n        sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n            y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n            y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n            labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n            target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n            \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n        '''\n\n        # \u6df7\u6dc6\u77e9\u9635\n        print(\"Confusion Matrix...\")\n        cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n        '''\n        \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n        \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n        https://blog.csdn.net/u011734144/article/details/80277225\n        '''\n        print(cm)\n\n        time_dif = get_time_dif(start_time)\n        print(\"Time usage:\", time_dif)\n        session.close()\n        \n        return acc_test\n        \n        \n    def predict(self, predict_sentences, word_to_id, pad_max_length, split_type, fold_id):\n        \"\"\"\n        \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n        \"\"\"\n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        session = tf.Session()\n        self.saver.restore(sess=session, save_path=savePath)\n        \n        data_id = []\n        # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n        for i in range(len(predict_sentences)):\n            data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n\n        # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n        x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n        ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n        pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n            sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n            maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n            dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n            padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n            truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n            value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n        '''\n        feed_dict = {\n            self.input_x: x_pad,\n            self.keep_prob: 1.0\n        }\n        predict_result = session.run(self.y_pred_cls, feed_dict=feed_dict)\n        predict_result = [i+1 for i in predict_result]\n        session.close()\n        \n        return predict_result\n    \n    \n    def evaluate(self, sess, x_pad, y_pad, loss1, acc1, batch_size):\n        \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n        data_len = len(x_pad)\n        batch_eval = rnn_cnn.batch_iter(x_pad, y_pad, batch_size)  \n        total_loss = 0.0\n        total_acc = 0.0\n        # print(dropout_keep_prob)\n        for x_batch1, y_batch1 in batch_eval:\n            batch_len = len(x_batch1)\n            feed_dict1 = {self.input_x: x_batch1, self.input_y: y_batch1, self.keep_prob: 1.0}\n            lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n            total_loss += lossTmp * batch_len\n            total_acc += accTmp * batch_len\n\n        return total_loss / data_len, total_acc / data_len\n    \n    \n    def batch_iter(x_pad, y_pad, batch_size=64):  # 128\n        \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n        data_len = len(x_pad)\n        num_batch = int((data_len - 1) / batch_size) + 1\n        # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n        # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n        indices = np.random.permutation(np.arange(data_len))\n        x_shuffle = x_pad[indices]\n        y_shuffle = y_pad[indices]\n\n        # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n        for i in range(num_batch):\n            start_id = i * batch_size\n            end_id = min((i + 1) * batch_size, data_len)\n            yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n            \n            ", "execution_count": 19, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nfold_id = 0\nmodel_train_acc = []\nmodel_test_acc = []\nmodel = rnn_cnn(savePath, num_classes)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-19-10b7ccfe3726>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-19-10b7ccfe3726>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_type = \"random\"\nfor train_i, test_i in kf.split(x_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    model_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    model_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Fold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    2.1, Train Acc:  32.81%, Time: 0:00:08 *\nIter:     60, Train Loss:    1.8, Train Acc:  50.00%, Time: 0:00:14 *\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(model_test_acc),np.std(model_test_acc),np.std(model_test_acc,ddof=1),np.var(model_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def expert_split():\n    train_data = [[] for i in range(20)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0]\n            index = int(info.split(\":\")[4]) - 1\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(20):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)\n        \n    return np.asarray(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_expert = expert_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_expert[0][:2]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(train_data_expert)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(x_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    expert_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(expert_test_acc),np.std(expert_test_acc),np.std(expert_test_acc,ddof=1),np.var(expert_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "ex_train = []\ney_train = []\nfor ti in train_data_expert:\n    x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n    ex_train.append(x_train)\n    ey_train.append(y_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "ex_train = np.asarray(ex_train)\ney_train = np.asarray(ey_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(ex_train[0]),len(ex_train[1]),len(ex_train[0][0]),len(ex_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(ex_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    train_x, train_y = mergeData(ex_train[train_i],ey_train[train_i])\n    test_x, test_y = mergeData(ex_train[test_i],ey_train[test_i])\n    expert_train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(expert_test_acc),np.std(expert_test_acc),np.std(expert_test_acc,ddof=1),np.var(expert_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def bundle_split():\n    train_data = [[] for i in range(920)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0].split(\":\")\n            index = int(info[1]) - 1\n            label = int(info[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(920):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)   \n    \n    return train_data\n\ntrain_data_bundle = bundle_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_bundle[0][:5,1]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(train_data, split_type):\n    \n    print(split_type)\n    \n    tx = []\n    ty = []\n    for ti in train_data:\n        x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n        tx.append(x_train)\n        ty.append(y_train)\n\n    tx = np.asarray(tx)\n    ty = np.asarray(ty)\n    \n    print(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n    \n    train_acc = []\n    test_acc = []\n    fold_id = 0\n    \n    for train_i, test_i in kf.split(tx):\n        fold_id += 1\n        print(\"Fold: \", fold_id)\n        train_x, train_y = mergeData(tx[train_i],ty[train_i])\n        test_x, test_y = mergeData(tx[test_i],ty[test_i])\n        train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n        test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))\n        \n    print(test_acc)\n    print(np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc))\n    \n    return test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "bundle_test_acc = train_split_data(train_data_bundle, \"bundle\")", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def table_split():\n    train_data = [[] for i in range(37)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0].split(\":\")\n            index = int(info[3]) - 1\n            label = int(info[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(37):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)   \n    \n    return train_data\n\ntrain_data_table = table_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "table_test_acc = train_split_data(train_data_table, \"table\")", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(ex_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    train_x, train_y = mergeData(ex_train[train_i],ey_train[train_i])\n    test_x, test_y = mergeData(ex_train[test_i],ey_train[test_i])\n    expert_train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))\n\n\nbundle_train_acc = []\nbundle_test_acc = []\nsplit_type = \"bundle\"\nfold_id = 0\nfor train_i, test_i in kf.split(bx_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    expert_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(model_test_acc),np.std(model_test_acc),np.std(model_test_acc,ddof=1),np.var(model_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  #  7\n                    \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n                    \"List center - backs by what their transfer _ fee was .\"]  # 5\npredict(predict_sentences, word_to_id, seq_length)"}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}