{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-text-classification1/data/train_5500.label.txt\"\ntestDataPath = \"s3://corpus-text-classification1/data/TREC_10.label.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model_test/model_test\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    contents, labels = [], []\n    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n        for line in f:\n            try:\n                word = line.strip().split()\n                label = word[0].split(\":\")[0]\n                content = word[1:]\n                \n                contents.append(content)\n                labels.append(label)\n            except:\n                pass\n    return contents, labels\n\n\ndef readCategory():\n    \"\"\"\u8bfb\u53d6\u5206\u7c7b\u76ee\u5f55\uff0c\u56fa\u5b9aid\"\"\"\n    categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n    cat_to_id = dict(zip(categories, range(len(categories))))\n    return categories, cat_to_id\n\n\ndef loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_test, labels_test = readfile(testDataPath)\n# contents_val, labels_val = contents_train[-452:], labels_train[-452:]\ncontents_train, labels_train = readfile(trainDataPath)", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "category_test = set(labels_test)\ncategory_train = set(labels_train)\nprint(len(labels_test),len(category_test))\nprint(len(labels_train),len(category_train))\ncategory_test,category_train", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "500 6\n5452 6\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "({'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'},\n {'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'})"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath, 100)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\ncat_to_id = {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n\nlen(embedding),embedding_dim,vocab_size", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "(400000, 100, 400000)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "raw", "source": "# \u5982\u679c\u4e0d\u5b58\u5728\u8bcd\u6c47\u8868\uff0c\u5219\u91cd\u5efa\nif not os.path.exists(vocabPath):\n    print('======build vocab=======')\n    buildVocab(contents_train, contents_test, vocabPath)\ncategories, cat_to_id = readCategory()  # cat_to_id {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\nwords, word_to_id = read_vocab(vocabPath)\nvocab_size = len(words)\nnum_classes = len(categories)"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_all = contents_train + contents_test\nseq_length = 0\nfor content in contents_all:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 37", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "seq_length += 1  # seq_length = 38"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "cat_to_id", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "{'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, cat_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"Loading training and validation and testing data...\")\nstart_time = time.time()\nx_train, y_train = process_file(contents_train, labels_train, word_to_id, cat_to_id, seq_length)  # seq_length = 600\n# x_val, y_val = process_file(contents_val, labels_val, word_to_id, cat_to_id, seq_length)\nx_test, y_test = process_file(contents_test, labels_test, word_to_id, cat_to_id, seq_length)\ntime_dif = get_time_dif(start_time)\nprint(\"Loading data Time usage:\", time_dif)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Loading training and validation and testing data...\nLoading data Time usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length,num_classes", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "(37, 6)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "num_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 20\nbatch_size = 64\nprint_per_batch = 100  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\nsave_per_batch = 10  # \u6bcf\u591a\u5c11\u8f6e\u5b58\u5165tensorboard", "execution_count": 12, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# ======================================================CNN Model Start===============================================\n# \u8f93\u5165\u5185\u5bb9\u53ca\u5bf9\u5e94\u7684\u6807\u7b7e\ninput_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\ninput_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n# dropout\u7684\u635f\u5931\u7387\nkeep_prob = tf.placeholder(tf.float64, name='keep_prob')", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u8bcd\u5411\u91cf\u6620\u5c04;\u5b9e\u9645\u4e0a\u6b64\u5904\u7684\u8bcd\u5411\u91cf\u5e76\u4e0d\u662f\u7528\u7684\u9884\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u800c\u662f\u672a\u7ecf\u4efb\u4f55\u8bad\u7ec3\u76f4\u63a5\u751f\u6210\u4e86\u4e00\u4e2a\u77e9\u9635\uff0c\u5c06\u6b64\u77e9\u9635\u4f5c\u4e3a\u8bcd\u5411\u91cf\u77e9\u9635\u4f7f\u7528\uff0c\u6548\u679c\u4e5f\u8fd8\u4e0d\u9519\u3002\n# \u82e5\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u6216\u8bb8\u8bad\u7ec3\u6b64\u6b21\u6587\u672c\u5206\u7c7b\u7684\u6a21\u578b\u65f6\u4f1a\u66f4\u5feb\uff0c\u66f4\u597d\u3002\n# embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])  # embedding_dim 300\nembedding = tf.Variable(initial_value=embedding)\nembedding_inputs = tf.nn.embedding_lookup(embedding, input_x)", "execution_count": 14, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# CNN layer\nconv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 \u8fd9\u662f\u4e2a\u5565\n''' https://blog.csdn.net/khy19940520/article/details/89934335\ntf.layers.conv1d\uff1a\u4e00\u7ef4\u5377\u79ef\u4e00\u822c\u7528\u4e8e\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u5e38\u7528\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u8f93\u5165\u4e00\u822c\u662f\u6587\u672c\u7ecf\u8fc7embedding\u7684\u4e8c\u7ef4\u6570\u636e\u3002\n    inputs\uff1a \u8f93\u5165tensor\uff0c \u7ef4\u5ea6(batch_size, seq_length, embedding_dim) \u662f\u4e00\u4e2a\u4e09\u7ef4\u7684tensor\uff1b\u5176\u4e2d\uff0c\n        batch_size\u6307\u6bcf\u6b21\u8f93\u5165\u7684\u6587\u672c\u6570\u91cf\uff1b\n        seq_length\u6307\u6bcf\u4e2a\u6587\u672c\u7684\u8bcd\u8bed\u6570\u6216\u8005\u5355\u5b57\u6570\uff1b\n        embedding_dim\u6307\u6bcf\u4e2a\u8bcd\u8bed\u6216\u8005\u6bcf\u4e2a\u5b57\u7684\u5411\u91cf\u957f\u5ea6\uff1b\n        \u4f8b\u5982\u6bcf\u6b21\u8bad\u7ec3\u8f93\u51652\u7bc7\u6587\u672c\uff0c\u6bcf\u7bc7\u6587\u672c\u6709100\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u957f\u5ea6\u4e3a20\uff0c\u90a3input\u7ef4\u5ea6\u5373\u4e3a(2, 100, 20)\u3002\n    filters\uff1a\u8fc7\u6ee4\u5668\uff08\u5377\u79ef\u6838\uff09\u7684\u6570\u76ee\n    kernel_size\uff1a\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u5377\u79ef\u6838\u672c\u8eab\u5e94\u8be5\u662f\u4e8c\u7ef4\u7684\uff0c\u8fd9\u91cc\u53ea\u9700\u8981\u6307\u5b9a\u4e00\u7ef4\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5373\u957f\u5ea6\u4e0e\u8bcd\u5411\u91cf\u7684\u957f\u5ea6\u4e00\u81f4\uff0c\u5377\u79ef\u6838\u53ea\u80fd\u4ece\u4e0a\u5f80\u4e0b\u8d70\uff0c\u4e0d\u80fd\u4ece\u5de6\u5f80\u53f3\u8d70\uff0c\u5373\u53ea\u80fd\u6309\u7167\u6587\u672c\u4e2d\u8bcd\u7684\u987a\u5e8f\uff0c\u4e5f\u662f\u5217\u7684\u987a\u5e8f\u3002\n'''\n# global max pooling layer\ngmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n\n# \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b\nfc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim\uff1a128\n''' https://blog.csdn.net/yangfengling1023/article/details/81774580\ndense \uff1a\u5168\u8fde\u63a5\u5c42  inputs\uff1a\u8f93\u5165\u8be5\u7f51\u7edc\u5c42\u7684\u6570\u636e\uff1bunits\uff1a\u8f93\u51fa\u7684\u7ef4\u5ea6\u5927\u5c0f\uff0c\u6539\u53d8inputs\u7684\u6700\u540e\u4e00\u7ef4\n'''\nfc = tf.nn.dropout(fc, keep_prob)\nfc = tf.nn.relu(fc)\n\n# \u5206\u7c7b\u5668\nlogits = tf.layers.dense(fc, num_classes, name='fc2')\ny_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n\n# \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\nloss = tf.reduce_mean(cross_entropy)\n# \u4f18\u5316\u5668\noptim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n# \u51c6\u786e\u7387\ncorrect_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\nacc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n# ======================================================CNN Model End============================================", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-15-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-15-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, x_pad, y_pad, loss1, acc1):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    # print(dropout_keep_prob)\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {input_x: x_batch1, input_y: y_batch1, keep_prob: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len", "execution_count": 16, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u521b\u5efasession\nsession = tf.Session()\nsaver = tf.train.Saver()\nsession.run(tf.global_variables_initializer())", "execution_count": 17, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# ======================================================Train Start===============================================\n# \u8bad\u7ec3\u6a21\u578b\u7684\u4ee3\u7801\uff0c\u5982\u679c\u8981\u91cd\u65b0\u8bad\u7ec3\u5219\u6253\u5f00\u6ce8\u91ca\u5373\u53ef\u3002\u56e0\u4e3a\u540e\u9762\u8c03\u7528\u4e86\u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u6240\u4ee5\u6b64\u5904\u5148\u6ce8\u91ca\u6389\u3002\nprint('Training and evaluating...')\nstart_time = time.time()\ntotal_batch = 0  # \u603b\u6279\u6b21\nbest_acc_val = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\nlast_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\nrequire_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\nflag = False\n\nfor epoch in range(num_epochs):  # 20\n    print('Epoch:', epoch + 1)\n    batch_train = batch_iter(x_train, y_train, batch_size)\n    for x_batch, y_batch in batch_train:\n        feed_dict = {input_x: x_batch, input_y: y_batch, keep_prob: dropout_keep_prob}\n        session.run(optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n        total_batch += 1\n\n        if total_batch % print_per_batch == 0:\n            # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n            feed_dict[keep_prob] = 1.0\n            loss_train, acc_train = session.run([loss, acc], feed_dict=feed_dict)\n            # loss_val, acc_val = evaluate(session, x_val, y_val, loss, acc)\n            if acc_train > best_acc_val:\n                # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                best_acc_val = acc_train\n                last_improved = total_batch\n                saver.save(sess=session, save_path=savePath)\n                improved_str = '*'\n            else:\n                improved_str = ''\n\n            time_dif = get_time_dif(start_time)\n            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, Time: {3} {4}'\n            print(msg.format(total_batch, loss_train, acc_train, time_dif, improved_str))\n\n        if total_batch - last_improved > require_improvement:\n            # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n            print(\"No optimization for a long time, auto-stopping...\")\n            flag = True\n            break  # \u8df3\u51fa\u5faa\u73af\n    if flag:  # \u540c\u4e0a\n        break\n# ======================================================Train End===============================================", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nEpoch: 2\nIter:    100, Train Loss:  0.066, Train Acc: 100.00%, Time: 0:00:43 *\nEpoch: 3\nIter:    200, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:01:02 \nEpoch: 4\nIter:    300, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:20 \nEpoch: 5\nIter:    400, Train Loss:  0.064, Train Acc:  96.88%, Time: 0:01:39 \nEpoch: 6\nIter:    500, Train Loss:  0.034, Train Acc:  96.88%, Time: 0:01:58 \nEpoch: 7\nIter:    600, Train Loss:  0.084, Train Acc:  96.88%, Time: 0:02:16 \nNo optimization for a long time, auto-stopping...\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def evaluate_model():\n    # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n    saver.restore(sess=session, save_path=savePath)\n    start_time = time.time()\n    print('Testing...')\n    loss_test, acc_test = evaluate(session, x_test, y_test, loss, acc)\n    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n    print(msg.format(loss_test, acc_test))\n\n    test_data_len = len(x_test)\n    test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, test_data_len)\n        feed_dict = {\n            input_x: x_test[start_id:end_id],\n            keep_prob: 1.0\n        }\n        y_test_pred_cls[start_id:end_id] = session.run(y_pred_cls, feed_dict=feed_dict)\n\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n    '''\n    \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n    \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n    https://blog.csdn.net/u011734144/article/details/80277225\n    '''\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "evaluate_model()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model_test/model_test\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model_test/model_test\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.54, Test Acc:  82.00%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.78      0.88         9\n        DESC       0.72      0.96      0.82       138\n        ENTY       0.89      0.72      0.80        94\n         HUM       0.78      0.89      0.83        65\n         LOC       0.88      0.63      0.73        81\n         NUM       0.93      0.82      0.87       113\n\n   micro avg       0.82      0.82      0.82       500\n   macro avg       0.87      0.80      0.82       500\nweighted avg       0.84      0.82      0.82       500\n\nConfusion Matrix...\n[[  7   2   0   0   0   0]\n [  0 133   4   0   0   1]\n [  0  13  68   7   3   3]\n [  0   6   0  58   1   0]\n [  0  21   1   5  51   3]\n [  0  10   3   4   3  93]]\nTime usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "session.run(embedding[40])", "execution_count": 27, "outputs": [{"output_type": "execute_result", "execution_count": 27, "data": {"text/plain": "array([ 0.6882867 , -0.17556788, -0.10129495, -0.28756248, -0.16070651,\n       -0.09726952,  0.53536057,  0.3911603 , -0.06500677,  0.18553473,\n        0.36868343,  0.49436227,  0.66227881,  0.36517585,  0.32528194,\n       -0.44319155,  0.33491747,  0.01811357, -0.9882898 , -0.23288624,\n        0.62235553, -0.00298484,  0.3369283 ,  0.12579099,  0.03790958,\n       -0.71938964, -0.23830954, -0.31998238, -0.14831669, -0.29507794,\n        0.80162897,  0.91515708, -0.08995922,  0.76881952,  0.12605144,\n        0.0922839 ,  0.10565054,  0.92960896, -0.13068716,  0.45848915,\n       -0.9637406 , -0.2292045 ,  0.82865689, -0.44755706,  0.40714046,\n        0.04993685,  0.18303235, -0.70209989,  0.94685774, -0.97316061,\n       -0.24201143,  0.04436725,  0.33168641,  1.35185808, -0.21625318,\n       -2.47651052, -0.48717237, -0.3140198 ,  0.83374926,  1.59214919,\n        0.35837211,  0.70655816, -0.51495416,  0.4982509 ,  0.65462978,\n       -0.45505691,  0.56443133,  0.2863574 , -0.1489293 ,  0.18179406,\n       -0.08846705, -0.23763017,  0.07358103, -0.47418727, -0.23370684,\n        0.13139249, -0.26076125,  0.29652634, -1.55298187,  0.35050861,\n        0.63968962,  0.0934197 , -0.39517463,  0.14905281, -1.98346082,\n       -0.84407785,  0.52682384, -0.3010951 , -0.73371624, -0.31383388,\n       -0.06879784, -0.60579766, -0.1815146 ,  0.12261865, -0.86108127,\n        0.23139905, -0.52209243, -0.32494812,  0.21293391, -0.02087989])"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(predict_sentences, word_to_id, cat_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    \n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(predict_sentences)):\n        data_id.append([word_to_id[x] for x in predict_sentences[i].strip().split() if x in word_to_id])\n        \n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    feed_dict = {\n        input_x: x_pad,\n        keep_prob: 1.0\n    }\n    predict_result = session.run(y_pred_cls, feed_dict=feed_dict)\n    return predict_result", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "cat_to_id", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "'''\nLOC:mount Where are the Rocky Mountains ?\nDESC:def What are invertebrates ?\nNUM:temp What is the temperature at the center of the earth ?\n'''\npredict([\"Where are the Rocky Mountains ?\",\"What are invertebrates ?\",\"What is the temperature at the center of the earth ?\"],word_to_id, cat_to_id, seq_length)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}