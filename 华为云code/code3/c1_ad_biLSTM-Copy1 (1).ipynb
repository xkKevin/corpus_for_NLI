{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-2/dataset/corpus_5.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\nsavePath = \"s3://corpus-2/model/ad_biLSTM\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\nsplit_info = {\n    \"random\": False,\n    \"expert\": [20, 4],\n    \"bundle\": [920, 1],\n    \"table\": [37, 3]\n}\n\n\ndef dataset_split(info):\n    if info:\n        [num, pi] = info\n        train_data = [[] for i in range(num)]\n        with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n            for line in fp.readlines():\n                word = line.split()\n                info = word[0].split(\":\")\n                index = int(info[pi]) - 1\n                label = int(info[0])\n                content = word[1:]\n                train_data[index].append([content,label])\n\n        for i in range(num):\n            np.random.shuffle(train_data[i])\n            train_data[i] = np.asarray(train_data[i])\n\n        np.random.shuffle(train_data)   \n        return train_data\n    \n    \n    train_data = []\n    with open(trainDataPath, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            word = line.split()\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data.append([content,label])\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data)\n\n\ndef mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(model, train_data, split_type):\n    \n    print(split_type)\n    \n    train_acc = []\n    test_acc = []\n    fold_id = 0\n    \n    if split_type != \"random\":\n        tx = []\n        ty = []\n        for ti in train_data:\n            x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n            tx.append(x_train)\n            ty.append(y_train)\n\n        tx = np.asarray(tx)\n        ty = np.asarray(ty)\n\n        print(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n        \n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_x, train_y = mergeData(tx[train_i],ty[train_i])\n            test_x, test_y = mergeData(tx[test_i],ty[test_i])\n            train_acc.append(model_train(model,train_x, train_y,split_type,fold_id))\n            test_acc.append(model_evaluate(model,test_x, test_y,split_type,fold_id,categories))\n        \n    else:\n        tx, ty = process_file(train_data[:,0], train_data[:,1], word_to_id, num_classes, seq_length)\n        print(len(tx),len(tx[0]),len(tx[1]))\n\n        for train_i, test_i in kf.split(tx):\n            fold_id += 1\n            print(\"Fold: \", fold_id)\n            train_acc.append(model_train(model,tx[train_i], ty[train_i],split_type,fold_id))\n            test_acc.append(model_evaluate(model,tx[test_i], ty[test_i],split_type,fold_id,categories))\n        \n    print(test_acc)\n    print(\"%s, %s, %s, %s\" % (np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc)))\n    return test_acc\n\n\ndef batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, model, x_pad, y_pad, loss1, acc1, batch_size):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {model.inputX: x_batch1, model.inputY: y_batch1, model.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef model_train(model, x_train, y_train, split_type, fold_id):\n    \n    save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n\n    print('Training and evaluating...')\n    start_time = time.time()\n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n    flag = False\n\n    for epoch in range(num_epochs):  # 20\n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = {model.inputX: x_batch, model.inputY: y_batch, model.dropoutKeepProb: dropout_keep_prob}\n            session.run(model.trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch % print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.dropoutKeepProb] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                # loss_val, acc_val = evaluate(session, x_dev, y_dev, loss, acc)\n                if acc_train > best_acc_train:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_train = acc_train\n                    last_improved = total_batch\n                    saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n\n                time_dif = get_time_dif(start_time)\n                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, Time: {3} {4}'\n                print(msg.format(total_batch, loss_train, acc_train, time_dif, improved_str))\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n    session.close()\n    return best_acc_train\n\n\ndef model_evaluate(model, x_test, y_test, split_type, fold_id, categories, batch_size=64):\n        \n    save_path = \"%s/%s/%s/%s\" % (savePath, split_type, fold_id, fold_id)\n    # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n    session = tf.Session()\n    saver.restore(sess=session, save_path=save_path)\n    start_time = time.time()\n    print('Testing...')\n    loss_test, acc_test = evaluate(session, model, x_test, y_test, model.loss, model.acc, batch_size)\n    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n    print(msg.format(loss_test, acc_test))\n\n    test_data_len = len(x_test)\n    test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, test_data_len)\n        feed_dict = {\n            model.inputX: x_test[start_id:end_id],\n            model.dropoutKeepProb: 1.0\n        }\n        y_test_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n    '''\n    \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n    \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n    https://blog.csdn.net/u011734144/article/details/80277225\n    '''\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n    session.close()\n\n    return acc_test", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "contents_all = contents_train + contents_dev + contents_test\nseq_length = 0\nfor content in contents_train:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 41"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length = 41", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocab, embd = loadGloVe(vocabPath, 50)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id = dict(zip(vocab, range(vocab_size)))", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(embedding),embedding_dim,vocab_size", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "(400000, 100, 400000)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\nnum_classes = len(categories)", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        \n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        \n        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n        # \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        # \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\n        gradsAndVars = optimizer.compute_gradients(self.loss)\n        # \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\n        self.trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.inputY, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        # self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 11, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 50\nbatch_size = 64\nprint_per_batch = 30  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\n\nlstm = AdversarailLSTM(embedding)\nsaver = tf.train.Saver()", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-11-128725d8d774>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-11-128725d8d774>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-11-128725d8d774>:170: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-11-128725d8d774>:170: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-11-128725d8d774>:171: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-11-128725d8d774>:171: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n", "name": "stderr"}, {"output_type": "stream", "text": "perturbSize:Tensor(\"perturloss/Bi-LSTM/add_2:0\", shape=(?, 41, 100), dtype=float64)\noutputSize:128\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\ntest_acc_split = []\nfor split_type,info in split_info.items():\n    train_data = dataset_split(info)\n    test_acc_split.append(train_split_data(lstm, train_data, split_type))\n    ", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "random\n14035 41 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  18.75%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.1, Train Acc:  32.81%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.4, Train Acc:  50.00%, Time: 0:00:35 *\nIter:    120, Train Loss:    2.7, Train Acc:  57.81%, Time: 0:00:46 *\nIter:    150, Train Loss:    2.6, Train Acc:  57.81%, Time: 0:00:52 \nIter:    180, Train Loss:    2.0, Train Acc:  73.44%, Time: 0:01:03 *\nEpoch: 2\nIter:    210, Train Loss:    2.6, Train Acc:  57.81%, Time: 0:01:09 \nIter:    240, Train Loss:    2.2, Train Acc:  62.50%, Time: 0:01:15 \nIter:    270, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:01:25 *\nIter:    300, Train Loss:    2.4, Train Acc:  65.62%, Time: 0:01:31 \nIter:    330, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:01:37 \nIter:    360, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:01:43 \nIter:    390, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:01:49 \nEpoch: 3\nIter:    420, Train Loss:    1.8, Train Acc:  68.75%, Time: 0:01:55 \nIter:    450, Train Loss:   0.85, Train Acc:  85.94%, Time: 0:02:05 *\nIter:    480, Train Loss:    1.3, Train Acc:  76.56%, Time: 0:02:11 \nIter:    510, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:17 \nIter:    540, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:23 \nIter:    570, Train Loss:    1.0, Train Acc:  89.06%, Time: 0:02:33 *\nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:38 \nIter:    630, Train Loss:   0.93, Train Acc:  85.94%, Time: 0:02:44 \nIter:    660, Train Loss:   0.88, Train Acc:  84.38%, Time: 0:02:50 \nIter:    690, Train Loss:   0.62, Train Acc:  92.19%, Time: 0:03:01 *\nIter:    720, Train Loss:   0.94, Train Acc:  85.94%, Time: 0:03:07 \nIter:    750, Train Loss:   0.69, Train Acc:  92.19%, Time: 0:03:13 \nIter:    780, Train Loss:   0.84, Train Acc:  89.06%, Time: 0:03:19 \nEpoch: 5\nIter:    810, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:03:24 \nIter:    840, Train Loss:   0.93, Train Acc:  85.94%, Time: 0:03:30 \nIter:    870, Train Loss:   0.66, Train Acc:  87.50%, Time: 0:03:36 \nIter:    900, Train Loss:   0.97, Train Acc:  87.50%, Time: 0:03:42 \nIter:    930, Train Loss:    1.2, Train Acc:  76.56%, Time: 0:03:48 \nIter:    960, Train Loss:   0.87, Train Acc:  90.62%, Time: 0:03:54 \nIter:    990, Train Loss:   0.33, Train Acc:  91.30%, Time: 0:03:59 \nEpoch: 6\nIter:   1020, Train Loss:   0.73, Train Acc:  90.62%, Time: 0:04:05 \nIter:   1050, Train Loss:   0.49, Train Acc:  92.19%, Time: 0:04:11 \nIter:   1080, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:04:22 *\nIter:   1110, Train Loss:   0.56, Train Acc:  93.75%, Time: 0:04:28 \nIter:   1140, Train Loss:   0.57, Train Acc:  90.62%, Time: 0:04:34 \nIter:   1170, Train Loss:   0.44, Train Acc:  95.31%, Time: 0:04:39 \nEpoch: 7\nIter:   1200, Train Loss:   0.55, Train Acc:  92.19%, Time: 0:04:45 \nIter:   1230, Train Loss:   0.71, Train Acc:  92.19%, Time: 0:04:51 \nIter:   1260, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:04:57 \nIter:   1290, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:05:08 *\nIter:   1320, Train Loss:   0.61, Train Acc:  95.31%, Time: 0:05:15 \nIter:   1350, Train Loss:    0.2, Train Acc:  98.44%, Time: 0:05:20 \nIter:   1380, Train Loss:    0.6, Train Acc:  85.94%, Time: 0:05:26 \nEpoch: 8\nIter:   1410, Train Loss:   0.36, Train Acc:  96.88%, Time: 0:05:32 \nIter:   1440, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:05:38 \nIter:   1470, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:44 \nIter:   1500, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:05:49 \nIter:   1530, Train Loss:   0.35, Train Acc:  92.19%, Time: 0:05:55 \nIter:   1560, Train Loss:   0.45, Train Acc:  89.06%, Time: 0:06:01 \nEpoch: 9\nIter:   1590, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:06:07 \nIter:   1620, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:12 \nIter:   1650, Train Loss:    0.2, Train Acc:  96.88%, Time: 0:06:18 \nIter:   1680, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:06:24 \nIter:   1710, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:06:30 \nIter:   1740, Train Loss:    0.3, Train Acc:  98.44%, Time: 0:06:36 \nIter:   1770, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:06:42 \nEpoch: 10\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/1/1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/1/1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.72, Test Acc:  89.53%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.86      0.94      0.90       140\n                   Filter       0.77      0.90      0.83       138\n    Compute Derived Value       0.89      0.91      0.90       158\n            Find Extremum       0.90      0.82      0.86       162\n                     Sort       0.92      0.95      0.94       115\n          Determine Range       0.87      0.88      0.88       138\nCharacterize Distribution       0.97      0.90      0.93       140\n           Find Anomalies       0.92      0.80      0.86       128\n                  Cluster       0.98      0.91      0.94       134\n                Correlate       0.93      0.95      0.94       151\n\n                micro avg       0.90      0.90      0.90      1404\n                macro avg       0.90      0.90      0.90      1404\n             weighted avg       0.90      0.90      0.90      1404\n\nConfusion Matrix...\n[[132   1   4   1   0   1   1   0   0   0]\n [  4 124   4   1   0   3   0   2   0   0]\n [  2   5 143   1   0   3   0   3   0   1]\n [  9   9   2 133   5   2   0   1   0   1]\n [  0   2   1   2 109   0   0   0   0   1]\n [  1   3   3   4   2 122   1   0   0   2]\n [  1   2   2   2   1   4 126   1   1   0]\n [  1  10   0   2   0   4   1 103   2   5]\n [  2   5   0   0   1   1   1   1 122   1]\n [  2   1   2   2   0   0   0   1   0 143]]\nTime usage: 0:00:05\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  14.06%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.1, Train Acc:  26.56%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.3, Train Acc:  40.62%, Time: 0:00:38 *\nIter:    120, Train Loss:    2.9, Train Acc:  48.44%, Time: 0:00:49 *\nIter:    150, Train Loss:    2.6, Train Acc:  53.12%, Time: 0:01:01 *\nIter:    180, Train Loss:    2.5, Train Acc:  59.38%, Time: 0:01:12 *\nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  65.62%, Time: 0:01:23 *\nIter:    240, Train Loss:    2.0, Train Acc:  65.62%, Time: 0:01:28 \nIter:    270, Train Loss:    2.3, Train Acc:  68.75%, Time: 0:01:40 *\nIter:    300, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:01:51 *\nIter:    330, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:57 \nIter:    360, Train Loss:    1.5, Train Acc:  84.38%, Time: 0:02:09 *\nIter:    390, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:02:15 \nEpoch: 3\nIter:    420, Train Loss:    1.5, Train Acc:  75.00%, Time: 0:02:22 \nIter:    450, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:28 \nIter:    480, Train Loss:    1.7, Train Acc:  78.12%, Time: 0:02:34 \nIter:    510, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:40 \nIter:    540, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:46 \nIter:    570, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:53 \nEpoch: 4\nIter:    600, Train Loss:   0.94, Train Acc:  89.06%, Time: 0:03:04 *\nIter:    630, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:11 \nIter:    660, Train Loss:   0.95, Train Acc:  81.25%, Time: 0:03:17 \nIter:    690, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:03:23 \nIter:    720, Train Loss:    1.0, Train Acc:  90.62%, Time: 0:03:35 *\nIter:    750, Train Loss:    1.3, Train Acc:  76.56%, Time: 0:03:41 \nIter:    780, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:03:47 \nEpoch: 5\nIter:    810, Train Loss:   0.66, Train Acc:  89.06%, Time: 0:03:53 \nIter:    840, Train Loss:   0.63, Train Acc:  90.62%, Time: 0:03:59 \nIter:    870, Train Loss:   0.77, Train Acc:  82.81%, Time: 0:04:06 \nIter:    900, Train Loss:   0.49, Train Acc:  92.19%, Time: 0:04:16 *\nIter:    930, Train Loss:   0.85, Train Acc:  82.81%, Time: 0:04:23 \nIter:    960, Train Loss:    1.0, Train Acc:  92.19%, Time: 0:04:29 \nIter:    990, Train Loss:    1.1, Train Acc:  82.61%, Time: 0:04:36 \nEpoch: 6\nIter:   1020, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:04:48 *\nIter:   1050, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:04:55 \nIter:   1080, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:05:02 \nIter:   1110, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:05:14 *\nIter:   1140, Train Loss:   0.53, Train Acc:  90.62%, Time: 0:05:21 \nIter:   1170, Train Loss:   0.55, Train Acc:  96.88%, Time: 0:05:31 *\nEpoch: 7\nIter:   1200, Train Loss:   0.42, Train Acc:  96.88%, Time: 0:05:37 \nIter:   1230, Train Loss:   0.45, Train Acc:  95.31%, Time: 0:05:43 \nIter:   1260, Train Loss:   0.38, Train Acc:  96.88%, Time: 0:05:48 \nIter:   1290, Train Loss:   0.63, Train Acc:  90.62%, Time: 0:05:54 \nIter:   1320, Train Loss:   0.37, Train Acc:  96.88%, Time: 0:06:00 \nIter:   1350, Train Loss:   0.45, Train Acc:  95.31%, Time: 0:06:06 \nIter:   1380, Train Loss:   0.66, Train Acc:  92.19%, Time: 0:06:12 \nEpoch: 8\nIter:   1410, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:06:18 \nIter:   1440, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:30 *\nIter:   1470, Train Loss:   0.29, Train Acc:  98.44%, Time: 0:06:36 \nIter:   1500, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:06:41 \nIter:   1530, Train Loss:   0.52, Train Acc:  89.06%, Time: 0:06:47 \nIter:   1560, Train Loss:   0.55, Train Acc:  93.75%, Time: 0:06:53 \nEpoch: 9\nIter:   1590, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:06:59 \nIter:   1620, Train Loss:   0.16, Train Acc: 100.00%, Time: 0:07:09 *\nIter:   1650, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:07:15 \nIter:   1680, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:07:21 \nIter:   1710, Train Loss:   0.37, Train Acc:  95.31%, Time: 0:07:26 \nIter:   1740, Train Loss:   0.17, Train Acc:  96.88%, Time: 0:07:32 \nIter:   1770, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:07:38 \nEpoch: 10\nIter:   1800, Train Loss:   0.32, Train Acc:  98.44%, Time: 0:07:44 \nIter:   1830, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:07:50 \nIter:   1860, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:07:56 \nIter:   1890, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:08:01 \nIter:   1920, Train Loss:    0.3, Train Acc:  96.88%, Time: 0:08:07 \nIter:   1950, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:08:13 \nIter:   1980, Train Loss:   0.59, Train Acc:  95.65%, Time: 0:08:19 \nEpoch: 11\nIter:   2010, Train Loss:   0.24, Train Acc:  95.31%, Time: 0:08:25 \nIter:   2040, Train Loss:   0.15, Train Acc:  96.88%, Time: 0:08:31 \nIter:   2070, Train Loss:   0.17, Train Acc:  96.88%, Time: 0:08:36 \nIter:   2100, Train Loss:   0.12, Train Acc: 100.00%, Time: 0:08:42 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/2/2\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/2/2\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.59, Test Acc:  91.31%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.90      0.98      0.94       132\n                   Filter       0.89      0.93      0.91       139\n    Compute Derived Value       0.90      0.90      0.90       150\n            Find Extremum       0.96      0.92      0.94       171\n                     Sort       0.96      0.96      0.96       115\n          Determine Range       0.86      0.92      0.89       122\nCharacterize Distribution       0.91      0.87      0.89       131\n           Find Anomalies       0.90      0.84      0.87       157\n                  Cluster       0.90      0.95      0.92       145\n                Correlate       0.95      0.87      0.91       142\n\n                micro avg       0.91      0.91      0.91      1404\n                macro avg       0.91      0.91      0.91      1404\n             weighted avg       0.91      0.91      0.91      1404\n\nConfusion Matrix...\n[[130   0   0   1   0   1   0   0   0   0]\n [  3 129   2   0   0   3   1   1   0   0]\n [  3   4 135   1   0   2   2   2   0   1]\n [  1   1   0 158   1   4   4   2   0   0]\n [  0   1   0   1 110   0   0   0   3   0]\n [  1   2   2   2   1 112   0   2   0   0]\n [  0   3   4   0   1   2 114   0   5   2]\n [  4   3   5   0   0   2   1 132   6   4]\n [  1   1   0   0   1   1   2   1 138   0]\n [  2   1   2   1   0   3   1   6   2 124]]\nTime usage: 0:00:05\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  21.88%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.1, Train Acc:  32.81%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.7, Train Acc:  37.50%, Time: 0:00:46 *\nIter:    120, Train Loss:    3.2, Train Acc:  45.31%, Time: 0:00:57 *\nIter:    150, Train Loss:    2.4, Train Acc:  65.62%, Time: 0:01:08 *\nIter:    180, Train Loss:    2.3, Train Acc:  60.94%, Time: 0:01:14 \nEpoch: 2\nIter:    210, Train Loss:    1.8, Train Acc:  70.31%, Time: 0:01:25 *\nIter:    240, Train Loss:    1.7, Train Acc:  71.88%, Time: 0:01:38 *\nIter:    270, Train Loss:    2.0, Train Acc:  65.62%, Time: 0:01:44 \nIter:    300, Train Loss:    1.5, Train Acc:  82.81%, Time: 0:01:56 *\nIter:    330, Train Loss:    1.5, Train Acc:  78.12%, Time: 0:02:02 \nIter:    360, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:02:08 \nIter:    390, Train Loss:    1.1, Train Acc:  87.50%, Time: 0:02:19 *\nEpoch: 3\nIter:    420, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:26 \nIter:    450, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:32 \nIter:    480, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:38 \nIter:    510, Train Loss:    1.5, Train Acc:  78.12%, Time: 0:02:44 \nIter:    540, Train Loss:    1.6, Train Acc:  70.31%, Time: 0:02:49 \nIter:    570, Train Loss:   0.97, Train Acc:  84.38%, Time: 0:02:55 \nEpoch: 4\nIter:    600, Train Loss:    0.6, Train Acc:  93.75%, Time: 0:03:06 *\nIter:    630, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:12 \nIter:    660, Train Loss:   0.69, Train Acc:  92.19%, Time: 0:03:18 \nIter:    690, Train Loss:   0.86, Train Acc:  90.62%, Time: 0:03:24 \nIter:    720, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:29 \nIter:    750, Train Loss:   0.98, Train Acc:  87.50%, Time: 0:03:35 \nIter:    780, Train Loss:   0.93, Train Acc:  85.94%, Time: 0:03:41 \nEpoch: 5\nIter:    810, Train Loss:   0.77, Train Acc:  89.06%, Time: 0:03:47 \nIter:    840, Train Loss:   0.45, Train Acc:  96.88%, Time: 0:03:57 *\nIter:    870, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:04:03 \nIter:    900, Train Loss:   0.75, Train Acc:  89.06%, Time: 0:04:09 \nIter:    930, Train Loss:   0.52, Train Acc:  89.06%, Time: 0:04:15 \nIter:    960, Train Loss:   0.75, Train Acc:  85.94%, Time: 0:04:20 \nIter:    990, Train Loss:   0.41, Train Acc:  95.65%, Time: 0:04:26 \nEpoch: 6\nIter:   1020, Train Loss:   0.56, Train Acc:  89.06%, Time: 0:04:32 \nIter:   1050, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:04:38 \nIter:   1080, Train Loss:   0.63, Train Acc:  93.75%, Time: 0:04:44 \nIter:   1110, Train Loss:   0.61, Train Acc:  90.62%, Time: 0:04:50 \nIter:   1140, Train Loss:   0.63, Train Acc:  89.06%, Time: 0:04:56 \nIter:   1170, Train Loss:   0.62, Train Acc:  87.50%, Time: 0:05:01 \nEpoch: 7\nIter:   1200, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:05:07 \nIter:   1230, Train Loss:   0.53, Train Acc:  90.62%, Time: 0:05:13 \nIter:   1260, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:05:19 \nIter:   1290, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:05:25 \nIter:   1320, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:05:39 *\nIter:   1350, Train Loss:   0.39, Train Acc:  92.19%, Time: 0:05:45 \nIter:   1380, Train Loss:   0.67, Train Acc:  90.62%, Time: 0:05:50 \nEpoch: 8\nIter:   1410, Train Loss:   0.29, Train Acc:  98.44%, Time: 0:05:56 \nIter:   1440, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:06:02 \nIter:   1470, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:06:08 \nIter:   1500, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:06:14 \nIter:   1530, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:06:19 \nIter:   1560, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:06:25 \nEpoch: 9\nIter:   1590, Train Loss:   0.37, Train Acc:  96.88%, Time: 0:06:31 \nIter:   1620, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:06:37 \nIter:   1650, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:06:43 \nIter:   1680, Train Loss:   0.45, Train Acc:  96.88%, Time: 0:06:48 \nIter:   1710, Train Loss:   0.57, Train Acc:  90.62%, Time: 0:06:54 \nIter:   1740, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:07:00 \nIter:   1770, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:07:06 \nEpoch: 10\nIter:   1800, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:07:12 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/3/3\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/3/3\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.81, Test Acc:  88.46%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.91      0.88      0.89       133\n                   Filter       0.87      0.90      0.89       130\n    Compute Derived Value       0.79      0.92      0.85       140\n            Find Extremum       0.93      0.89      0.91       185\n                     Sort       0.96      0.85      0.90       135\n          Determine Range       0.86      0.90      0.88       147\nCharacterize Distribution       0.96      0.82      0.88       121\n           Find Anomalies       0.86      0.86      0.86       124\n                  Cluster       0.89      0.88      0.88       150\n                Correlate       0.85      0.93      0.89       139\n\n                micro avg       0.88      0.88      0.88      1404\n                macro avg       0.89      0.88      0.88      1404\n             weighted avg       0.89      0.88      0.89      1404\n\nConfusion Matrix...\n[[117   3  11   1   0   0   0   0   0   1]\n [  2 117   0   1   0   4   0   5   0   1]\n [  1   2 129   2   0   3   0   0   1   2]\n [  2   2   9 164   1   3   0   1   0   3]\n [  2   0   0   4 115   3   0   1   8   2]\n [  2   2   4   2   1 133   0   3   0   0]\n [  1   3   7   0   0   3  99   2   4   2]\n [  2   2   1   1   0   3   0 107   2   6]\n [  0   1   2   1   3   2   2   2 132   5]\n [  0   2   1   0   0   0   2   3   2 129]]\nTime usage: 0:00:05\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  15.62%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.1, Train Acc:  32.81%, Time: 0:00:27 *\nIter:     90, Train Loss:    3.3, Train Acc:  50.00%, Time: 0:00:39 *\nIter:    120, Train Loss:    3.4, Train Acc:  39.06%, Time: 0:00:45 \nIter:    150, Train Loss:    2.7, Train Acc:  54.69%, Time: 0:00:57 *\nIter:    180, Train Loss:    2.5, Train Acc:  54.69%, Time: 0:01:03 \nEpoch: 2\nIter:    210, Train Loss:    2.3, Train Acc:  67.19%, Time: 0:01:15 *\nIter:    240, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:21 \nIter:    270, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:33 *\nIter:    300, Train Loss:    1.6, Train Acc:  70.31%, Time: 0:01:39 \nIter:    330, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:45 \nIter:    360, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:57 *\nIter:    390, Train Loss:    1.6, Train Acc:  79.69%, Time: 0:02:09 *\nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  82.81%, Time: 0:02:21 *\nIter:    450, Train Loss:    1.2, Train Acc:  76.56%, Time: 0:02:27 \nIter:    480, Train Loss:    1.1, Train Acc:  78.12%, Time: 0:02:34 \nIter:    510, Train Loss:    1.4, Train Acc:  75.00%, Time: 0:02:40 \nIter:    540, Train Loss:    1.5, Train Acc:  79.69%, Time: 0:02:46 \nIter:    570, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:52 \nEpoch: 4\nIter:    600, Train Loss:   0.95, Train Acc:  87.50%, Time: 0:03:05 *\nIter:    630, Train Loss:   0.86, Train Acc:  89.06%, Time: 0:03:16 *\nIter:    660, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:03:28 *\nIter:    690, Train Loss:   0.79, Train Acc:  84.38%, Time: 0:03:34 \nIter:    720, Train Loss:   0.71, Train Acc:  89.06%, Time: 0:03:40 \nIter:    750, Train Loss:   0.82, Train Acc:  78.12%, Time: 0:03:46 \nIter:    780, Train Loss:   0.74, Train Acc:  90.62%, Time: 0:03:52 \nEpoch: 5\nIter:    810, Train Loss:   0.95, Train Acc:  84.38%, Time: 0:03:58 \nIter:    840, Train Loss:   0.91, Train Acc:  90.62%, Time: 0:04:05 \nIter:    870, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:04:23 *\nIter:    900, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:04:29 \nIter:    930, Train Loss:   0.49, Train Acc:  92.19%, Time: 0:04:35 \nIter:    960, Train Loss:   0.47, Train Acc:  89.06%, Time: 0:04:41 \nIter:    990, Train Loss:   0.16, Train Acc: 100.00%, Time: 0:04:53 *\nEpoch: 6\nIter:   1020, Train Loss:   0.47, Train Acc:  95.31%, Time: 0:04:58 \nIter:   1050, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:05:04 \nIter:   1080, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:05:10 \nIter:   1110, Train Loss:   0.43, Train Acc:  98.44%, Time: 0:05:16 \nIter:   1140, Train Loss:   0.55, Train Acc:  90.62%, Time: 0:05:22 \nIter:   1170, Train Loss:   0.59, Train Acc:  87.50%, Time: 0:05:28 \nEpoch: 7\nIter:   1200, Train Loss:   0.73, Train Acc:  90.62%, Time: 0:05:33 \nIter:   1230, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:05:39 \nIter:   1260, Train Loss:   0.47, Train Acc:  92.19%, Time: 0:05:45 \nIter:   1290, Train Loss:    0.3, Train Acc:  93.75%, Time: 0:05:51 \nIter:   1320, Train Loss:   0.72, Train Acc:  87.50%, Time: 0:05:56 \nIter:   1350, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:06:02 \nIter:   1380, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:06:08 \nEpoch: 8\nIter:   1410, Train Loss:   0.38, Train Acc:  95.31%, Time: 0:06:14 \nIter:   1440, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:06:20 \nIter:   1470, Train Loss:   0.57, Train Acc:  89.06%, Time: 0:06:26 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/4/4\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/4/4\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.92, Test Acc:  85.90%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.80      0.91      0.85       155\n                   Filter       0.81      0.90      0.85       154\n    Compute Derived Value       0.76      0.92      0.83       153\n            Find Extremum       0.96      0.86      0.91       165\n                     Sort       0.88      0.88      0.88       105\n          Determine Range       0.85      0.75      0.80       117\nCharacterize Distribution       0.97      0.77      0.86       154\n           Find Anomalies       0.92      0.81      0.86       135\n                  Cluster       0.90      0.88      0.89       120\n                Correlate       0.84      0.90      0.87       146\n\n                micro avg       0.86      0.86      0.86      1404\n                macro avg       0.87      0.86      0.86      1404\n             weighted avg       0.87      0.86      0.86      1404\n\nConfusion Matrix...\n[[141   2   7   2   1   1   1   0   0   0]\n [ 10 138   3   0   0   2   0   0   0   1]\n [  4   2 141   1   0   1   0   0   1   3]\n [  4   5   4 142   2   5   0   1   0   2]\n [  2   1   4   0  92   2   0   0   3   1]\n [  5   9   7   0   1  88   0   1   2   4]\n [  5   4  12   0   3   1 118   1   4   6]\n [  4   6   5   2   1   0   2 109   1   5]\n [  1   1   2   0   3   3   0   1 105   4]\n [  1   2   1   1   1   0   1   6   1 132]]\nTime usage: 0:00:05\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  12.50%, Time: 0:00:18 *\nIter:     60, Train Loss:    3.8, Train Acc:  53.12%, Time: 0:00:36 *\nIter:     90, Train Loss:    3.0, Train Acc:  56.25%, Time: 0:00:46 *\nIter:    120, Train Loss:    3.0, Train Acc:  51.56%, Time: 0:00:52 \nIter:    150, Train Loss:    2.8, Train Acc:  51.56%, Time: 0:00:58 \nIter:    180, Train Loss:    2.0, Train Acc:  71.88%, Time: 0:01:09 *\nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  68.75%, Time: 0:01:15 \nIter:    240, Train Loss:    2.5, Train Acc:  62.50%, Time: 0:01:20 \nIter:    270, Train Loss:    2.0, Train Acc:  67.19%, Time: 0:01:26 \nIter:    300, Train Loss:    2.2, Train Acc:  70.31%, Time: 0:01:32 \nIter:    330, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:01:38 \nIter:    360, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:01:49 *\nIter:    390, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:55 \nEpoch: 3\nIter:    420, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:02:01 \nIter:    450, Train Loss:    1.3, Train Acc:  84.38%, Time: 0:02:12 *\nIter:    480, Train Loss:    1.1, Train Acc:  78.12%, Time: 0:02:18 \nIter:    510, Train Loss:   0.95, Train Acc:  84.38%, Time: 0:02:24 \nIter:    540, Train Loss:    1.2, Train Acc:  76.56%, Time: 0:02:30 \nIter:    570, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:36 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:41 \nIter:    630, Train Loss:    1.0, Train Acc:  82.81%, Time: 0:02:47 \nIter:    660, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:53 \nIter:    690, Train Loss:   0.88, Train Acc:  85.94%, Time: 0:03:10 *\nIter:    720, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:15 \nIter:    750, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:03:21 \nIter:    780, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:03:27 \nEpoch: 5\nIter:    810, Train Loss:   0.61, Train Acc:  90.62%, Time: 0:03:39 *\nIter:    840, Train Loss:   0.85, Train Acc:  85.94%, Time: 0:03:45 \nIter:    870, Train Loss:    0.9, Train Acc:  81.25%, Time: 0:03:51 \nIter:    900, Train Loss:   0.76, Train Acc:  84.38%, Time: 0:03:57 \nIter:    930, Train Loss:   0.56, Train Acc:  92.19%, Time: 0:04:11 *\nIter:    960, Train Loss:   0.55, Train Acc:  93.75%, Time: 0:04:23 *\nIter:    990, Train Loss:   0.89, Train Acc:  82.61%, Time: 0:04:29 \nEpoch: 6\nIter:   1020, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:04:34 \nIter:   1050, Train Loss:   0.42, Train Acc:  96.88%, Time: 0:04:47 *\nIter:   1080, Train Loss:   0.67, Train Acc:  90.62%, Time: 0:04:53 \nIter:   1110, Train Loss:   0.46, Train Acc:  92.19%, Time: 0:04:58 \nIter:   1140, Train Loss:   0.49, Train Acc:  92.19%, Time: 0:05:04 \nIter:   1170, Train Loss:   0.36, Train Acc:  98.44%, Time: 0:05:23 *\nEpoch: 7\nIter:   1200, Train Loss:   0.73, Train Acc:  85.94%, Time: 0:05:29 \nIter:   1230, Train Loss:   0.86, Train Acc:  89.06%, Time: 0:05:35 \nIter:   1260, Train Loss:   0.79, Train Acc:  93.75%, Time: 0:05:41 \nIter:   1290, Train Loss:   0.31, Train Acc:  93.75%, Time: 0:05:47 \nIter:   1320, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:05:52 \nIter:   1350, Train Loss:   0.48, Train Acc:  92.19%, Time: 0:05:58 \nIter:   1380, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:06:04 \nEpoch: 8\nIter:   1410, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:06:10 \nIter:   1440, Train Loss:   0.54, Train Acc:  90.62%, Time: 0:06:16 \nIter:   1470, Train Loss:   0.33, Train Acc:  93.75%, Time: 0:06:21 \nIter:   1500, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:27 \nIter:   1530, Train Loss:   0.21, Train Acc:  98.44%, Time: 0:06:33 \nIter:   1560, Train Loss:   0.25, Train Acc:  95.31%, Time: 0:06:39 \nEpoch: 9\nIter:   1590, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:06:45 \nIter:   1620, Train Loss:    0.2, Train Acc:  98.44%, Time: 0:06:51 \nIter:   1650, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:06:56 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.71, Test Acc:  89.74%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.94      0.93      0.93       144\n                   Filter       0.86      0.86      0.86       159\n    Compute Derived Value       0.91      0.88      0.90       147\n            Find Extremum       0.95      0.93      0.94       171\n                     Sort       0.87      0.96      0.91       120\n          Determine Range       0.89      0.84      0.86       141\nCharacterize Distribution       0.94      0.90      0.92       124\n           Find Anomalies       0.81      0.93      0.87       136\n                  Cluster       0.89      0.85      0.87       123\n                Correlate       0.91      0.90      0.91       139\n\n                micro avg       0.90      0.90      0.90      1404\n                macro avg       0.90      0.90      0.90      1404\n             weighted avg       0.90      0.90      0.90      1404\n\nConfusion Matrix...\n[[134   2   3   0   3   0   1   1   0   0]\n [  4 136   3   1   2   4   0   7   1   1]\n [  4   3 129   1   1   3   0   3   1   2]\n [  0   3   2 159   1   1   0   5   0   0]\n [  0   0   0   2 115   2   0   0   1   0]\n [  1   8   1   2   2 118   2   2   4   1]\n [  0   2   1   1   0   3 112   0   3   2]\n [  0   3   1   0   1   0   1 127   0   3]\n [  0   0   1   1   7   1   1   4 105   3]\n [  0   1   0   0   0   1   2   7   3 125]]\nTime usage: 0:00:06\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  21.88%, Time: 0:00:17 *\nIter:     60, Train Loss:    4.1, Train Acc:  34.38%, Time: 0:00:29 *\nIter:     90, Train Loss:    3.5, Train Acc:  42.19%, Time: 0:00:41 *\nIter:    120, Train Loss:    2.9, Train Acc:  60.94%, Time: 0:00:53 *\nIter:    150, Train Loss:    2.7, Train Acc:  57.81%, Time: 0:00:59 \nIter:    180, Train Loss:    2.3, Train Acc:  59.38%, Time: 0:01:05 \nEpoch: 2\nIter:    210, Train Loss:    1.7, Train Acc:  71.88%, Time: 0:01:17 *\nIter:    240, Train Loss:    2.1, Train Acc:  71.88%, Time: 0:01:23 \nIter:    270, Train Loss:    2.2, Train Acc:  67.19%, Time: 0:01:29 \nIter:    300, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:40 *\nIter:    330, Train Loss:    1.8, Train Acc:  79.69%, Time: 0:01:52 *\nIter:    360, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:58 \nIter:    390, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:04 \nEpoch: 3\nIter:    420, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:16 *\nIter:    450, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:21 \nIter:    480, Train Loss:    1.4, Train Acc:  84.38%, Time: 0:02:35 *\nIter:    510, Train Loss:    1.7, Train Acc:  79.69%, Time: 0:02:41 \nIter:    540, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:47 \nIter:    570, Train Loss:    1.4, Train Acc:  82.81%, Time: 0:02:53 \nEpoch: 4\nIter:    600, Train Loss:   0.77, Train Acc:  89.06%, Time: 0:03:04 *\nIter:    630, Train Loss:   0.84, Train Acc:  93.75%, Time: 0:03:26 *\nIter:    660, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:03:32 \nIter:    690, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:03:38 \nIter:    720, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:03:43 \nIter:    750, Train Loss:   0.56, Train Acc:  95.31%, Time: 0:03:57 *\nIter:    780, Train Loss:   0.69, Train Acc:  87.50%, Time: 0:04:02 \nEpoch: 5\nIter:    810, Train Loss:   0.58, Train Acc:  89.06%, Time: 0:04:08 \nIter:    840, Train Loss:   0.71, Train Acc:  92.19%, Time: 0:04:14 \nIter:    870, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:04:20 \nIter:    900, Train Loss:    1.0, Train Acc:  79.69%, Time: 0:04:26 \nIter:    930, Train Loss:   0.88, Train Acc:  82.81%, Time: 0:04:32 \nIter:    960, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:04:38 \nIter:    990, Train Loss:   0.33, Train Acc:  95.83%, Time: 0:04:48 *\nEpoch: 6\nIter:   1020, Train Loss:   0.49, Train Acc:  90.62%, Time: 0:04:54 \nIter:   1050, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:05:00 \nIter:   1080, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:05:23 *\nIter:   1110, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:05:28 \nIter:   1140, Train Loss:   0.83, Train Acc:  87.50%, Time: 0:05:34 \nIter:   1170, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:40 \nEpoch: 7\nIter:   1200, Train Loss:   0.92, Train Acc:  90.62%, Time: 0:05:46 \nIter:   1230, Train Loss:   0.23, Train Acc:  98.44%, Time: 0:05:57 *\nIter:   1260, Train Loss:   0.66, Train Acc:  93.75%, Time: 0:06:03 \nIter:   1290, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:06:09 \nIter:   1320, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:06:15 \nIter:   1350, Train Loss:   0.37, Train Acc:  96.88%, Time: 0:06:21 \nIter:   1380, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:27 \nEpoch: 8\nIter:   1410, Train Loss:   0.47, Train Acc:  93.75%, Time: 0:06:32 \nIter:   1440, Train Loss:    0.2, Train Acc:  96.88%, Time: 0:06:38 \nIter:   1470, Train Loss:   0.36, Train Acc:  93.75%, Time: 0:06:44 \nIter:   1500, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:06:50 \nIter:   1530, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:56 \nIter:   1560, Train Loss:   0.54, Train Acc:  92.19%, Time: 0:07:02 \nEpoch: 9\nIter:   1590, Train Loss:   0.38, Train Acc:  95.31%, Time: 0:07:08 \nIter:   1620, Train Loss:   0.39, Train Acc:  92.19%, Time: 0:07:13 \nIter:   1650, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:07:19 \nIter:   1680, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:07:25 \nIter:   1710, Train Loss:   0.12, Train Acc: 100.00%, Time: 0:07:36 *\nIter:   1740, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:07:42 \nIter:   1770, Train Loss:   0.43, Train Acc:  90.62%, Time: 0:07:49 \nEpoch: 10\nIter:   1800, Train Loss:   0.44, Train Acc:  92.19%, Time: 0:07:55 \nIter:   1830, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:08:01 \nIter:   1860, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:08:07 \nIter:   1890, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:08:13 \nIter:   1920, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:08:19 \nIter:   1950, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:08:25 \nIter:   1980, Train Loss:   0.25, Train Acc:  95.83%, Time: 0:08:31 \nEpoch: 11\nIter:   2010, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:08:36 \nIter:   2040, Train Loss:   0.16, Train Acc: 100.00%, Time: 0:08:42 \nIter:   2070, Train Loss:   0.15, Train Acc:  95.31%, Time: 0:08:48 \nIter:   2100, Train Loss:  0.072, Train Acc: 100.00%, Time: 0:08:54 \nIter:   2130, Train Loss:  0.058, Train Acc: 100.00%, Time: 0:09:00 \nIter:   2160, Train Loss:   0.51, Train Acc:  92.19%, Time: 0:09:06 \nEpoch: 12\nIter:   2190, Train Loss:   0.08, Train Acc: 100.00%, Time: 0:09:11 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/6/6\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/6/6\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.61, Test Acc:  91.45%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.84      0.98      0.91       128\n                   Filter       0.87      0.92      0.89       140\n    Compute Derived Value       0.95      0.85      0.90       172\n            Find Extremum       0.94      0.93      0.94       165\n                     Sort       0.98      0.94      0.96       131\n          Determine Range       0.87      0.90      0.88       115\nCharacterize Distribution       0.98      0.89      0.93       143\n           Find Anomalies       0.82      0.91      0.86       134\n                  Cluster       0.93      0.96      0.95       114\n                Correlate       0.96      0.90      0.93       161\n\n                micro avg       0.91      0.91      0.91      1403\n                macro avg       0.92      0.92      0.91      1403\n             weighted avg       0.92      0.91      0.92      1403\n\nConfusion Matrix...\n[[125   0   0   0   0   0   1   2   0   0]\n [  1 129   1   1   0   3   0   4   1   0]\n [  8   7 146   4   0   4   1   1   0   1]\n [  5   1   2 153   1   1   0   1   0   1]\n [  0   0   0   1 123   4   0   2   1   0]\n [  2   3   2   0   0 103   1   4   0   0]\n [  3   1   1   3   0   2 127   2   3   1]\n [  0   6   1   0   1   1   0 122   1   2]\n [  0   0   0   0   0   0   0   3 110   1]\n [  4   2   0   0   0   0   0   8   2 145]]\nTime usage: 0:00:05\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  17.19%, Time: 0:00:31 *\nIter:     60, Train Loss:    3.8, Train Acc:  40.62%, Time: 0:00:55 *\nIter:     90, Train Loss:    3.3, Train Acc:  39.06%, Time: 0:01:01 \nIter:    120, Train Loss:    3.2, Train Acc:  46.88%, Time: 0:01:26 *\nIter:    150, Train Loss:    2.5, Train Acc:  62.50%, Time: 0:01:38 *\nIter:    180, Train Loss:    2.3, Train Acc:  73.44%, Time: 0:01:55 *\nEpoch: 2\nIter:    210, Train Loss:    2.7, Train Acc:  59.38%, Time: 0:02:01 \nIter:    240, Train Loss:    2.3, Train Acc:  57.81%, Time: 0:02:07 \nIter:    270, Train Loss:    2.0, Train Acc:  73.44%, Time: 0:02:13 \nIter:    300, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:02:24 *\nIter:    330, Train Loss:    2.1, Train Acc:  65.62%, Time: 0:02:30 \nIter:    360, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:40 *\nIter:    390, Train Loss:    1.6, Train Acc:  75.00%, Time: 0:02:46 \nEpoch: 3\nIter:    420, Train Loss:    1.9, Train Acc:  75.00%, Time: 0:02:52 \nIter:    450, Train Loss:    1.7, Train Acc:  78.12%, Time: 0:02:58 \nIter:    480, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:03:04 \nIter:    510, Train Loss:    1.6, Train Acc:  70.31%, Time: 0:03:09 \nIter:    540, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:03:15 \nIter:    570, Train Loss:   0.93, Train Acc:  84.38%, Time: 0:03:21 \nEpoch: 4\nIter:    600, Train Loss:   0.85, Train Acc:  90.62%, Time: 0:03:32 *\nIter:    630, Train Loss:   0.82, Train Acc:  90.62%, Time: 0:03:37 \nIter:    660, Train Loss:   0.73, Train Acc:  87.50%, Time: 0:03:43 \nIter:    690, Train Loss:   0.93, Train Acc:  82.81%, Time: 0:03:49 \nIter:    720, Train Loss:    1.0, Train Acc:  82.81%, Time: 0:03:55 \nIter:    750, Train Loss:   0.59, Train Acc:  93.75%, Time: 0:04:05 *\nIter:    780, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:04:10 \nEpoch: 5\nIter:    810, Train Loss:   0.81, Train Acc:  92.19%, Time: 0:04:16 \nIter:    840, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:04:22 \nIter:    870, Train Loss:   0.86, Train Acc:  92.19%, Time: 0:04:28 \nIter:    900, Train Loss:   0.64, Train Acc:  85.94%, Time: 0:04:33 \nIter:    930, Train Loss:   0.47, Train Acc:  89.06%, Time: 0:04:39 \nIter:    960, Train Loss:   0.69, Train Acc:  87.50%, Time: 0:04:45 \nIter:    990, Train Loss:   0.59, Train Acc:  91.67%, Time: 0:04:51 \nEpoch: 6\nIter:   1020, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:04:56 \nIter:   1050, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:05:02 \nIter:   1080, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:05:15 *\nIter:   1110, Train Loss:   0.57, Train Acc:  95.31%, Time: 0:05:21 \nIter:   1140, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:27 \nIter:   1170, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:05:32 \nEpoch: 7\nIter:   1200, Train Loss:    0.6, Train Acc:  89.06%, Time: 0:05:38 \nIter:   1230, Train Loss:   0.62, Train Acc:  89.06%, Time: 0:05:44 \nIter:   1260, Train Loss:   0.47, Train Acc:  95.31%, Time: 0:05:50 \nIter:   1290, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:05:56 \nIter:   1320, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:01 \nIter:   1350, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:12 *\nIter:   1380, Train Loss:    0.6, Train Acc:  93.75%, Time: 0:06:18 \nEpoch: 8\nIter:   1410, Train Loss:   0.36, Train Acc:  92.19%, Time: 0:06:24 \nIter:   1440, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:06:29 \nIter:   1470, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:06:35 \nIter:   1500, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:06:41 \nIter:   1530, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:06:47 \nIter:   1560, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:06:53 \nEpoch: 9\nIter:   1590, Train Loss:  0.073, Train Acc: 100.00%, Time: 0:07:03 *\nIter:   1620, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:07:10 \nIter:   1650, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:07:16 \nIter:   1680, Train Loss:   0.68, Train Acc:  92.19%, Time: 0:07:23 \nIter:   1710, Train Loss:   0.31, Train Acc:  93.75%, Time: 0:07:29 \nIter:   1740, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:07:36 \nIter:   1770, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:07:42 \nEpoch: 10\nIter:   1800, Train Loss:   0.38, Train Acc:  96.88%, Time: 0:07:49 \nIter:   1830, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:07:55 \nIter:   1860, Train Loss:   0.19, Train Acc:  98.44%, Time: 0:08:01 \nIter:   1890, Train Loss:   0.13, Train Acc:  96.88%, Time: 0:08:08 \nIter:   1920, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:08:15 \nIter:   1950, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:08:21 \nIter:   1980, Train Loss:   0.87, Train Acc:  95.83%, Time: 0:08:27 \nEpoch: 11\nIter:   2010, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:08:34 \nIter:   2040, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:08:41 \nIter:   2070, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:08:47 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/7/7\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/7/7\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    0.7, Test Acc:  89.81%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.93      0.88      0.90       139\n                   Filter       0.83      0.95      0.89       150\n    Compute Derived Value       0.86      0.95      0.91       130\n            Find Extremum       0.89      0.91      0.90       173\n                     Sort       0.93      0.95      0.94       110\n          Determine Range       0.92      0.81      0.86       144\nCharacterize Distribution       0.86      0.89      0.87       129\n           Find Anomalies       0.92      0.85      0.88       157\n                  Cluster       0.95      0.86      0.90       121\n                Correlate       0.93      0.93      0.93       150\n\n                micro avg       0.90      0.90      0.90      1403\n                macro avg       0.90      0.90      0.90      1403\n             weighted avg       0.90      0.90      0.90      1403\n\nConfusion Matrix...\n[[122   3   4   1   0   2   4   2   0   1]\n [  1 143   2   2   1   1   0   0   0   0]\n [  2   0 124   2   0   1   0   0   0   1]\n [  3   3   2 158   3   0   2   1   1   0]\n [  0   0   0   0 104   1   4   0   1   0]\n [  2  10   4   6   1 117   1   1   2   0]\n [  0   1   4   1   0   4 115   1   0   3]\n [  1  10   2   5   0   1   0 133   0   5]\n [  0   1   1   0   3   0   5   6 104   1]\n [  0   1   1   2   0   0   3   1   2 140]]\nTime usage: 0:00:06\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  12.50%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  34.38%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.2, Train Acc:  54.69%, Time: 0:00:36 *\nIter:    120, Train Loss:    2.7, Train Acc:  60.94%, Time: 0:00:46 *\nIter:    150, Train Loss:    2.9, Train Acc:  45.31%, Time: 0:00:52 \nIter:    180, Train Loss:    2.7, Train Acc:  59.38%, Time: 0:00:57 \nEpoch: 2\nIter:    210, Train Loss:    2.4, Train Acc:  60.94%, Time: 0:01:03 \nIter:    240, Train Loss:    2.0, Train Acc:  67.19%, Time: 0:01:13 *\nIter:    270, Train Loss:    1.8, Train Acc:  75.00%, Time: 0:01:24 *\nIter:    300, Train Loss:    1.9, Train Acc:  64.06%, Time: 0:01:30 \nIter:    330, Train Loss:    1.5, Train Acc:  78.12%, Time: 0:01:40 *\nIter:    360, Train Loss:    1.8, Train Acc:  67.19%, Time: 0:01:46 \nIter:    390, Train Loss:    1.5, Train Acc:  78.12%, Time: 0:01:52 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  71.88%, Time: 0:01:58 \nIter:    450, Train Loss:    1.9, Train Acc:  73.44%, Time: 0:02:04 \nIter:    480, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:02:10 \nIter:    510, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:20 *\nIter:    540, Train Loss:    1.1, Train Acc:  89.06%, Time: 0:02:30 *\nIter:    570, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:02:36 \nEpoch: 4\nIter:    600, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:42 \nIter:    630, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:48 \nIter:    660, Train Loss:    1.0, Train Acc:  81.25%, Time: 0:02:53 \nIter:    690, Train Loss:   0.93, Train Acc:  89.06%, Time: 0:02:59 \nIter:    720, Train Loss:   0.69, Train Acc:  89.06%, Time: 0:03:05 \nIter:    750, Train Loss:   0.77, Train Acc:  89.06%, Time: 0:03:11 \nIter:    780, Train Loss:    1.0, Train Acc:  81.25%, Time: 0:03:17 \nEpoch: 5\nIter:    810, Train Loss:   0.88, Train Acc:  89.06%, Time: 0:03:23 \nIter:    840, Train Loss:   0.74, Train Acc:  85.94%, Time: 0:03:28 \nIter:    870, Train Loss:   0.73, Train Acc:  89.06%, Time: 0:03:34 \nIter:    900, Train Loss:   0.64, Train Acc:  96.88%, Time: 0:03:46 *\nIter:    930, Train Loss:   0.96, Train Acc:  85.94%, Time: 0:03:52 \nIter:    960, Train Loss:   0.75, Train Acc:  87.50%, Time: 0:03:57 \nIter:    990, Train Loss:    0.7, Train Acc:  83.33%, Time: 0:04:03 \nEpoch: 6\nIter:   1020, Train Loss:   0.62, Train Acc:  89.06%, Time: 0:04:09 \nIter:   1050, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:04:15 \nIter:   1080, Train Loss:   0.44, Train Acc:  90.62%, Time: 0:04:21 \nIter:   1110, Train Loss:   0.47, Train Acc:  93.75%, Time: 0:04:26 \nIter:   1140, Train Loss:   0.76, Train Acc:  87.50%, Time: 0:04:32 \nIter:   1170, Train Loss:   0.56, Train Acc:  92.19%, Time: 0:04:38 \nEpoch: 7\nIter:   1200, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:04:44 \nIter:   1230, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:04:50 \nIter:   1260, Train Loss:    0.3, Train Acc:  96.88%, Time: 0:04:55 \nIter:   1290, Train Loss:   0.44, Train Acc:  92.19%, Time: 0:05:01 \nIter:   1320, Train Loss:   0.53, Train Acc:  93.75%, Time: 0:05:07 \nIter:   1350, Train Loss:   0.49, Train Acc:  93.75%, Time: 0:05:13 \nIter:   1380, Train Loss:   0.43, Train Acc:  92.19%, Time: 0:05:19 \nEpoch: 8\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/8/8\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/8/8\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.95, Test Acc:  86.53%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.84      0.92      0.88       141\n                   Filter       0.86      0.76      0.81       148\n    Compute Derived Value       0.76      0.91      0.83       157\n            Find Extremum       0.98      0.82      0.89       148\n                     Sort       0.93      0.94      0.93       130\n          Determine Range       0.79      0.92      0.85       107\nCharacterize Distribution       0.90      0.86      0.88       129\n           Find Anomalies       0.82      0.85      0.84       149\n                  Cluster       0.91      0.91      0.91       119\n                Correlate       0.92      0.81      0.86       175\n\n                micro avg       0.87      0.87      0.87      1403\n                macro avg       0.87      0.87      0.87      1403\n             weighted avg       0.87      0.87      0.87      1403\n\nConfusion Matrix...\n[[130   1   3   0   0   2   1   0   1   3]\n [ 12 113   9   0   0   6   2   5   1   0]\n [  5   3 143   0   1   3   0   1   0   1]\n [  5   4   6 121   2   4   0   3   1   2]\n [  0   0   3   1 122   2   1   0   1   0]\n [  2   0   2   1   0  98   1   2   0   1]\n [  1   0   9   0   2   1 111   3   2   0]\n [  0   7   3   0   1   3   1 127   2   5]\n [  0   1   1   0   3   1   1   3 108   1]\n [  0   2   9   1   0   4   5  10   3 141]]\nTime usage: 0:00:05\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  17.19%, Time: 0:00:16 *\nIter:     60, Train Loss:    3.8, Train Acc:  28.12%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.8, Train Acc:  35.94%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.4, Train Acc:  40.62%, Time: 0:00:53 *\nIter:    150, Train Loss:    2.7, Train Acc:  62.50%, Time: 0:01:05 *\nIter:    180, Train Loss:    2.4, Train Acc:  64.06%, Time: 0:01:18 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  67.19%, Time: 0:01:30 *\nIter:    240, Train Loss:    2.6, Train Acc:  60.94%, Time: 0:01:36 \nIter:    270, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:01:48 *\nIter:    300, Train Loss:    2.4, Train Acc:  59.38%, Time: 0:01:55 \nIter:    330, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:02:09 *\nIter:    360, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:15 \nIter:    390, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:02:22 \nEpoch: 3\nIter:    420, Train Loss:    1.5, Train Acc:  70.31%, Time: 0:02:28 \nIter:    450, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:02:34 \nIter:    480, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:02:41 \nIter:    510, Train Loss:    1.4, Train Acc:  84.38%, Time: 0:02:53 *\nIter:    540, Train Loss:    1.8, Train Acc:  73.44%, Time: 0:03:00 \nIter:    570, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:03:06 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:03:12 \nIter:    630, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:03:19 \nIter:    660, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:25 \nIter:    690, Train Loss:   0.78, Train Acc:  90.62%, Time: 0:03:35 *\nIter:    720, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:03:41 \nIter:    750, Train Loss:   0.93, Train Acc:  84.38%, Time: 0:03:47 \nIter:    780, Train Loss:    0.8, Train Acc:  87.50%, Time: 0:03:53 \nEpoch: 5\nIter:    810, Train Loss:   0.99, Train Acc:  85.94%, Time: 0:03:59 \nIter:    840, Train Loss:   0.78, Train Acc:  87.50%, Time: 0:04:05 \nIter:    870, Train Loss:   0.79, Train Acc:  90.62%, Time: 0:04:10 \nIter:    900, Train Loss:   0.63, Train Acc:  90.62%, Time: 0:04:16 \nIter:    930, Train Loss:   0.63, Train Acc:  92.19%, Time: 0:04:26 *\nIter:    960, Train Loss:   0.84, Train Acc:  85.94%, Time: 0:04:32 \nIter:    990, Train Loss:   0.23, Train Acc: 100.00%, Time: 0:04:43 *\nEpoch: 6\nIter:   1020, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:04:49 \nIter:   1050, Train Loss:   0.97, Train Acc:  84.38%, Time: 0:04:55 \nIter:   1080, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:01 \nIter:   1110, Train Loss:   0.34, Train Acc:  93.75%, Time: 0:05:06 \nIter:   1140, Train Loss:   0.85, Train Acc:  89.06%, Time: 0:05:12 \nIter:   1170, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:18 \nEpoch: 7\nIter:   1200, Train Loss:   0.52, Train Acc:  89.06%, Time: 0:05:24 \nIter:   1230, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:05:29 \nIter:   1260, Train Loss:   0.52, Train Acc:  96.88%, Time: 0:05:35 \nIter:   1290, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:05:41 \nIter:   1320, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:05:47 \nIter:   1350, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:05:53 \nIter:   1380, Train Loss:   0.37, Train Acc:  98.44%, Time: 0:05:59 \nEpoch: 8\nIter:   1410, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:06:05 \nIter:   1440, Train Loss:   0.42, Train Acc:  93.75%, Time: 0:06:11 \nIter:   1470, Train Loss:   0.36, Train Acc:  98.44%, Time: 0:06:17 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/9/9\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/9/9\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.86, Test Acc:  86.39%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.87      0.83      0.85       123\n                   Filter       0.74      0.91      0.82       138\n    Compute Derived Value       0.77      0.90      0.83       153\n            Find Extremum       0.84      0.91      0.88       152\n                     Sort       0.99      0.80      0.88       137\n          Determine Range       0.89      0.77      0.83       155\nCharacterize Distribution       0.97      0.86      0.91       132\n           Find Anomalies       0.84      0.89      0.87       138\n                  Cluster       0.91      0.87      0.89       121\n                Correlate       0.92      0.90      0.91       154\n\n                micro avg       0.86      0.86      0.86      1403\n                macro avg       0.87      0.86      0.87      1403\n             weighted avg       0.87      0.86      0.86      1403\n\nConfusion Matrix...\n[[102   4  12   3   0   0   1   0   0   1]\n [  1 126   4   3   0   3   0   1   0   0]\n [  5   3 137   2   0   1   1   0   1   3]\n [  3   6   3 139   0   0   0   1   0   0]\n [  1   7   5   5 109   1   0   1   6   2]\n [  2  16   9   7   0 119   0   2   0   0]\n [  1   2   3   2   0   4 113   1   3   3]\n [  2   6   1   3   0   1   0 123   0   2]\n [  0   0   0   1   1   2   2   9 105   1]\n [  0   1   4   0   0   2   0   8   0 139]]\nTime usage: 0:00:06\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:   6.25%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.0, Train Acc:  35.94%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.3, Train Acc:  46.88%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.5, Train Acc:  40.62%, Time: 0:00:46 \nIter:    150, Train Loss:    2.2, Train Acc:  70.31%, Time: 0:00:59 *\nIter:    180, Train Loss:    2.7, Train Acc:  56.25%, Time: 0:01:05 \nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  67.19%, Time: 0:01:10 \nIter:    240, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:23 *\nIter:    270, Train Loss:    2.5, Train Acc:  56.25%, Time: 0:01:29 \nIter:    300, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:01:34 \nIter:    330, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:47 *\nIter:    360, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:01:59 *\nIter:    390, Train Loss:   0.92, Train Acc:  90.62%, Time: 0:02:11 *\nEpoch: 3\nIter:    420, Train Loss:    1.6, Train Acc:  81.25%, Time: 0:02:17 \nIter:    450, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:24 \nIter:    480, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:30 \nIter:    510, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:37 \nIter:    540, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:43 \nIter:    570, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:49 \nEpoch: 4\nIter:    600, Train Loss:   0.98, Train Acc:  89.06%, Time: 0:02:55 \nIter:    630, Train Loss:   0.87, Train Acc:  85.94%, Time: 0:03:01 \nIter:    660, Train Loss:   0.81, Train Acc:  85.94%, Time: 0:03:08 \nIter:    690, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:14 \nIter:    720, Train Loss:   0.83, Train Acc:  89.06%, Time: 0:03:20 \nIter:    750, Train Loss:   0.75, Train Acc:  90.62%, Time: 0:03:26 \nIter:    780, Train Loss:   0.82, Train Acc:  87.50%, Time: 0:03:32 \nEpoch: 5\nIter:    810, Train Loss:   0.77, Train Acc:  92.19%, Time: 0:03:43 *\nIter:    840, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:03:54 *\nIter:    870, Train Loss:   0.87, Train Acc:  89.06%, Time: 0:04:00 \nIter:    900, Train Loss:   0.55, Train Acc:  93.75%, Time: 0:04:06 \nIter:    930, Train Loss:   0.52, Train Acc:  92.19%, Time: 0:04:12 \nIter:    960, Train Loss:   0.96, Train Acc:  84.38%, Time: 0:04:18 \nIter:    990, Train Loss:   0.46, Train Acc:  91.67%, Time: 0:04:24 \nEpoch: 6\nIter:   1020, Train Loss:   0.78, Train Acc:  84.38%, Time: 0:04:31 \nIter:   1050, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:04:37 \nIter:   1080, Train Loss:   0.47, Train Acc:  95.31%, Time: 0:04:48 *\nIter:   1110, Train Loss:   0.66, Train Acc:  90.62%, Time: 0:04:54 \nIter:   1140, Train Loss:   0.57, Train Acc:  89.06%, Time: 0:05:00 \nIter:   1170, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:05:06 \nEpoch: 7\nIter:   1200, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:05:12 \nIter:   1230, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:05:18 \nIter:   1260, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:05:24 \nIter:   1290, Train Loss:   0.56, Train Acc:  93.75%, Time: 0:05:31 \nIter:   1320, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:05:37 \nIter:   1350, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:05:43 \nIter:   1380, Train Loss:   0.51, Train Acc:  89.06%, Time: 0:05:49 \nEpoch: 8\nIter:   1410, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:05:55 \nIter:   1440, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:06:01 \nIter:   1470, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:06:07 \nIter:   1500, Train Loss:   0.55, Train Acc:  93.75%, Time: 0:06:13 \nIter:   1530, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:06:23 *\nIter:   1560, Train Loss:   0.33, Train Acc:  96.88%, Time: 0:06:29 \nEpoch: 9\nIter:   1590, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:40 *\nIter:   1620, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:06:45 \nIter:   1650, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:51 \nIter:   1680, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:57 \nIter:   1710, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:07:03 \nIter:   1740, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:07:08 \nIter:   1770, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:07:14 \nEpoch: 10\nIter:   1800, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:07:20 \nIter:   1830, Train Loss:   0.74, Train Acc:  90.62%, Time: 0:07:26 \nIter:   1860, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:31 \nIter:   1890, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:07:37 \nIter:   1920, Train Loss:   0.33, Train Acc:  98.44%, Time: 0:07:43 \nIter:   1950, Train Loss:   0.19, Train Acc: 100.00%, Time: 0:07:55 *\nIter:   1980, Train Loss:    0.2, Train Acc:  95.83%, Time: 0:08:01 \nEpoch: 11\nIter:   2010, Train Loss:   0.12, Train Acc: 100.00%, Time: 0:08:07 \nIter:   2040, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:08:12 \nIter:   2070, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:08:18 \nIter:   2100, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:08:24 \nIter:   2130, Train Loss:  0.097, Train Acc:  98.44%, Time: 0:08:30 \nIter:   2160, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:08:35 \nEpoch: 12\nIter:   2190, Train Loss:    0.2, Train Acc:  93.75%, Time: 0:08:41 \nIter:   2220, Train Loss:  0.072, Train Acc: 100.00%, Time: 0:08:47 \nIter:   2250, Train Loss:   0.33, Train Acc:  93.75%, Time: 0:08:53 \nIter:   2280, Train Loss:   0.11, Train Acc:  98.44%, Time: 0:08:58 \nIter:   2310, Train Loss:  0.058, Train Acc: 100.00%, Time: 0:09:04 \nIter:   2340, Train Loss:   0.15, Train Acc:  98.44%, Time: 0:09:10 \nIter:   2370, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:09:16 \nEpoch: 13\nIter:   2400, Train Loss:   0.15, Train Acc: 100.00%, Time: 0:09:21 \nIter:   2430, Train Loss:  0.038, Train Acc: 100.00%, Time: 0:09:27 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/10/10\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/random/10/10\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.55, Test Acc:  92.44%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.95      0.94      0.95       129\n                   Filter       0.93      0.90      0.91       156\n    Compute Derived Value       0.89      0.95      0.92       175\n            Find Extremum       0.95      0.93      0.94       168\n                     Sort       0.90      0.96      0.93       108\n          Determine Range       0.90      0.89      0.89       136\nCharacterize Distribution       0.99      0.87      0.92       126\n           Find Anomalies       0.89      0.96      0.92       115\n                  Cluster       0.96      0.91      0.93       123\n                Correlate       0.92      0.94      0.93       167\n\n                micro avg       0.92      0.92      0.92      1403\n                macro avg       0.93      0.92      0.92      1403\n             weighted avg       0.93      0.92      0.92      1403\n\nConfusion Matrix...\n[[121   0   5   1   0   0   0   1   0   1]\n [  1 140   1   3   0   5   0   4   0   2]\n [  1   3 167   1   0   1   1   1   0   0]\n [  1   1   5 156   3   2   0   0   0   0]\n [  0   0   1   0 104   1   0   0   2   0]\n [  2   2   4   2   4 121   0   0   0   1]\n [  1   0   2   1   3   3 109   1   1   5]\n [  0   1   1   0   0   0   0 110   0   3]\n [  0   2   0   1   1   2   0   3 112   2]\n [  0   2   2   0   0   0   0   4   2 157]]\nTime usage: 0:00:05\n[0.8952991447897038, 0.9131054136148545, 0.8846153847851984, 0.8589743593139866, 0.8974358970962698, 0.9144689944584031, 0.8980755512831641, 0.8652886669719041, 0.8638631512416913, 0.9244476131940856]\n0.8915574176749261, 0.021746107050300542, 0.02292240950693183, 0.00047289317184313087\nexpert\n20 983 625 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  15.62%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.2, Train Acc:  34.38%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.4, Train Acc:  54.69%, Time: 0:00:35 *\nIter:    120, Train Loss:    3.0, Train Acc:  53.12%, Time: 0:00:41 \nIter:    150, Train Loss:    2.8, Train Acc:  56.25%, Time: 0:00:51 *\nIter:    180, Train Loss:    2.3, Train Acc:  68.75%, Time: 0:01:02 *\nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  70.31%, Time: 0:01:12 *\nIter:    240, Train Loss:    2.0, Train Acc:  68.75%, Time: 0:01:18 \nIter:    270, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:01:30 *\nIter:    300, Train Loss:    2.1, Train Acc:  64.06%, Time: 0:01:36 \nIter:    330, Train Loss:    1.4, Train Acc:  73.44%, Time: 0:01:43 \nIter:    360, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:54 *\nIter:    390, Train Loss:    1.5, Train Acc:  81.82%, Time: 0:02:05 *\nEpoch: 3\nIter:    420, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:02:17 *\nIter:    450, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:02:23 \nIter:    480, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:29 \nIter:    510, Train Loss:   0.84, Train Acc:  87.50%, Time: 0:02:40 *\nIter:    540, Train Loss:   0.77, Train Acc:  90.62%, Time: 0:02:50 *\nIter:    570, Train Loss:   0.85, Train Acc:  87.50%, Time: 0:02:57 \nEpoch: 4\nIter:    600, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:03:02 \nIter:    630, Train Loss:   0.74, Train Acc:  87.50%, Time: 0:03:09 \nIter:    660, Train Loss:   0.74, Train Acc:  89.06%, Time: 0:03:15 \nIter:    690, Train Loss:   0.65, Train Acc:  90.62%, Time: 0:03:21 \nIter:    720, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:27 \nIter:    750, Train Loss:   0.65, Train Acc:  92.19%, Time: 0:03:39 *\nIter:    780, Train Loss:   0.45, Train Acc: 100.00%, Time: 0:03:50 *\nEpoch: 5\nIter:    810, Train Loss:   0.96, Train Acc:  82.81%, Time: 0:03:56 \nIter:    840, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:04:02 \nIter:    870, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:04:08 \nIter:    900, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:04:13 \nIter:    930, Train Loss:   0.81, Train Acc:  90.62%, Time: 0:04:19 \nIter:    960, Train Loss:   0.77, Train Acc:  89.06%, Time: 0:04:25 \nEpoch: 6\nIter:    990, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:04:31 \nIter:   1020, Train Loss:    0.8, Train Acc:  90.62%, Time: 0:04:36 \nIter:   1050, Train Loss:   0.69, Train Acc:  90.62%, Time: 0:04:42 \nIter:   1080, Train Loss:   0.44, Train Acc:  92.19%, Time: 0:04:48 \nIter:   1110, Train Loss:   0.64, Train Acc:  92.19%, Time: 0:04:54 \nIter:   1140, Train Loss:   0.26, Train Acc:  98.44%, Time: 0:05:00 \nIter:   1170, Train Loss:   0.14, Train Acc: 100.00%, Time: 0:05:06 \nEpoch: 7\nIter:   1200, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:05:12 \nIter:   1230, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:05:18 \nIter:   1260, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:05:24 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/1/1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/1/1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.4, Test Acc:  66.98%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.59      0.56      0.58       113\n                   Filter       0.46      0.66      0.54       125\n    Compute Derived Value       0.67      0.62      0.65       207\n            Find Extremum       0.78      0.83      0.81       327\n                     Sort       0.93      0.69      0.79       115\n          Determine Range       0.31      0.24      0.27       120\nCharacterize Distribution       0.71      0.68      0.69       173\n           Find Anomalies       0.67      0.53      0.59       126\n                  Cluster       0.94      0.63      0.75       118\n                Correlate       0.63      0.89      0.74       184\n\n                micro avg       0.67      0.67      0.67      1608\n                macro avg       0.67      0.63      0.64      1608\n             weighted avg       0.68      0.67      0.67      1608\n\nConfusion Matrix...\n[[ 63   9  16   5   0   6   0  11   0   3]\n [  0  82   2  10   0  10   0   2   0  19]\n [  6  18 129  14   0   8  12   0   0  20]\n [ 18   5  14 273   0  12   0   2   1   2]\n [  0   3   0  12  79  15   2   1   2   1]\n [  7  58   3  21   1  29   1   0   0   0]\n [  9   1  10   1   1   3 117   5   0  26]\n [  1   1  13  12   0   8   9  67   1  14]\n [  0   0   4   0   4   1  16   7  74  12]\n [  2   1   2   0   0   1   8   5   1 164]]\nTime usage: 0:00:06\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  17.19%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  34.38%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.3, Train Acc:  42.19%, Time: 0:00:37 *\nIter:    120, Train Loss:    3.3, Train Acc:  45.31%, Time: 0:00:50 *\nIter:    150, Train Loss:    2.3, Train Acc:  64.06%, Time: 0:01:01 *\nIter:    180, Train Loss:    2.4, Train Acc:  60.94%, Time: 0:01:07 \nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  78.12%, Time: 0:01:18 *\nIter:    240, Train Loss:    1.6, Train Acc:  81.25%, Time: 0:01:30 *\nIter:    270, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:36 \nIter:    300, Train Loss:    2.1, Train Acc:  65.62%, Time: 0:01:42 \nIter:    330, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:48 \nIter:    360, Train Loss:    2.1, Train Acc:  75.00%, Time: 0:01:54 \nEpoch: 3\nIter:    390, Train Loss:    1.6, Train Acc:  79.69%, Time: 0:02:00 \nIter:    420, Train Loss:    1.4, Train Acc:  73.44%, Time: 0:02:07 \nIter:    450, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:02:19 *\nIter:    480, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:25 \nIter:    510, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:38 *\nIter:    540, Train Loss:   0.81, Train Acc:  92.19%, Time: 0:02:50 *\nIter:    570, Train Loss:   0.77, Train Acc:  89.06%, Time: 0:02:57 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:03:03 \nIter:    630, Train Loss:    1.2, Train Acc:  76.56%, Time: 0:03:09 \nIter:    660, Train Loss:   0.86, Train Acc:  85.94%, Time: 0:03:15 \nIter:    690, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:21 \nIter:    720, Train Loss:   0.86, Train Acc:  82.81%, Time: 0:03:27 \nIter:    750, Train Loss:   0.67, Train Acc:  87.50%, Time: 0:03:32 \nEpoch: 5\nIter:    780, Train Loss:   0.79, Train Acc:  89.06%, Time: 0:03:39 \nIter:    810, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:03:51 *\nIter:    840, Train Loss:    0.8, Train Acc:  82.81%, Time: 0:03:57 \nIter:    870, Train Loss:   0.71, Train Acc:  85.94%, Time: 0:04:03 \nIter:    900, Train Loss:   0.77, Train Acc:  90.62%, Time: 0:04:10 \nIter:    930, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:04:16 \nIter:    960, Train Loss:   0.92, Train Acc:  85.00%, Time: 0:04:22 \nEpoch: 6\nIter:    990, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:04:34 *\nIter:   1020, Train Loss:   0.62, Train Acc:  87.50%, Time: 0:04:40 \nIter:   1050, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:04:46 \nIter:   1080, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:04:52 \nIter:   1110, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:04:58 \nIter:   1140, Train Loss:   0.46, Train Acc:  92.19%, Time: 0:05:04 \nEpoch: 7\nIter:   1170, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:05:10 \nIter:   1200, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:05:16 \nIter:   1230, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:22 \nIter:   1260, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:29 \nIter:   1290, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:05:35 \nIter:   1320, Train Loss:   0.48, Train Acc:  92.19%, Time: 0:05:41 \nEpoch: 8\nIter:   1350, Train Loss:   0.41, Train Acc:  92.19%, Time: 0:05:47 \nIter:   1380, Train Loss:   0.19, Train Acc:  95.31%, Time: 0:05:53 \nIter:   1410, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:05:59 \nIter:   1440, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:06:05 \nIter:   1470, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:11 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/2/2\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/2/2\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.9, Test Acc:  61.26%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.72      0.70      0.71       228\n                   Filter       0.40      0.55      0.46       218\n    Compute Derived Value       0.62      0.30      0.41       175\n            Find Extremum       0.59      0.80      0.68       201\n                     Sort       0.79      0.86      0.82       130\n          Determine Range       0.57      0.68      0.62       127\nCharacterize Distribution       0.86      0.56      0.68       158\n           Find Anomalies       0.45      0.38      0.41       203\n                  Cluster       0.57      0.65      0.60       119\n                Correlate       0.80      0.71      0.75       212\n\n                micro avg       0.61      0.61      0.61      1771\n                macro avg       0.64      0.62      0.62      1771\n             weighted avg       0.63      0.61      0.61      1771\n\nConfusion Matrix...\n[[160   6   2  26   4   0   0  19   1  10]\n [  5 120   4  13   5  21   1  39   8   2]\n [ 36  29  53  38   0  12   1   3   1   2]\n [  8  17   1 160  11   0   0   3   0   1]\n [  5   1   2   3 112   3   0   0   4   0]\n [  0  27   2   7   0  86   1   3   1   0]\n [  0   0  15   5   1  15  89  11   6  16]\n [  0  58   3  10   3   5   7  77  34   6]\n [  9  12   0   0   4   4   3   9  77   1]\n [  0  30   3   7   2   5   1   9   4 151]]\nTime usage: 0:00:06\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.2, Train Acc:  28.12%, Time: 0:00:15 *\nIter:     60, Train Loss:    3.9, Train Acc:  37.50%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.2, Train Acc:  46.88%, Time: 0:00:37 *\nIter:    120, Train Loss:    3.1, Train Acc:  46.88%, Time: 0:00:43 \nIter:    150, Train Loss:    2.3, Train Acc:  67.19%, Time: 0:00:55 *\nIter:    180, Train Loss:    2.6, Train Acc:  56.25%, Time: 0:01:01 \nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  68.75%, Time: 0:01:12 *\nIter:    240, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:23 *\nIter:    270, Train Loss:    1.7, Train Acc:  82.81%, Time: 0:01:33 *\nIter:    300, Train Loss:    1.4, Train Acc:  85.94%, Time: 0:01:43 *\nIter:    330, Train Loss:    1.5, Train Acc:  79.69%, Time: 0:01:49 \nIter:    360, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:55 \nIter:    390, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:02:02 \nEpoch: 3\nIter:    420, Train Loss:    1.3, Train Acc:  76.56%, Time: 0:02:08 \nIter:    450, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:14 \nIter:    480, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:20 \nIter:    510, Train Loss:    1.6, Train Acc:  78.12%, Time: 0:02:26 \nIter:    540, Train Loss:    1.2, Train Acc:  82.81%, Time: 0:02:32 \nIter:    570, Train Loss:    1.3, Train Acc:  78.12%, Time: 0:02:38 \nIter:    600, Train Loss:   0.69, Train Acc:  93.33%, Time: 0:02:52 *\nEpoch: 4\nIter:    630, Train Loss:   0.79, Train Acc:  92.19%, Time: 0:02:58 \nIter:    660, Train Loss:   0.99, Train Acc:  84.38%, Time: 0:03:04 \nIter:    690, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:03:09 \nIter:    720, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:03:15 \nIter:    750, Train Loss:   0.81, Train Acc:  87.50%, Time: 0:03:21 \nIter:    780, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:03:27 \nEpoch: 5\nIter:    810, Train Loss:   0.71, Train Acc:  89.06%, Time: 0:03:32 \nIter:    840, Train Loss:   0.64, Train Acc:  93.75%, Time: 0:03:45 *\nIter:    870, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:03:50 \nIter:    900, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:03:56 \nIter:    930, Train Loss:   0.55, Train Acc:  93.75%, Time: 0:04:02 \nIter:    960, Train Loss:    1.1, Train Acc:  79.69%, Time: 0:04:08 \nIter:    990, Train Loss:   0.89, Train Acc:  85.94%, Time: 0:04:14 \nEpoch: 6\nIter:   1020, Train Loss:    0.5, Train Acc:  90.62%, Time: 0:04:19 \nIter:   1050, Train Loss:   0.55, Train Acc:  92.19%, Time: 0:04:25 \nIter:   1080, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:04:36 *\nIter:   1110, Train Loss:   0.69, Train Acc:  92.19%, Time: 0:04:41 \nIter:   1140, Train Loss:   0.43, Train Acc:  92.19%, Time: 0:04:47 \nIter:   1170, Train Loss:   0.86, Train Acc:  89.06%, Time: 0:04:53 \nIter:   1200, Train Loss:   0.38, Train Acc:  93.33%, Time: 0:04:59 \nEpoch: 7\nIter:   1230, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:05:05 \nIter:   1260, Train Loss:   0.62, Train Acc:  87.50%, Time: 0:05:10 \nIter:   1290, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:05:22 *\nIter:   1320, Train Loss:   0.63, Train Acc:  92.19%, Time: 0:05:27 \nIter:   1350, Train Loss:   0.37, Train Acc:  95.31%, Time: 0:05:33 \nIter:   1380, Train Loss:    0.5, Train Acc:  95.31%, Time: 0:05:39 \nEpoch: 8\nIter:   1410, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:05:45 \nIter:   1440, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:05:51 \nIter:   1470, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:05:57 \nIter:   1500, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:06:03 \nIter:   1530, Train Loss:   0.35, Train Acc:  93.75%, Time: 0:06:09 \nIter:   1560, Train Loss:   0.36, Train Acc:  92.19%, Time: 0:06:15 \nIter:   1590, Train Loss:  0.087, Train Acc: 100.00%, Time: 0:06:27 *\nEpoch: 9\nIter:   1620, Train Loss:   0.36, Train Acc:  92.19%, Time: 0:06:33 \nIter:   1650, Train Loss:  0.074, Train Acc: 100.00%, Time: 0:06:39 \nIter:   1680, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:06:45 \nIter:   1710, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:52 \nIter:   1740, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:06:58 \nIter:   1770, Train Loss:   0.16, Train Acc:  96.88%, Time: 0:07:04 \nIter:   1800, Train Loss:   0.15, Train Acc:  98.33%, Time: 0:07:10 \nEpoch: 10\nIter:   1830, Train Loss:    0.2, Train Acc:  95.31%, Time: 0:07:16 \nIter:   1860, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:07:22 \nIter:   1890, Train Loss:   0.15, Train Acc:  98.44%, Time: 0:07:28 \nIter:   1920, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:07:33 \nIter:   1950, Train Loss:  0.097, Train Acc:  98.44%, Time: 0:07:39 \nIter:   1980, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:07:45 \nEpoch: 11\nIter:   2010, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:07:51 \nIter:   2040, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:07:56 \nIter:   2070, Train Loss:  0.096, Train Acc: 100.00%, Time: 0:08:02 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/3/3\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/3/3\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.2, Test Acc:  60.05%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.49      0.64      0.55       124\n                   Filter       0.55      0.66      0.60       119\n    Compute Derived Value       0.45      0.53      0.48       131\n            Find Extremum       0.76      0.68      0.72       123\n                     Sort       0.93      0.73      0.81       121\n          Determine Range       0.76      0.50      0.61       125\nCharacterize Distribution       0.70      0.88      0.78       127\n           Find Anomalies       0.49      0.38      0.43       121\n                  Cluster       0.67      0.50      0.57       123\n                Correlate       0.43      0.50      0.46       125\n\n                micro avg       0.60      0.60      0.60      1239\n                macro avg       0.62      0.60      0.60      1239\n             weighted avg       0.62      0.60      0.60      1239\n\nConfusion Matrix...\n[[ 79   6  25   3   2   7   1   0   1   0]\n [  5  78   4   1   0   0   1  20   0  10]\n [ 31   4  69   0   0   1   6   1   1  18]\n [ 17   9   9  84   0   0   0   0   0   4]\n [  2   4  12   7  88   1   0   1   5   1]\n [  6  13  11   7   1  63  11   4   5   4]\n [  2   0   7   1   0   0 112   3   1   1]\n [  0  27   4   2   1  11   0  46   1  29]\n [  9   2   2   1   3   0  12  16  62  16]\n [ 10   0  12   4   0   0  16   3  17  63]]\nTime usage: 0:00:05\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  31.25%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  35.94%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.5, Train Acc:  46.88%, Time: 0:00:36 *\nIter:    120, Train Loss:    3.2, Train Acc:  45.31%, Time: 0:00:42 \nIter:    150, Train Loss:    2.5, Train Acc:  62.50%, Time: 0:00:52 *\nIter:    180, Train Loss:    2.6, Train Acc:  56.25%, Time: 0:00:58 \nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  71.88%, Time: 0:01:09 *\nIter:    240, Train Loss:    2.3, Train Acc:  62.50%, Time: 0:01:14 \nIter:    270, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:25 *\nIter:    300, Train Loss:    1.6, Train Acc:  82.81%, Time: 0:01:36 *\nIter:    330, Train Loss:    2.2, Train Acc:  62.50%, Time: 0:01:42 \nIter:    360, Train Loss:    1.8, Train Acc:  70.31%, Time: 0:01:48 \nIter:    390, Train Loss:    1.3, Train Acc:  75.00%, Time: 0:01:54 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:59 \nIter:    450, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:05 \nIter:    480, Train Loss:   0.93, Train Acc:  84.38%, Time: 0:02:17 *\nIter:    510, Train Loss:   0.97, Train Acc:  87.50%, Time: 0:02:28 *\nIter:    540, Train Loss:    1.3, Train Acc:  78.12%, Time: 0:02:34 \nIter:    570, Train Loss:   0.82, Train Acc:  87.50%, Time: 0:02:39 \nIter:    600, Train Loss:    1.1, Train Acc:  79.69%, Time: 0:02:45 \nEpoch: 4\nIter:    630, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:51 \nIter:    660, Train Loss:   0.98, Train Acc:  85.94%, Time: 0:02:57 \nIter:    690, Train Loss:   0.79, Train Acc:  81.25%, Time: 0:03:03 \nIter:    720, Train Loss:   0.75, Train Acc:  92.19%, Time: 0:03:15 *\nIter:    750, Train Loss:   0.72, Train Acc:  89.06%, Time: 0:03:20 \nIter:    780, Train Loss:    1.0, Train Acc:  87.50%, Time: 0:03:26 \nEpoch: 5\nIter:    810, Train Loss:   0.81, Train Acc:  82.81%, Time: 0:03:32 \nIter:    840, Train Loss:   0.63, Train Acc:  90.62%, Time: 0:03:38 \nIter:    870, Train Loss:   0.69, Train Acc:  93.75%, Time: 0:03:50 *\nIter:    900, Train Loss:   0.73, Train Acc:  87.50%, Time: 0:03:55 \nIter:    930, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:04:08 *\nIter:    960, Train Loss:   0.65, Train Acc:  89.06%, Time: 0:04:14 \nIter:    990, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:04:24 *\nEpoch: 6\nIter:   1020, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:04:30 \nIter:   1050, Train Loss:    0.6, Train Acc:  95.31%, Time: 0:04:35 \nIter:   1080, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:04:41 \nIter:   1110, Train Loss:   0.42, Train Acc:  93.75%, Time: 0:04:47 \nIter:   1140, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:04:53 \nIter:   1170, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:04:59 \nIter:   1200, Train Loss:   0.33, Train Acc:  98.44%, Time: 0:05:10 *\nEpoch: 7\nIter:   1230, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:05:16 \nIter:   1260, Train Loss:   0.34, Train Acc:  93.75%, Time: 0:05:22 \nIter:   1290, Train Loss:   0.28, Train Acc:  98.44%, Time: 0:05:27 \nIter:   1320, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:33 \nIter:   1350, Train Loss:   0.44, Train Acc:  95.31%, Time: 0:05:39 \nIter:   1380, Train Loss:   0.44, Train Acc:  96.88%, Time: 0:05:45 \nEpoch: 8\nIter:   1410, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:05:51 \nIter:   1440, Train Loss:   0.38, Train Acc:  92.19%, Time: 0:05:56 \nIter:   1470, Train Loss:   0.37, Train Acc:  95.31%, Time: 0:06:02 \nIter:   1500, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:06:08 \nIter:   1530, Train Loss:   0.57, Train Acc:  92.19%, Time: 0:06:14 \nIter:   1560, Train Loss:    0.3, Train Acc:  93.75%, Time: 0:06:20 \nIter:   1590, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:06:26 \nEpoch: 9\nIter:   1620, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:06:32 \nIter:   1650, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:06:38 \nIter:   1680, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:06:44 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/4/4\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/4/4\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.6, Test Acc:  66.64%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.29      0.36       112\n                   Filter       0.47      0.73      0.57       112\n    Compute Derived Value       0.34      0.55      0.42       119\n            Find Extremum       0.96      0.41      0.58       123\n                     Sort       0.95      0.77      0.85       124\n          Determine Range       0.77      0.38      0.51       130\nCharacterize Distribution       0.76      0.84      0.80       131\n           Find Anomalies       0.72      0.80      0.76       124\n                  Cluster       0.87      0.90      0.89       131\n                Correlate       0.74      0.94      0.83       120\n\n                micro avg       0.67      0.67      0.67      1226\n                macro avg       0.71      0.66      0.66      1226\n             weighted avg       0.71      0.67      0.66      1226\n\nConfusion Matrix...\n[[ 33  19  46   0   0   3   6   0   0   5]\n [  4  82  11   0   0   1   2   6   6   0]\n [  8   1  66   0   0   5  17   2   3  17]\n [  6   9  33  51   0   4   1  17   0   2]\n [  6   5   3   1  95   0   3   1   7   3]\n [ 15  43  18   0   0  50   3   0   1   0]\n [  0   0  14   0   0   0 110   3   0   4]\n [  0  13   2   1   0   2   0  99   0   7]\n [  0   2   0   0   5   0   2   3 118   1]\n [  0   0   1   0   0   0   0   6   0 113]]\nTime usage: 0:00:05\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  18.75%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.1, Train Acc:  28.12%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.1, Train Acc:  50.00%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.2, Train Acc:  50.00%, Time: 0:00:46 \nIter:    150, Train Loss:    2.6, Train Acc:  62.50%, Time: 0:00:56 *\nIter:    180, Train Loss:    2.3, Train Acc:  68.75%, Time: 0:01:07 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:13 \nIter:    240, Train Loss:    1.8, Train Acc:  75.00%, Time: 0:01:25 *\nIter:    270, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:01:36 *\nIter:    300, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:01:48 *\nIter:    330, Train Loss:    1.8, Train Acc:  70.31%, Time: 0:01:54 \nIter:    360, Train Loss:    1.5, Train Acc:  75.00%, Time: 0:02:00 \nIter:    390, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:02:05 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:11 \nIter:    450, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:02:17 \nIter:    480, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:02:23 \nIter:    510, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:35 *\nIter:    540, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:40 \nIter:    570, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:02:46 \nIter:    600, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:02:56 *\nEpoch: 4\nIter:    630, Train Loss:   0.88, Train Acc:  90.62%, Time: 0:03:02 \nIter:    660, Train Loss:   0.88, Train Acc:  93.75%, Time: 0:03:14 *\nIter:    690, Train Loss:   0.96, Train Acc:  87.50%, Time: 0:03:19 \nIter:    720, Train Loss:   0.71, Train Acc:  90.62%, Time: 0:03:25 \nIter:    750, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:03:36 *\nIter:    780, Train Loss:   0.66, Train Acc:  89.06%, Time: 0:03:42 \nIter:    810, Train Loss:   0.79, Train Acc:  89.06%, Time: 0:03:47 \nEpoch: 5\nIter:    840, Train Loss:   0.54, Train Acc:  95.31%, Time: 0:03:53 \nIter:    870, Train Loss:    0.6, Train Acc:  87.50%, Time: 0:03:59 \nIter:    900, Train Loss:   0.79, Train Acc:  90.62%, Time: 0:04:05 \nIter:    930, Train Loss:   0.83, Train Acc:  85.94%, Time: 0:04:10 \nIter:    960, Train Loss:   0.58, Train Acc:  90.62%, Time: 0:04:16 \nIter:    990, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:04:22 \nEpoch: 6\nIter:   1020, Train Loss:   0.32, Train Acc:  98.44%, Time: 0:04:33 *\nIter:   1050, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:04:38 \nIter:   1080, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:04:44 \nIter:   1110, Train Loss:   0.46, Train Acc:  90.62%, Time: 0:04:50 \nIter:   1140, Train Loss:   0.28, Train Acc:  98.44%, Time: 0:04:56 \nIter:   1170, Train Loss:   0.46, Train Acc:  92.19%, Time: 0:05:02 \nIter:   1200, Train Loss:   0.46, Train Acc:  92.19%, Time: 0:05:07 \nEpoch: 7\nIter:   1230, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:05:13 \nIter:   1260, Train Loss:   0.66, Train Acc:  87.50%, Time: 0:05:19 \nIter:   1290, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:25 \nIter:   1320, Train Loss:   0.43, Train Acc:  93.75%, Time: 0:05:31 \nIter:   1350, Train Loss:   0.19, Train Acc:  98.44%, Time: 0:05:36 \nIter:   1380, Train Loss:   0.11, Train Acc: 100.00%, Time: 0:05:49 *\nIter:   1410, Train Loss:   0.52, Train Acc:  90.62%, Time: 0:05:55 \nEpoch: 8\nIter:   1440, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:01 \nIter:   1470, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:06:07 \nIter:   1500, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:12 \nIter:   1530, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:06:18 \nIter:   1560, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:06:24 \nIter:   1590, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:06:30 \nIter:   1620, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:36 \nEpoch: 9\nIter:   1650, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:06:41 \nIter:   1680, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:06:47 \nIter:   1710, Train Loss:   0.17, Train Acc: 100.00%, Time: 0:06:53 \nIter:   1740, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:06:59 \nIter:   1770, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:07:05 \nIter:   1800, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:10 \nEpoch: 10\nIter:   1830, Train Loss:   0.23, Train Acc: 100.00%, Time: 0:07:16 \nIter:   1860, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:07:22 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.3, Test Acc:  66.94%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.50      0.49      0.49       112\n                   Filter       0.64      0.50      0.56       136\n    Compute Derived Value       0.61      0.64      0.63       123\n            Find Extremum       0.92      0.50      0.64       115\n                     Sort       0.80      0.80      0.80       115\n          Determine Range       0.45      0.40      0.42       107\nCharacterize Distribution       0.71      0.75      0.73        89\n           Find Anomalies       0.66      0.91      0.76        97\n                  Cluster       0.78      0.85      0.81        98\n                Correlate       0.72      0.96      0.82       112\n\n                micro avg       0.67      0.67      0.67      1104\n                macro avg       0.68      0.68      0.67      1104\n             weighted avg       0.68      0.67      0.66      1104\n\nConfusion Matrix...\n[[ 55   4  30   1   3   8   3   4   0   4]\n [ 38  68   0   1   1  18   1   6   3   0]\n [  9  20  79   0   2   2   7   0   2   2]\n [  5   2   1  57   5  11   0  24   5   5]\n [  0   1   0   0  92   5   3   2   7   5]\n [  1   8  15   3  12  43   6   4   1  14]\n [  3   3   2   0   0   7  67   2   4   1]\n [  0   0   2   0   0   0   0  88   1   6]\n [  0   0   0   0   0   0   8   3  83   4]\n [  0   1   0   0   0   2   0   1   1 107]]\nTime usage: 0:00:05\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  12.50%, Time: 0:00:15 *\nIter:     60, Train Loss:    3.9, Train Acc:  29.69%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.6, Train Acc:  43.75%, Time: 0:00:37 *\nIter:    120, Train Loss:    2.7, Train Acc:  57.81%, Time: 0:00:47 *\nIter:    150, Train Loss:    2.7, Train Acc:  57.81%, Time: 0:00:53 \nIter:    180, Train Loss:    2.2, Train Acc:  73.44%, Time: 0:01:03 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  68.75%, Time: 0:01:09 \nIter:    240, Train Loss:    2.0, Train Acc:  68.75%, Time: 0:01:15 \nIter:    270, Train Loss:    1.8, Train Acc:  68.75%, Time: 0:01:21 \nIter:    300, Train Loss:    1.6, Train Acc:  81.25%, Time: 0:01:33 *\nIter:    330, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:39 \nIter:    360, Train Loss:    1.3, Train Acc:  84.38%, Time: 0:01:50 *\nIter:    390, Train Loss:    1.4, Train Acc:  73.44%, Time: 0:01:55 \nEpoch: 3\nIter:    420, Train Loss:    1.6, Train Acc:  75.00%, Time: 0:02:01 \nIter:    450, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:07 \nIter:    480, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:13 \nIter:    510, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:23 *\nIter:    540, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:02:29 \nIter:    570, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:34 \nIter:    600, Train Loss:   0.82, Train Acc:  89.06%, Time: 0:02:45 *\nEpoch: 4\nIter:    630, Train Loss:   0.84, Train Acc:  85.94%, Time: 0:02:51 \nIter:    660, Train Loss:   0.83, Train Acc:  89.06%, Time: 0:02:57 \nIter:    690, Train Loss:   0.85, Train Acc:  85.94%, Time: 0:03:02 \nIter:    720, Train Loss:    0.9, Train Acc:  82.81%, Time: 0:03:08 \nIter:    750, Train Loss:   0.94, Train Acc:  84.38%, Time: 0:03:14 \nIter:    780, Train Loss:   0.83, Train Acc:  87.50%, Time: 0:03:20 \nEpoch: 5\nIter:    810, Train Loss:   0.57, Train Acc:  92.19%, Time: 0:03:30 *\nIter:    840, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:03:36 \nIter:    870, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:03:47 *\nIter:    900, Train Loss:   0.61, Train Acc:  93.75%, Time: 0:03:53 \nIter:    930, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:03:59 \nIter:    960, Train Loss:   0.52, Train Acc:  92.19%, Time: 0:04:05 \nIter:    990, Train Loss:   0.75, Train Acc:  85.94%, Time: 0:04:10 \nEpoch: 6\nIter:   1020, Train Loss:   0.74, Train Acc:  90.62%, Time: 0:04:16 \nIter:   1050, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:04:22 \nIter:   1080, Train Loss:   0.69, Train Acc:  89.06%, Time: 0:04:28 \nIter:   1110, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:04:33 \nIter:   1140, Train Loss:   0.54, Train Acc:  93.75%, Time: 0:04:39 \nIter:   1170, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:04:51 *\nIter:   1200, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:04:57 \nEpoch: 7\nIter:   1230, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:05:03 \nIter:   1260, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:05:08 \nIter:   1290, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:05:14 \nIter:   1320, Train Loss:   0.84, Train Acc:  87.50%, Time: 0:05:20 \nIter:   1350, Train Loss:   0.33, Train Acc:  92.19%, Time: 0:05:26 \nIter:   1380, Train Loss:    0.7, Train Acc:  85.94%, Time: 0:05:31 \nEpoch: 8\nIter:   1410, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:05:37 \nIter:   1440, Train Loss:   0.24, Train Acc:  93.75%, Time: 0:05:43 \nIter:   1470, Train Loss:   0.16, Train Acc:  96.88%, Time: 0:05:49 \nIter:   1500, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:54 \nIter:   1530, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:06:00 \nIter:   1560, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:06 \nIter:   1590, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:06:17 *\nEpoch: 9\nIter:   1620, Train Loss:   0.65, Train Acc:  95.31%, Time: 0:06:22 \nIter:   1650, Train Loss:  0.086, Train Acc: 100.00%, Time: 0:06:34 *\nIter:   1680, Train Loss:  0.087, Train Acc: 100.00%, Time: 0:06:39 \nIter:   1710, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:06:45 \nIter:   1740, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:06:51 \nIter:   1770, Train Loss:   0.05, Train Acc: 100.00%, Time: 0:06:57 \nIter:   1800, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:07:02 \nEpoch: 10\nIter:   1830, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:07:08 \nIter:   1860, Train Loss:  0.098, Train Acc:  98.44%, Time: 0:07:14 \nIter:   1890, Train Loss:   0.31, Train Acc:  93.75%, Time: 0:07:20 \nIter:   1920, Train Loss:   0.35, Train Acc:  93.75%, Time: 0:07:26 \nIter:   1950, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:07:31 \nIter:   1980, Train Loss:   0.24, Train Acc:  95.31%, Time: 0:07:37 \nIter:   2010, Train Loss:  0.099, Train Acc: 100.00%, Time: 0:07:43 \nEpoch: 11\nIter:   2040, Train Loss:   0.35, Train Acc:  93.75%, Time: 0:07:49 \nIter:   2070, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:07:55 \nIter:   2100, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:08:01 \nIter:   2130, Train Loss:   0.08, Train Acc: 100.00%, Time: 0:08:07 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/6/6\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/6/6\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    4.2, Test Acc:  53.50%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.44      0.49      0.46       117\n                   Filter       0.35      0.23      0.28       126\n    Compute Derived Value       0.38      0.26      0.31       127\n            Find Extremum       0.39      0.85      0.53       122\n                     Sort       0.76      0.97      0.86       116\n          Determine Range       0.45      0.46      0.45       129\nCharacterize Distribution       0.61      0.60      0.60       121\n           Find Anomalies       0.64      0.52      0.57       123\n                  Cluster       0.76      0.59      0.66       120\n                Correlate       0.86      0.42      0.56       114\n\n                micro avg       0.53      0.53      0.53      1215\n                macro avg       0.56      0.54      0.53      1215\n             weighted avg       0.56      0.53      0.53      1215\n\nConfusion Matrix...\n[[ 57   2  10  22   0  10  15   0   1   0]\n [ 17  29  11  17   2  34   1  14   1   0]\n [ 23  16  33  37   0   2  13   1   1   1]\n [  8   7   0 104   2   1   0   0   0   0]\n [  0   3   0   0 113   0   0   0   0   0]\n [  0   7  21  25  14  59   0   1   0   2]\n [  9   3   6  15   3   7  72   5   1   0]\n [  3   3   1  20   3   4  14  64   9   2]\n [ 12   8   1   5   3  14   1   2  71   3]\n [  1   4   3  23   8   1   3  13  10  48]]\nTime usage: 0:00:05\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  10.94%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  23.44%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.3, Train Acc:  45.31%, Time: 0:00:37 *\nIter:    120, Train Loss:    2.8, Train Acc:  53.12%, Time: 0:00:48 *\nIter:    150, Train Loss:    2.6, Train Acc:  59.38%, Time: 0:00:58 *\nIter:    180, Train Loss:    2.5, Train Acc:  67.19%, Time: 0:01:10 *\nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  67.19%, Time: 0:01:16 \nIter:    240, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:26 *\nIter:    270, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:37 *\nIter:    300, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:01:43 \nIter:    330, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:01:49 \nIter:    360, Train Loss:    1.7, Train Acc:  78.12%, Time: 0:02:01 *\nIter:    390, Train Loss:    1.6, Train Acc:  77.08%, Time: 0:02:07 \nEpoch: 3\nIter:    420, Train Loss:    1.3, Train Acc:  76.56%, Time: 0:02:13 \nIter:    450, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:25 *\nIter:    480, Train Loss:   0.95, Train Acc:  82.81%, Time: 0:02:37 *\nIter:    510, Train Loss:    1.2, Train Acc:  82.81%, Time: 0:02:42 \nIter:    540, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:48 \nIter:    570, Train Loss:    1.1, Train Acc:  76.56%, Time: 0:02:54 \nEpoch: 4\nIter:    600, Train Loss:   0.74, Train Acc:  87.50%, Time: 0:03:06 *\nIter:    630, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:03:12 \nIter:    660, Train Loss:   0.65, Train Acc:  89.06%, Time: 0:03:23 *\nIter:    690, Train Loss:   0.92, Train Acc:  84.38%, Time: 0:03:29 \nIter:    720, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:35 \nIter:    750, Train Loss:   0.52, Train Acc:  92.19%, Time: 0:03:47 *\nIter:    780, Train Loss:   0.85, Train Acc:  87.50%, Time: 0:03:53 \nEpoch: 5\nIter:    810, Train Loss:   0.53, Train Acc:  96.88%, Time: 0:04:03 *\nIter:    840, Train Loss:    0.9, Train Acc:  89.06%, Time: 0:04:09 \nIter:    870, Train Loss:   0.47, Train Acc:  95.31%, Time: 0:04:15 \nIter:    900, Train Loss:   0.59, Train Acc:  93.75%, Time: 0:04:21 \nIter:    930, Train Loss:   0.83, Train Acc:  87.50%, Time: 0:04:26 \nIter:    960, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:04:32 \nEpoch: 6\nIter:    990, Train Loss:    0.5, Train Acc:  95.31%, Time: 0:04:38 \nIter:   1020, Train Loss:   0.51, Train Acc:  92.19%, Time: 0:04:44 \nIter:   1050, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:04:50 \nIter:   1080, Train Loss:   0.38, Train Acc:  96.88%, Time: 0:04:55 \nIter:   1110, Train Loss:   0.88, Train Acc:  89.06%, Time: 0:05:01 \nIter:   1140, Train Loss:   0.68, Train Acc:  87.50%, Time: 0:05:07 \nIter:   1170, Train Loss:   0.34, Train Acc:  95.83%, Time: 0:05:13 \nEpoch: 7\nIter:   1200, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:05:19 \nIter:   1230, Train Loss:   0.66, Train Acc:  90.62%, Time: 0:05:24 \nIter:   1260, Train Loss:   0.38, Train Acc:  90.62%, Time: 0:05:30 \nIter:   1290, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:05:36 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/7/7\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/7/7\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.5, Test Acc:  62.19%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.57      0.36      0.44       135\n                   Filter       0.62      0.67      0.64       183\n    Compute Derived Value       0.36      0.90      0.51       162\n            Find Extremum       0.88      0.47      0.61       215\n                     Sort       0.71      0.67      0.69       135\n          Determine Range       0.64      0.80      0.71       130\nCharacterize Distribution       0.86      0.74      0.80       135\n           Find Anomalies       0.69      0.53      0.60       137\n                  Cluster       0.78      0.75      0.77       159\n                Correlate       0.69      0.41      0.52       180\n\n                micro avg       0.62      0.62      0.62      1571\n                macro avg       0.68      0.63      0.63      1571\n             weighted avg       0.69      0.62      0.62      1571\n\nConfusion Matrix...\n[[ 48   3  71   5   0   6   0   1   0   1]\n [  7 123  29   0   2  12   0   5   1   4]\n [  2   1 145   3   0   2   5   1   3   0]\n [ 13  27  38 100  11  19   5   0   1   1]\n [ 10   6  13   0  91  11   1   1   2   0]\n [  2   7  10   3   1 104   1   1   0   1]\n [  1   0  15   0   2   0 100   4   4   9]\n [  0  16  16   0   1   4   0  73  11  16]\n [  0   9   9   0  19   0   0   2 119   1]\n [  1   7  57   2   1   5   4  18  11  74]]\nTime usage: 0:00:06\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  12.50%, Time: 0:00:16 *\nIter:     60, Train Loss:    3.9, Train Acc:  28.12%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.5, Train Acc:  42.19%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.1, Train Acc:  51.56%, Time: 0:00:52 *\nIter:    150, Train Loss:    2.6, Train Acc:  67.19%, Time: 0:01:03 *\nIter:    180, Train Loss:    2.6, Train Acc:  57.81%, Time: 0:01:09 \nEpoch: 2\nIter:    210, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:21 *\nIter:    240, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:26 \nIter:    270, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:32 \nIter:    300, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:01:44 *\nIter:    330, Train Loss:    1.4, Train Acc:  82.81%, Time: 0:01:55 *\nIter:    360, Train Loss:    1.7, Train Acc:  79.69%, Time: 0:02:01 \nIter:    390, Train Loss:    1.6, Train Acc:  78.12%, Time: 0:02:07 \nEpoch: 3\nIter:    420, Train Loss:    1.3, Train Acc:  78.12%, Time: 0:02:13 \nIter:    450, Train Loss:    1.4, Train Acc:  75.00%, Time: 0:02:18 \nIter:    480, Train Loss:    1.1, Train Acc:  87.50%, Time: 0:02:31 *\nIter:    510, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:37 \nIter:    540, Train Loss:    0.9, Train Acc:  84.38%, Time: 0:02:43 \nIter:    570, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:48 \nIter:    600, Train Loss:   0.91, Train Acc:  92.31%, Time: 0:03:01 *\nEpoch: 4\nIter:    630, Train Loss:    1.5, Train Acc:  68.75%, Time: 0:03:07 \nIter:    660, Train Loss:   0.83, Train Acc:  85.94%, Time: 0:03:13 \nIter:    690, Train Loss:   0.83, Train Acc:  92.19%, Time: 0:03:19 \nIter:    720, Train Loss:    1.1, Train Acc:  89.06%, Time: 0:03:25 \nIter:    750, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:03:30 \nIter:    780, Train Loss:    0.7, Train Acc:  90.62%, Time: 0:03:36 \nEpoch: 5\nIter:    810, Train Loss:   0.74, Train Acc:  87.50%, Time: 0:03:42 \nIter:    840, Train Loss:   0.61, Train Acc:  96.88%, Time: 0:03:53 *\nIter:    870, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:03:59 \nIter:    900, Train Loss:   0.81, Train Acc:  87.50%, Time: 0:04:05 \nIter:    930, Train Loss:   0.72, Train Acc:  87.50%, Time: 0:04:11 \nIter:    960, Train Loss:   0.84, Train Acc:  89.06%, Time: 0:04:16 \nIter:    990, Train Loss:   0.62, Train Acc:  93.75%, Time: 0:04:22 \nEpoch: 6\nIter:   1020, Train Loss:   0.57, Train Acc:  90.62%, Time: 0:04:28 \nIter:   1050, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:04:34 \nIter:   1080, Train Loss:   0.53, Train Acc:  89.06%, Time: 0:04:39 \nIter:   1110, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:04:45 \nIter:   1140, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:04:51 \nIter:   1170, Train Loss:   0.53, Train Acc:  93.75%, Time: 0:04:57 \nIter:   1200, Train Loss:   0.55, Train Acc:  87.18%, Time: 0:05:02 \nEpoch: 7\nIter:   1230, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:05:08 \nIter:   1260, Train Loss:   0.43, Train Acc:  93.75%, Time: 0:05:14 \nIter:   1290, Train Loss:   0.23, Train Acc:  98.44%, Time: 0:05:25 *\nIter:   1320, Train Loss:   0.39, Train Acc:  90.62%, Time: 0:05:31 \nIter:   1350, Train Loss:    0.6, Train Acc:  90.62%, Time: 0:05:37 \nIter:   1380, Train Loss:   0.17, Train Acc:  96.88%, Time: 0:05:43 \nEpoch: 8\nIter:   1410, Train Loss:   0.25, Train Acc:  95.31%, Time: 0:05:48 \nIter:   1440, Train Loss:   0.44, Train Acc:  95.31%, Time: 0:05:54 \nIter:   1470, Train Loss:   0.35, Train Acc:  92.19%, Time: 0:06:00 \nIter:   1500, Train Loss:   0.42, Train Acc:  92.19%, Time: 0:06:06 \nIter:   1530, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:06:12 \nIter:   1560, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:06:18 \nIter:   1590, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:06:23 \nEpoch: 9\nIter:   1620, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:06:29 \nIter:   1650, Train Loss:   0.19, Train Acc: 100.00%, Time: 0:06:41 *\nIter:   1680, Train Loss:   0.36, Train Acc:  96.88%, Time: 0:06:46 \nIter:   1710, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:06:52 \nIter:   1740, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:06:58 \nIter:   1770, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:07:04 \nIter:   1800, Train Loss:   0.49, Train Acc:  97.44%, Time: 0:07:09 \nEpoch: 10\nIter:   1830, Train Loss:   0.21, Train Acc:  95.31%, Time: 0:07:15 \nIter:   1860, Train Loss:   0.11, Train Acc: 100.00%, Time: 0:07:21 \nIter:   1890, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:07:27 \nIter:   1920, Train Loss:   0.23, Train Acc:  95.31%, Time: 0:07:33 \nIter:   1950, Train Loss:   0.16, Train Acc:  95.31%, Time: 0:07:38 \nIter:   1980, Train Loss:  0.087, Train Acc: 100.00%, Time: 0:07:44 \nEpoch: 11\nIter:   2010, Train Loss:  0.076, Train Acc: 100.00%, Time: 0:07:50 \nIter:   2040, Train Loss:  0.088, Train Acc:  98.44%, Time: 0:07:56 \nIter:   2070, Train Loss:   0.13, Train Acc:  96.88%, Time: 0:08:02 \nIter:   2100, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:08:07 \nIter:   2130, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:08:13 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/8/8\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/8/8\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.0, Test Acc:  74.44%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.66      0.80      0.72       125\n                   Filter       0.77      0.70      0.73       125\n    Compute Derived Value       0.66      0.74      0.70       116\n            Find Extremum       0.82      0.73      0.77       122\n                     Sort       0.79      0.71      0.75       117\n          Determine Range       0.69      0.84      0.76       124\nCharacterize Distribution       0.90      0.76      0.82       148\n           Find Anomalies       0.66      0.86      0.75       134\n                  Cluster       0.79      0.54      0.64       125\n                Correlate       0.80      0.77      0.78       124\n\n                micro avg       0.74      0.74      0.74      1260\n                macro avg       0.75      0.74      0.74      1260\n             weighted avg       0.76      0.74      0.74      1260\n\nConfusion Matrix...\n[[100   2   9   0   0  11   0   3   0   0]\n [ 17  87   0   0   2   4   0   7   8   0]\n [ 18   0  86   1   1   5   0   5   0   0]\n [  6   6   5  89   0   4   0  12   0   0]\n [  1   2   2  15  83   4   2   7   1   0]\n [  1   1   7   3   1 104   2   0   1   4]\n [  1   9   3   0   1   2 112   5   5  10]\n [  1   2   1   0   1   8   1 115   2   3]\n [  3   3  13   1   8   9   7   7  67   7]\n [  3   1   4   0   8   0   0  12   1  95]]\nTime usage: 0:00:05\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  18.75%, Time: 0:00:14 *\nIter:     60, Train Loss:    3.9, Train Acc:  39.06%, Time: 0:00:24 *\nIter:     90, Train Loss:    3.3, Train Acc:  46.88%, Time: 0:00:35 *\nIter:    120, Train Loss:    3.0, Train Acc:  57.81%, Time: 0:00:45 *\nIter:    150, Train Loss:    2.9, Train Acc:  46.88%, Time: 0:00:51 \nIter:    180, Train Loss:    2.6, Train Acc:  56.25%, Time: 0:00:58 \nEpoch: 2\nIter:    210, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:08 *\nIter:    240, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:19 *\nIter:    270, Train Loss:    1.9, Train Acc:  65.62%, Time: 0:01:25 \nIter:    300, Train Loss:    2.0, Train Acc:  68.75%, Time: 0:01:31 \nIter:    330, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:01:41 *\nIter:    360, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:47 \nIter:    390, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:53 \nEpoch: 3\nIter:    420, Train Loss:    1.5, Train Acc:  71.88%, Time: 0:01:59 \nIter:    450, Train Loss:   0.93, Train Acc:  89.06%, Time: 0:02:09 *\nIter:    480, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:15 \nIter:    510, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:02:21 \nIter:    540, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:02:27 \nIter:    570, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:33 \nEpoch: 4\nIter:    600, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:38 \nIter:    630, Train Loss:   0.96, Train Acc:  87.50%, Time: 0:02:44 \nIter:    660, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:50 \nIter:    690, Train Loss:   0.91, Train Acc:  87.50%, Time: 0:02:56 \nIter:    720, Train Loss:   0.57, Train Acc:  92.19%, Time: 0:03:08 *\nIter:    750, Train Loss:    1.1, Train Acc:  78.12%, Time: 0:03:13 \nIter:    780, Train Loss:   0.75, Train Acc:  85.94%, Time: 0:03:19 \nEpoch: 5\nIter:    810, Train Loss:    0.8, Train Acc:  89.06%, Time: 0:03:25 \nIter:    840, Train Loss:   0.71, Train Acc:  85.94%, Time: 0:03:31 \nIter:    870, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:03:37 \nIter:    900, Train Loss:   0.65, Train Acc:  89.06%, Time: 0:03:43 \nIter:    930, Train Loss:   0.78, Train Acc:  87.50%, Time: 0:03:49 \nIter:    960, Train Loss:   0.98, Train Acc:  87.50%, Time: 0:03:54 \nIter:    990, Train Loss:   0.88, Train Acc:  89.06%, Time: 0:04:00 \nEpoch: 6\nIter:   1020, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:04:06 \nIter:   1050, Train Loss:   0.77, Train Acc:  92.19%, Time: 0:04:12 \nIter:   1080, Train Loss:   0.76, Train Acc:  84.38%, Time: 0:04:18 \nIter:   1110, Train Loss:   0.61, Train Acc:  89.06%, Time: 0:04:23 \nIter:   1140, Train Loss:   0.73, Train Acc:  85.94%, Time: 0:04:29 \nIter:   1170, Train Loss:   0.46, Train Acc:  90.62%, Time: 0:04:35 \nEpoch: 7\nIter:   1200, Train Loss:   0.27, Train Acc: 100.00%, Time: 0:04:45 *\nIter:   1230, Train Loss:   0.49, Train Acc:  90.62%, Time: 0:04:51 \nIter:   1260, Train Loss:   0.53, Train Acc:  92.19%, Time: 0:04:57 \nIter:   1290, Train Loss:   0.15, Train Acc:  98.44%, Time: 0:05:03 \nIter:   1320, Train Loss:   0.51, Train Acc:  90.62%, Time: 0:05:09 \nIter:   1350, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:05:15 \nIter:   1380, Train Loss:   0.34, Train Acc:  93.75%, Time: 0:05:21 \nEpoch: 8\nIter:   1410, Train Loss:   0.67, Train Acc:  87.50%, Time: 0:05:27 \nIter:   1440, Train Loss:   0.28, Train Acc:  92.19%, Time: 0:05:33 \nIter:   1470, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:05:39 \nIter:   1500, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:05:45 \nIter:   1530, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:05:52 \nIter:   1560, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:05:58 \nIter:   1590, Train Loss:   0.11, Train Acc: 100.00%, Time: 0:06:04 \nEpoch: 9\nIter:   1620, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:06:09 \nIter:   1650, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:06:15 \nIter:   1680, Train Loss:    0.2, Train Acc:  95.31%, Time: 0:06:21 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/9/9\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/9/9\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.4, Test Acc:  66.95%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.86      0.82      0.84       113\n                   Filter       0.68      0.61      0.64       137\n    Compute Derived Value       0.59      0.54      0.57       127\n            Find Extremum       0.58      0.80      0.67       127\n                     Sort       0.74      0.75      0.75       116\n          Determine Range       0.70      0.70      0.70       123\nCharacterize Distribution       0.80      0.62      0.70       130\n           Find Anomalies       0.62      0.62      0.62       143\n                  Cluster       0.59      0.81      0.68       139\n                Correlate       0.68      0.50      0.58       161\n\n                micro avg       0.67      0.67      0.67      1316\n                macro avg       0.68      0.68      0.67      1316\n             weighted avg       0.68      0.67      0.67      1316\n\nConfusion Matrix...\n[[ 93   2  13   3   0   1   0   0   1   0]\n [  2  83   6  16  12  12   0   5   1   0]\n [ 10   0  69  13   6  13   2   7   0   7]\n [  0   3  10 101   4   0   0   1   8   0]\n [  0   1   2  11  87   4   1   0   9   1]\n [  0   1   3  16   6  86   1   4   6   0]\n [  1  19   1   1   0   0  81   3  23   1]\n [  2  12   5   6   0   1   0  88   4  25]\n [  0   0   0   2   2   2   0  16 112   5]\n [  0   1   8   5   0   4  16  19  27  81]]\nTime usage: 0:00:05\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.6, Train Acc:  14.06%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.0, Train Acc:  34.38%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.5, Train Acc:  39.06%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.0, Train Acc:  51.56%, Time: 0:00:51 *\nIter:    150, Train Loss:    2.7, Train Acc:  53.12%, Time: 0:01:02 *\nIter:    180, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:12 *\nEpoch: 2\nIter:    210, Train Loss:    2.3, Train Acc:  64.06%, Time: 0:01:18 \nIter:    240, Train Loss:    2.0, Train Acc:  68.75%, Time: 0:01:29 *\nIter:    270, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:41 *\nIter:    300, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:47 \nIter:    330, Train Loss:    1.5, Train Acc:  75.00%, Time: 0:01:58 *\nIter:    360, Train Loss:    1.2, Train Acc:  87.50%, Time: 0:02:08 *\nEpoch: 3\nIter:    390, Train Loss:    1.6, Train Acc:  70.31%, Time: 0:02:14 \nIter:    420, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:20 \nIter:    450, Train Loss:   0.81, Train Acc:  87.50%, Time: 0:02:26 \nIter:    480, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:31 \nIter:    510, Train Loss:   0.96, Train Acc:  87.50%, Time: 0:02:37 \nIter:    540, Train Loss:    0.9, Train Acc:  90.62%, Time: 0:02:49 *\nIter:    570, Train Loss:    1.2, Train Acc:  85.94%, Time: 0:02:55 \nEpoch: 4\nIter:    600, Train Loss:    1.0, Train Acc:  81.25%, Time: 0:03:00 \nIter:    630, Train Loss:    1.0, Train Acc:  89.06%, Time: 0:03:06 \nIter:    660, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:12 \nIter:    690, Train Loss:    1.4, Train Acc:  87.50%, Time: 0:03:18 \nIter:    720, Train Loss:   0.61, Train Acc:  89.06%, Time: 0:03:24 \nIter:    750, Train Loss:   0.62, Train Acc:  92.19%, Time: 0:03:36 *\nEpoch: 5\nIter:    780, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:03:48 *\nIter:    810, Train Loss:   0.57, Train Acc:  92.19%, Time: 0:03:54 \nIter:    840, Train Loss:    0.8, Train Acc:  87.50%, Time: 0:03:59 \nIter:    870, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:04:05 \nIter:    900, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:04:11 \nIter:    930, Train Loss:   0.86, Train Acc:  84.38%, Time: 0:04:17 \nIter:    960, Train Loss:   0.59, Train Acc:  89.06%, Time: 0:04:23 \nEpoch: 6\nIter:    990, Train Loss:   0.44, Train Acc:  95.31%, Time: 0:04:28 \nIter:   1020, Train Loss:   0.61, Train Acc:  93.75%, Time: 0:04:34 \nIter:   1050, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:04:46 *\nIter:   1080, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:04:52 \nIter:   1110, Train Loss:   0.56, Train Acc:  93.75%, Time: 0:04:58 \nIter:   1140, Train Loss:    0.7, Train Acc:  89.06%, Time: 0:05:05 \nEpoch: 7\nIter:   1170, Train Loss:   0.55, Train Acc:  92.19%, Time: 0:05:11 \nIter:   1200, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:05:17 \nIter:   1230, Train Loss:   0.36, Train Acc:  96.88%, Time: 0:05:23 \nIter:   1260, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:05:29 \nIter:   1290, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:36 \nIter:   1320, Train Loss:   0.39, Train Acc:  93.75%, Time: 0:05:42 \nIter:   1350, Train Loss:   0.54, Train Acc:  93.75%, Time: 0:05:48 \nEpoch: 8\nIter:   1380, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:05:53 \nIter:   1410, Train Loss:   0.31, Train Acc:  93.75%, Time: 0:05:59 \nIter:   1440, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:06:12 *\nIter:   1470, Train Loss:   0.32, Train Acc:  93.75%, Time: 0:06:18 \nIter:   1500, Train Loss:   0.27, Train Acc:  93.75%, Time: 0:06:23 \nIter:   1530, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:06:29 \nEpoch: 9\nIter:   1560, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:06:35 \nIter:   1590, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:06:41 \nIter:   1620, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:06:46 \nIter:   1650, Train Loss:   0.09, Train Acc: 100.00%, Time: 0:07:00 *\nIter:   1680, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:05 \nIter:   1710, Train Loss:   0.14, Train Acc: 100.00%, Time: 0:07:11 \nEpoch: 10\nIter:   1740, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:07:17 \nIter:   1770, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:07:23 \nIter:   1800, Train Loss:  0.088, Train Acc: 100.00%, Time: 0:07:29 \nIter:   1830, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:07:34 \nIter:   1860, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:07:40 \nIter:   1890, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:07:46 \nIter:   1920, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:07:52 \nEpoch: 11\nIter:   1950, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:58 \nIter:   1980, Train Loss:   0.32, Train Acc:  98.44%, Time: 0:08:03 \nIter:   2010, Train Loss:   0.43, Train Acc:  95.31%, Time: 0:08:09 \nIter:   2040, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:08:15 \nIter:   2070, Train Loss:  0.082, Train Acc: 100.00%, Time: 0:08:21 \nIter:   2100, Train Loss:   0.12, Train Acc: 100.00%, Time: 0:08:27 \nEpoch: 12\nIter:   2130, Train Loss:  0.049, Train Acc: 100.00%, Time: 0:08:33 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/10/10\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/expert/10/10\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.4, Test Acc:  59.65%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.56      0.59      0.58       185\n                   Filter       0.47      0.26      0.34       171\n    Compute Derived Value       0.40      0.44      0.42       248\n            Find Extremum       0.64      0.78      0.70       185\n                     Sort       0.64      0.64      0.64       117\n          Determine Range       0.80      0.66      0.72       207\nCharacterize Distribution       0.72      0.80      0.76       117\n           Find Anomalies       0.42      0.68      0.52       165\n                  Cluster       0.90      0.70      0.79       138\n                Correlate       0.73      0.55      0.62       192\n\n                micro avg       0.60      0.60      0.60      1725\n                macro avg       0.63      0.61      0.61      1725\n             weighted avg       0.62      0.60      0.60      1725\n\nConfusion Matrix...\n[[110  15  17   8  11   0   4  13   6   1]\n [ 32  45  49   4   6   2   0  30   0   3]\n [ 16   0 109  38   2  22  15  32   2  12]\n [  5   0  29 145   1   0   0   5   0   0]\n [  0   9   3   9  75   1   3  13   0   4]\n [ 12   3  19  13  15 136   3   4   1   1]\n [  9   0   2   1   3   5  94   0   2   1]\n [  0  19  10   7   0   0   3 113   0  13]\n [  1   1   2   1   4   3   7  18  97   4]\n [ 11   3  30   1   0   0   2  40   0 105]]\nTime usage: 0:00:06\n[0.6697761194029851, 0.6126482216467769, 0.6004842613568895, 0.6663947799660837, 0.6693840579710145, 0.5349794251438031, 0.6218968815745858, 0.7444444442552234, 0.6694528882626705, 0.5965217395796292]\n0.6385982819159661, 0.05455291700225951, 0.057503823577755, 0.0029760207534554146\nbundle\n920 14 17 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  18.75%, Time: 0:00:14 *\nIter:     60, Train Loss:    4.1, Train Acc:  29.69%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.4, Train Acc:  45.31%, Time: 0:00:35 *\nIter:    120, Train Loss:    2.8, Train Acc:  53.12%, Time: 0:00:46 *\nIter:    150, Train Loss:    2.9, Train Acc:  51.56%, Time: 0:00:52 \nIter:    180, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:02 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:08 \nIter:    240, Train Loss:    2.0, Train Acc:  71.88%, Time: 0:01:19 *\nIter:    270, Train Loss:    1.9, Train Acc:  76.56%, Time: 0:01:30 *\nIter:    300, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:01:36 \nIter:    330, Train Loss:    1.1, Train Acc:  78.12%, Time: 0:01:48 *\nIter:    360, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:01:59 *\nIter:    390, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:02:04 \nEpoch: 3\nIter:    420, Train Loss:    1.9, Train Acc:  73.44%, Time: 0:02:10 \nIter:    450, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:16 \nIter:    480, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:22 \nIter:    510, Train Loss:    1.5, Train Acc:  71.88%, Time: 0:02:27 \nIter:    540, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:02:33 \nIter:    570, Train Loss:   0.81, Train Acc:  92.19%, Time: 0:02:45 *\nEpoch: 4\nIter:    600, Train Loss:    1.1, Train Acc:  79.69%, Time: 0:02:51 \nIter:    630, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:56 \nIter:    660, Train Loss:   0.65, Train Acc:  93.75%, Time: 0:03:08 *\nIter:    690, Train Loss:    1.0, Train Acc:  79.69%, Time: 0:03:14 \nIter:    720, Train Loss:   0.84, Train Acc:  89.06%, Time: 0:03:20 \nIter:    750, Train Loss:   0.83, Train Acc:  87.50%, Time: 0:03:26 \nIter:    780, Train Loss:   0.66, Train Acc:  90.62%, Time: 0:03:31 \nEpoch: 5\nIter:    810, Train Loss:   0.75, Train Acc:  90.62%, Time: 0:03:37 \nIter:    840, Train Loss:    0.5, Train Acc:  95.31%, Time: 0:03:49 *\nIter:    870, Train Loss:   0.93, Train Acc:  82.81%, Time: 0:03:55 \nIter:    900, Train Loss:    0.3, Train Acc:  98.44%, Time: 0:04:06 *\nIter:    930, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:04:12 \nIter:    960, Train Loss:   0.46, Train Acc:  96.88%, Time: 0:04:18 \nIter:    990, Train Loss:   0.55, Train Acc:  86.67%, Time: 0:04:23 \nEpoch: 6\nIter:   1020, Train Loss:   0.55, Train Acc:  95.31%, Time: 0:04:29 \nIter:   1050, Train Loss:   0.47, Train Acc:  93.75%, Time: 0:04:35 \nIter:   1080, Train Loss:   0.89, Train Acc:  89.06%, Time: 0:04:41 \nIter:   1110, Train Loss:   0.66, Train Acc:  85.94%, Time: 0:04:47 \nIter:   1140, Train Loss:   0.67, Train Acc:  85.94%, Time: 0:04:53 \nIter:   1170, Train Loss:   0.61, Train Acc:  92.19%, Time: 0:04:58 \nEpoch: 7\nIter:   1200, Train Loss:   0.36, Train Acc:  92.19%, Time: 0:05:04 \nIter:   1230, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:05:10 \nIter:   1260, Train Loss:   0.71, Train Acc:  90.62%, Time: 0:05:16 \nIter:   1290, Train Loss:   0.41, Train Acc:  96.88%, Time: 0:05:21 \nIter:   1320, Train Loss:   0.45, Train Acc:  95.31%, Time: 0:05:27 \nIter:   1350, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:05:33 \nIter:   1380, Train Loss:   0.43, Train Acc:  90.62%, Time: 0:05:39 \nEpoch: 8\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/1/1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/1/1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.1, Test Acc:  72.03%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.68      0.71      0.70       149\n                   Filter       0.83      0.76      0.79       153\n    Compute Derived Value       0.48      0.58      0.53       103\n            Find Extremum       0.89      0.80      0.84       237\n                     Sort       0.58      0.97      0.73        61\n          Determine Range       0.79      0.65      0.71        94\nCharacterize Distribution       0.92      0.62      0.74       228\n           Find Anomalies       0.69      0.70      0.70       130\n                  Cluster       0.61      0.74      0.66       152\n                Correlate       0.61      0.77      0.68       105\n\n                micro avg       0.72      0.72      0.72      1412\n                macro avg       0.71      0.73      0.71      1412\n             weighted avg       0.75      0.72      0.72      1412\n\nConfusion Matrix...\n[[106   5  25   8   0   3   0   2   0   0]\n [ 21 116   5   0   0   5   0   5   1   0]\n [  6   7  60  10   1   0   0   9   5   5]\n [  0   5   3 190  12   6   0  10   5   6]\n [  0   1   0   0  59   0   0   0   1   0]\n [  4   0  10   0   4  61   8   3   3   1]\n [ 14   0  10   0   2   2 141   3  32  24]\n [  2   4   3   2   0   0   2  91  16  10]\n [  0   1   8   4  18   0   1   3 112   5]\n [  2   0   1   0   5   0   1   5  10  81]]\nTime usage: 0:00:05\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.3, Train Acc:  23.44%, Time: 0:00:16 *\nIter:     60, Train Loss:    3.9, Train Acc:  40.62%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.3, Train Acc:  46.88%, Time: 0:00:38 *\nIter:    120, Train Loss:    2.9, Train Acc:  50.00%, Time: 0:00:50 *\nIter:    150, Train Loss:    2.3, Train Acc:  67.19%, Time: 0:01:01 *\nIter:    180, Train Loss:    2.1, Train Acc:  68.75%, Time: 0:01:12 *\nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  67.19%, Time: 0:01:18 \nIter:    240, Train Loss:    1.8, Train Acc:  75.00%, Time: 0:01:29 *\nIter:    270, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:39 *\nIter:    300, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:45 \nIter:    330, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:01:51 \nIter:    360, Train Loss:    1.6, Train Acc:  76.56%, Time: 0:01:57 \nIter:    390, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:02:03 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  73.44%, Time: 0:02:08 \nIter:    450, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:14 \nIter:    480, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:20 \nIter:    510, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:31 *\nIter:    540, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:36 \nIter:    570, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:42 \nEpoch: 4\nIter:    600, Train Loss:   0.87, Train Acc:  85.94%, Time: 0:02:53 *\nIter:    630, Train Loss:   0.69, Train Acc:  84.38%, Time: 0:02:59 \nIter:    660, Train Loss:    1.1, Train Acc:  79.69%, Time: 0:03:05 \nIter:    690, Train Loss:   0.88, Train Acc:  85.94%, Time: 0:03:10 \nIter:    720, Train Loss:   0.97, Train Acc:  89.06%, Time: 0:03:21 *\nIter:    750, Train Loss:   0.87, Train Acc:  85.94%, Time: 0:03:27 \nIter:    780, Train Loss:   0.93, Train Acc:  87.50%, Time: 0:03:33 \nEpoch: 5\nIter:    810, Train Loss:   0.63, Train Acc:  92.19%, Time: 0:03:43 *\nIter:    840, Train Loss:    0.8, Train Acc:  87.50%, Time: 0:03:49 \nIter:    870, Train Loss:   0.63, Train Acc:  90.62%, Time: 0:03:55 \nIter:    900, Train Loss:   0.88, Train Acc:  85.94%, Time: 0:04:01 \nIter:    930, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:04:13 *\nIter:    960, Train Loss:   0.86, Train Acc:  89.06%, Time: 0:04:18 \nIter:    990, Train Loss:    0.4, Train Acc:  95.83%, Time: 0:04:30 *\nEpoch: 6\nIter:   1020, Train Loss:   0.28, Train Acc:  95.31%, Time: 0:04:35 \nIter:   1050, Train Loss:   0.47, Train Acc:  95.31%, Time: 0:04:41 \nIter:   1080, Train Loss:   0.65, Train Acc:  89.06%, Time: 0:04:47 \nIter:   1110, Train Loss:   0.51, Train Acc:  92.19%, Time: 0:04:53 \nIter:   1140, Train Loss:   0.53, Train Acc:  89.06%, Time: 0:04:59 \nIter:   1170, Train Loss:   0.41, Train Acc:  93.75%, Time: 0:05:04 \nEpoch: 7\nIter:   1200, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:05:15 *\nIter:   1230, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:05:21 \nIter:   1260, Train Loss:   0.39, Train Acc:  96.88%, Time: 0:05:27 \nIter:   1290, Train Loss:    0.3, Train Acc:  93.75%, Time: 0:05:32 \nIter:   1320, Train Loss:   0.68, Train Acc:  92.19%, Time: 0:05:38 \nIter:   1350, Train Loss:   0.29, Train Acc:  98.44%, Time: 0:05:49 *\nIter:   1380, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:05:54 \nEpoch: 8\nIter:   1410, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:06:00 \nIter:   1440, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:06:06 \nIter:   1470, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:06:12 \nIter:   1500, Train Loss:   0.22, Train Acc:  95.31%, Time: 0:06:17 \nIter:   1530, Train Loss:   0.12, Train Acc: 100.00%, Time: 0:06:28 *\nIter:   1560, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:06:33 \nEpoch: 9\nIter:   1590, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:06:39 \nIter:   1620, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:06:45 \nIter:   1650, Train Loss:    0.2, Train Acc:  95.31%, Time: 0:06:51 \nIter:   1680, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:06:57 \nIter:   1710, Train Loss:   0.37, Train Acc:  95.31%, Time: 0:07:03 \nIter:   1740, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:09 \nIter:   1770, Train Loss:   0.17, Train Acc:  98.44%, Time: 0:07:16 \nEpoch: 10\nIter:   1800, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:07:22 \nIter:   1830, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:07:28 \nIter:   1860, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:07:34 \nIter:   1890, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:07:40 \nIter:   1920, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:07:46 \nIter:   1950, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:07:52 \nIter:   1980, Train Loss:   0.29, Train Acc:  95.83%, Time: 0:07:58 \nEpoch: 11\nIter:   2010, Train Loss:    0.1, Train Acc:  98.44%, Time: 0:08:04 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/2/2\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/2/2\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.0, Test Acc:  61.78%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.68      0.44      0.54       145\n                   Filter       0.36      0.71      0.48       112\n    Compute Derived Value       0.42      0.47      0.44       104\n            Find Extremum       0.76      0.56      0.64       183\n                     Sort       0.93      0.65      0.77       185\n          Determine Range       0.59      0.54      0.56       158\nCharacterize Distribution       0.77      0.80      0.79       158\n           Find Anomalies       0.79      0.64      0.71       172\n                  Cluster       0.44      0.59      0.50        93\n                Correlate       0.55      0.86      0.67        69\n\n                micro avg       0.62      0.62      0.62      1379\n                macro avg       0.63      0.63      0.61      1379\n             weighted avg       0.67      0.62      0.63      1379\n\nConfusion Matrix...\n[[ 64  39  22   4   2   4   9   0   0   1]\n [  1  80  10   3   0  11   0   6   1   0]\n [  2  11  49   3   0   1  16   3   1  18]\n [ 15  18  14 102   0  24   6   1   1   2]\n [  5   9   5  11 121   7   0   1  24   2]\n [  6  15   9   7   5  85   3   9  16   3]\n [  1  14   3   0   0   5 127   0   6   2]\n [  0  15   4   5   0   6   3 110  20   9]\n [  0  18   0   0   1   1   0   6  55  12]\n [  0   3   2   0   1   0   0   3   1  59]]\nTime usage: 0:00:05\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.7, Train Acc:   7.81%, Time: 0:00:15 *\nIter:     60, Train Loss:    3.9, Train Acc:  34.38%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.3, Train Acc:  50.00%, Time: 0:00:35 *\nIter:    120, Train Loss:    3.1, Train Acc:  48.44%, Time: 0:00:41 \nIter:    150, Train Loss:    2.2, Train Acc:  73.44%, Time: 0:00:51 *\nIter:    180, Train Loss:    2.7, Train Acc:  59.38%, Time: 0:00:57 \nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  71.88%, Time: 0:01:03 \nIter:    240, Train Loss:    2.4, Train Acc:  65.62%, Time: 0:01:09 \nIter:    270, Train Loss:    1.9, Train Acc:  62.50%, Time: 0:01:15 \nIter:    300, Train Loss:    1.8, Train Acc:  73.44%, Time: 0:01:20 \nIter:    330, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:01:31 *\nIter:    360, Train Loss:    2.0, Train Acc:  68.75%, Time: 0:01:37 \nIter:    390, Train Loss:    1.8, Train Acc:  67.19%, Time: 0:01:42 \nEpoch: 3\nIter:    420, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:01:53 *\nIter:    450, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:01:59 \nIter:    480, Train Loss:    1.4, Train Acc:  76.56%, Time: 0:02:05 \nIter:    510, Train Loss:   0.96, Train Acc:  85.94%, Time: 0:02:11 \nIter:    540, Train Loss:   0.88, Train Acc:  84.38%, Time: 0:02:17 \nIter:    570, Train Loss:   0.82, Train Acc:  92.19%, Time: 0:02:28 *\nEpoch: 4\nIter:    600, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:02:34 \nIter:    630, Train Loss:   0.96, Train Acc:  84.38%, Time: 0:02:41 \nIter:    660, Train Loss:   0.41, Train Acc:  98.44%, Time: 0:02:51 *\nIter:    690, Train Loss:    0.8, Train Acc:  87.50%, Time: 0:02:58 \nIter:    720, Train Loss:   0.45, Train Acc:  95.31%, Time: 0:03:04 \nIter:    750, Train Loss:   0.97, Train Acc:  89.06%, Time: 0:03:11 \nIter:    780, Train Loss:   0.52, Train Acc:  90.62%, Time: 0:03:17 \nEpoch: 5\nIter:    810, Train Loss:   0.93, Train Acc:  87.50%, Time: 0:03:23 \nIter:    840, Train Loss:   0.61, Train Acc:  89.06%, Time: 0:03:30 \nIter:    870, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:03:36 \nIter:    900, Train Loss:   0.78, Train Acc:  85.94%, Time: 0:03:43 \nIter:    930, Train Loss:   0.37, Train Acc:  96.88%, Time: 0:03:49 \nIter:    960, Train Loss:   0.79, Train Acc:  89.06%, Time: 0:03:56 \nIter:    990, Train Loss:   0.21, Train Acc: 100.00%, Time: 0:04:08 *\nEpoch: 6\nIter:   1020, Train Loss:   0.42, Train Acc:  93.75%, Time: 0:04:15 \nIter:   1050, Train Loss:   0.59, Train Acc:  93.75%, Time: 0:04:21 \nIter:   1080, Train Loss:   0.81, Train Acc:  89.06%, Time: 0:04:28 \nIter:   1110, Train Loss:   0.59, Train Acc:  90.62%, Time: 0:04:34 \nIter:   1140, Train Loss:   0.49, Train Acc:  89.06%, Time: 0:04:41 \nIter:   1170, Train Loss:   0.33, Train Acc:  93.75%, Time: 0:04:48 \nEpoch: 7\nIter:   1200, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:04:54 \nIter:   1230, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:05:00 \nIter:   1260, Train Loss:   0.38, Train Acc:  92.19%, Time: 0:05:07 \nIter:   1290, Train Loss:   0.42, Train Acc:  95.31%, Time: 0:05:13 \nIter:   1320, Train Loss:    0.4, Train Acc:  96.88%, Time: 0:05:20 \nIter:   1350, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:05:26 \nIter:   1380, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:05:33 \nEpoch: 8\nIter:   1410, Train Loss:   0.35, Train Acc:  92.19%, Time: 0:05:39 \nIter:   1440, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:46 \nIter:   1470, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:05:52 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/3/3\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/3/3\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.4, Test Acc:  64.36%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.51      0.68      0.58        87\n                   Filter       0.62      0.56      0.59       157\n    Compute Derived Value       0.74      0.60      0.66       245\n            Find Extremum       0.67      0.82      0.74       165\n                     Sort       0.79      0.70      0.74       105\n          Determine Range       0.67      0.60      0.63       184\nCharacterize Distribution       0.54      0.82      0.65       110\n           Find Anomalies       0.57      0.71      0.63        96\n                  Cluster       0.50      0.54      0.52        85\n                Correlate       0.78      0.53      0.63       180\n\n                micro avg       0.64      0.64      0.64      1414\n                macro avg       0.64      0.65      0.64      1414\n             weighted avg       0.66      0.64      0.64      1414\n\nConfusion Matrix...\n[[ 59   1  21   2   0   1   0   3   0   0]\n [  6  88   7  11  11   4   0  13  15   2]\n [ 14  28 146   1   3  30  14   2   2   5]\n [ 13   2   1 135   0  10   0   3   0   1]\n [  1   4   0   5  73   4   4   1  13   0]\n [  7   4  15  24   4 110  18   2   0   0]\n [  1   2   1   1   1   2  90   2   9   1]\n [  5   3   1   2   0   1   8  68   3   5]\n [  3   1   2   2   0   1  11   6  46  13]\n [  6  10   3  19   0   1  23  19   4  95]]\nTime usage: 0:00:06\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.6, Train Acc:  14.06%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.3, Train Acc:  25.00%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.6, Train Acc:  43.75%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.1, Train Acc:  48.44%, Time: 0:00:51 *\nIter:    150, Train Loss:    2.7, Train Acc:  60.94%, Time: 0:01:02 *\nIter:    180, Train Loss:    2.0, Train Acc:  65.62%, Time: 0:01:13 *\nEpoch: 2\nIter:    210, Train Loss:    1.8, Train Acc:  68.75%, Time: 0:01:24 *\nIter:    240, Train Loss:    1.9, Train Acc:  65.62%, Time: 0:01:30 \nIter:    270, Train Loss:    2.0, Train Acc:  65.62%, Time: 0:01:36 \nIter:    300, Train Loss:    1.4, Train Acc:  81.25%, Time: 0:01:47 *\nIter:    330, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:54 \nIter:    360, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:04 *\nIter:    390, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:11 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:17 \nIter:    450, Train Loss:    1.0, Train Acc:  89.06%, Time: 0:02:28 *\nIter:    480, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:34 \nIter:    510, Train Loss:    1.5, Train Acc:  79.69%, Time: 0:02:41 \nIter:    540, Train Loss:   0.76, Train Acc:  90.62%, Time: 0:02:53 *\nIter:    570, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:02:59 \nEpoch: 4\nIter:    600, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:05 \nIter:    630, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:11 \nIter:    660, Train Loss:    0.9, Train Acc:  87.50%, Time: 0:03:17 \nIter:    690, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:03:23 \nIter:    720, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:28 \nIter:    750, Train Loss:    0.8, Train Acc:  92.19%, Time: 0:03:39 *\nIter:    780, Train Loss:    0.9, Train Acc:  78.12%, Time: 0:03:45 \nEpoch: 5\nIter:    810, Train Loss:   0.85, Train Acc:  89.06%, Time: 0:03:51 \nIter:    840, Train Loss:   0.71, Train Acc:  93.75%, Time: 0:04:02 *\nIter:    870, Train Loss:    0.8, Train Acc:  89.06%, Time: 0:04:08 \nIter:    900, Train Loss:   0.49, Train Acc:  93.75%, Time: 0:04:14 \nIter:    930, Train Loss:   0.81, Train Acc:  89.06%, Time: 0:04:19 \nIter:    960, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:04:25 \nIter:    990, Train Loss:   0.57, Train Acc:  88.89%, Time: 0:04:31 \nEpoch: 6\nIter:   1020, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:04:37 \nIter:   1050, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:04:49 *\nIter:   1080, Train Loss:   0.48, Train Acc:  95.31%, Time: 0:04:55 \nIter:   1110, Train Loss:   0.67, Train Acc:  90.62%, Time: 0:05:00 \nIter:   1140, Train Loss:   0.89, Train Acc:  87.50%, Time: 0:05:06 \nIter:   1170, Train Loss:   0.48, Train Acc:  96.88%, Time: 0:05:18 *\nEpoch: 7\nIter:   1200, Train Loss:   0.46, Train Acc:  95.31%, Time: 0:05:24 \nIter:   1230, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:05:30 \nIter:   1260, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:05:36 \nIter:   1290, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:05:41 \nIter:   1320, Train Loss:   0.28, Train Acc:  98.44%, Time: 0:05:53 *\nIter:   1350, Train Loss:    0.3, Train Acc:  96.88%, Time: 0:05:59 \nIter:   1380, Train Loss:   0.31, Train Acc:  95.31%, Time: 0:06:05 \nEpoch: 8\nIter:   1410, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:10 \nIter:   1440, Train Loss:   0.39, Train Acc:  93.75%, Time: 0:06:16 \nIter:   1470, Train Loss:   0.26, Train Acc:  98.44%, Time: 0:06:22 \nIter:   1500, Train Loss:   0.58, Train Acc:  87.50%, Time: 0:06:28 \nIter:   1530, Train Loss:   0.43, Train Acc:  92.19%, Time: 0:06:33 \nIter:   1560, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:06:39 \nEpoch: 9\nIter:   1590, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:06:45 \nIter:   1620, Train Loss:   0.36, Train Acc:  95.31%, Time: 0:06:51 \nIter:   1650, Train Loss:   0.48, Train Acc:  89.06%, Time: 0:06:56 \nIter:   1680, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:07:02 \nIter:   1710, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:07:08 \nIter:   1740, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:07:14 \nIter:   1770, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:07:19 \nEpoch: 10\nIter:   1800, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:07:25 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/4/4\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/4/4\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.3, Test Acc:  69.53%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.67      0.47      0.55       131\n                   Filter       0.58      0.65      0.61       169\n    Compute Derived Value       0.49      0.74      0.59       141\n            Find Extremum       0.86      0.47      0.60       105\n                     Sort       0.81      0.65      0.72       108\n          Determine Range       0.58      0.72      0.64        82\nCharacterize Distribution       0.83      0.82      0.83       146\n           Find Anomalies       0.61      0.69      0.65       172\n                  Cluster       0.88      0.89      0.88       155\n                Correlate       0.85      0.75      0.79       209\n\n                micro avg       0.70      0.70      0.70      1418\n                macro avg       0.72      0.68      0.69      1418\n             weighted avg       0.72      0.70      0.70      1418\n\nConfusion Matrix...\n[[ 61  26  29   3   2   2   0   2   0   6]\n [  1 110  10   0   0   5   0  40   1   2]\n [ 15   2 104   0   2   0  11   6   1   0]\n [  4  11  22  49   1  15   2   1   0   0]\n [  0  10  14   3  70   4   1   1   5   0]\n [  0  16   3   0   4  59   0   0   0   0]\n [  1   0   6   1   0   6 120   0   1  11]\n [  6  15   9   0   0  11   3 119   1   8]\n [  0   0   2   0   7   0   2   5 138   1]\n [  3   0  14   1   0   0   5  20  10 156]]\nTime usage: 0:00:05\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.6, Train Acc:  12.50%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.1, Train Acc:  20.31%, Time: 0:00:29 *\nIter:     90, Train Loss:    3.6, Train Acc:  42.19%, Time: 0:00:40 *\nIter:    120, Train Loss:    2.8, Train Acc:  60.94%, Time: 0:00:51 *\nIter:    150, Train Loss:    2.7, Train Acc:  57.81%, Time: 0:00:56 \nIter:    180, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:07 *\nEpoch: 2\nIter:    210, Train Loss:    2.1, Train Acc:  70.31%, Time: 0:01:17 *\nIter:    240, Train Loss:    2.0, Train Acc:  67.19%, Time: 0:01:23 \nIter:    270, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:01:34 *\nIter:    300, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:44 *\nIter:    330, Train Loss:    1.7, Train Acc:  70.31%, Time: 0:01:51 \nIter:    360, Train Loss:    1.2, Train Acc:  75.00%, Time: 0:02:02 *\nIter:    390, Train Loss:    1.6, Train Acc:  79.69%, Time: 0:02:12 *\nEpoch: 3\nIter:    420, Train Loss:    1.2, Train Acc:  87.50%, Time: 0:02:23 *\nIter:    450, Train Loss:    1.6, Train Acc:  82.81%, Time: 0:02:29 \nIter:    480, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:35 \nIter:    510, Train Loss:   0.87, Train Acc:  85.94%, Time: 0:02:40 \nIter:    540, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:02:51 *\nIter:    570, Train Loss:   0.96, Train Acc:  87.50%, Time: 0:02:56 \nEpoch: 4\nIter:    600, Train Loss:   0.93, Train Acc:  81.25%, Time: 0:03:02 \nIter:    630, Train Loss:   0.79, Train Acc:  82.81%, Time: 0:03:08 \nIter:    660, Train Loss:   0.94, Train Acc:  87.50%, Time: 0:03:14 \nIter:    690, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:20 \nIter:    720, Train Loss:   0.76, Train Acc:  89.06%, Time: 0:03:25 \nIter:    750, Train Loss:   0.65, Train Acc:  90.62%, Time: 0:03:36 *\nIter:    780, Train Loss:   0.69, Train Acc:  92.19%, Time: 0:03:47 *\nEpoch: 5\nIter:    810, Train Loss:   0.85, Train Acc:  89.06%, Time: 0:03:52 \nIter:    840, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:58 \nIter:    870, Train Loss:   0.58, Train Acc:  90.62%, Time: 0:04:04 \nIter:    900, Train Loss:   0.55, Train Acc:  89.06%, Time: 0:04:10 \nIter:    930, Train Loss:   0.62, Train Acc:  89.06%, Time: 0:04:17 \nIter:    960, Train Loss:    0.6, Train Acc:  89.06%, Time: 0:04:23 \nIter:    990, Train Loss:   0.97, Train Acc:  84.21%, Time: 0:04:29 \nEpoch: 6\nIter:   1020, Train Loss:   0.58, Train Acc:  90.62%, Time: 0:04:35 \nIter:   1050, Train Loss:   0.37, Train Acc: 100.00%, Time: 0:04:46 *\nIter:   1080, Train Loss:   0.76, Train Acc:  89.06%, Time: 0:04:52 \nIter:   1110, Train Loss:   0.94, Train Acc:  87.50%, Time: 0:04:59 \nIter:   1140, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:05:05 \nIter:   1170, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:05:12 \nEpoch: 7\nIter:   1200, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:05:18 \nIter:   1230, Train Loss:   0.45, Train Acc:  98.44%, Time: 0:05:24 \nIter:   1260, Train Loss:   0.47, Train Acc:  93.75%, Time: 0:05:31 \nIter:   1290, Train Loss:   0.23, Train Acc: 100.00%, Time: 0:05:37 \nIter:   1320, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:05:43 \nIter:   1350, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:05:49 \nIter:   1380, Train Loss:    0.7, Train Acc:  92.19%, Time: 0:05:55 \nEpoch: 8\nIter:   1410, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:06:01 \nIter:   1440, Train Loss:   0.34, Train Acc:  90.62%, Time: 0:06:07 \nIter:   1470, Train Loss:   0.15, Train Acc: 100.00%, Time: 0:06:13 \nIter:   1500, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:06:19 \nIter:   1530, Train Loss:   0.27, Train Acc:  98.44%, Time: 0:06:26 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.1, Test Acc:  72.51%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.75      0.67      0.71       168\n                   Filter       0.62      0.90      0.74       136\n    Compute Derived Value       0.69      0.62      0.65       154\n            Find Extremum       0.56      0.61      0.58       147\n                     Sort       0.81      0.80      0.80       122\n          Determine Range       0.72      0.72      0.72       172\nCharacterize Distribution       0.84      0.76      0.80       127\n           Find Anomalies       0.74      0.56      0.64        91\n                  Cluster       0.81      0.82      0.81       138\n                Correlate       0.82      0.78      0.80       153\n\n                micro avg       0.73      0.73      0.73      1408\n                macro avg       0.73      0.72      0.72      1408\n             weighted avg       0.73      0.73      0.72      1408\n\nConfusion Matrix...\n[[112   1  25   4   0   5   5   2   3  11]\n [  0 123   1   1   0   8   0   1   0   2]\n [ 14  16  96  16   1   2   2   2   0   5]\n [  9  10   8  89  14  10   1   0   3   3]\n [ 10   1   0   2  97   3   1   0   6   2]\n [  2   5   1  34   4 123   1   1   0   1]\n [  1  13   4   2   1   4  97   0   5   0]\n [  1  17   2  11   0   4   1  51   3   1]\n [  0  10   0   0   0   6   1   6 113   2]\n [  0   2   3   0   3   5   7   6   7 120]]\nTime usage: 0:00:06\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  12.50%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.0, Train Acc:  35.94%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.5, Train Acc:  40.62%, Time: 0:00:38 *\nIter:    120, Train Loss:    2.9, Train Acc:  53.12%, Time: 0:00:48 *\nIter:    150, Train Loss:    2.8, Train Acc:  46.88%, Time: 0:00:54 \nIter:    180, Train Loss:    2.4, Train Acc:  68.75%, Time: 0:01:06 *\nEpoch: 2\nIter:    210, Train Loss:    1.7, Train Acc:  67.19%, Time: 0:01:13 \nIter:    240, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:01:24 *\nIter:    270, Train Loss:    1.3, Train Acc:  84.38%, Time: 0:01:34 *\nIter:    300, Train Loss:    1.5, Train Acc:  73.44%, Time: 0:01:40 \nIter:    330, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:46 \nIter:    360, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:01:52 \nIter:    390, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:01:57 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  75.00%, Time: 0:02:03 \nIter:    450, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:02:13 *\nIter:    480, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:02:19 \nIter:    510, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:25 \nIter:    540, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:30 \nIter:    570, Train Loss:   0.95, Train Acc:  85.94%, Time: 0:02:36 \nEpoch: 4\nIter:    600, Train Loss:   0.91, Train Acc:  87.50%, Time: 0:02:47 *\nIter:    630, Train Loss:   0.97, Train Acc:  82.81%, Time: 0:02:53 \nIter:    660, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:59 \nIter:    690, Train Loss:   0.78, Train Acc:  85.94%, Time: 0:03:05 \nIter:    720, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:10 \nIter:    750, Train Loss:   0.73, Train Acc:  90.62%, Time: 0:03:22 *\nIter:    780, Train Loss:   0.85, Train Acc:  93.75%, Time: 0:03:34 *\nEpoch: 5\nIter:    810, Train Loss:   0.74, Train Acc:  93.75%, Time: 0:03:40 \nIter:    840, Train Loss:   0.75, Train Acc:  89.06%, Time: 0:03:45 \nIter:    870, Train Loss:   0.95, Train Acc:  87.50%, Time: 0:03:51 \nIter:    900, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:03:57 \nIter:    930, Train Loss:   0.73, Train Acc:  89.06%, Time: 0:04:03 \nIter:    960, Train Loss:   0.65, Train Acc:  87.50%, Time: 0:04:09 \nIter:    990, Train Loss:    0.7, Train Acc:  92.59%, Time: 0:04:14 \nEpoch: 6\nIter:   1020, Train Loss:   0.59, Train Acc:  90.62%, Time: 0:04:20 \nIter:   1050, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:04:33 *\nIter:   1080, Train Loss:   0.44, Train Acc:  93.75%, Time: 0:04:38 \nIter:   1110, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:04:44 \nIter:   1140, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:04:50 \nIter:   1170, Train Loss:   0.59, Train Acc:  93.75%, Time: 0:04:56 \nEpoch: 7\nIter:   1200, Train Loss:   0.42, Train Acc:  96.88%, Time: 0:05:08 *\nIter:   1230, Train Loss:   0.39, Train Acc:  93.75%, Time: 0:05:14 \nIter:   1260, Train Loss:   0.36, Train Acc:  93.75%, Time: 0:05:20 \nIter:   1290, Train Loss:   0.15, Train Acc: 100.00%, Time: 0:05:31 *\nIter:   1320, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:05:37 \nIter:   1350, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:43 \nIter:   1380, Train Loss:   0.44, Train Acc:  96.88%, Time: 0:05:49 \nEpoch: 8\nIter:   1410, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:05:54 \nIter:   1440, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:06:00 \nIter:   1470, Train Loss:   0.27, Train Acc:  95.31%, Time: 0:06:06 \nIter:   1500, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:06:13 \nIter:   1530, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:06:19 \nIter:   1560, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:06:25 \nEpoch: 9\nIter:   1590, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:06:31 \nIter:   1620, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:06:37 \nIter:   1650, Train Loss:   0.23, Train Acc:  98.44%, Time: 0:06:43 \nIter:   1680, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:06:49 \nIter:   1710, Train Loss:   0.45, Train Acc:  93.75%, Time: 0:06:55 \nIter:   1740, Train Loss:   0.21, Train Acc:  96.88%, Time: 0:07:01 \nIter:   1770, Train Loss:   0.37, Train Acc:  92.19%, Time: 0:07:07 \nEpoch: 10\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/6/6\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/6/6\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.1, Test Acc:  64.09%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.68      0.83      0.75       115\n                   Filter       0.34      0.49      0.40       110\n    Compute Derived Value       0.66      0.45      0.54       210\n            Find Extremum       0.66      0.68      0.67       170\n                     Sort       0.83      0.90      0.87       153\n          Determine Range       0.62      0.44      0.52       102\nCharacterize Distribution       0.77      0.55      0.64       141\n           Find Anomalies       0.60      0.63      0.61       136\n                  Cluster       0.56      0.75      0.64        88\n                Correlate       0.69      0.72      0.71       148\n\n                micro avg       0.64      0.64      0.64      1373\n                macro avg       0.64      0.65      0.63      1373\n             weighted avg       0.66      0.64      0.64      1373\n\nConfusion Matrix...\n[[ 95   0  11   7   0   0   0   0   1   1]\n [  7  54   3  22   1  13   1   9   0   0]\n [ 28  21  95   3   7  11  11  11   2  21]\n [  0   9   5 116  12   2   0  20   5   1]\n [  0   1   0   4 138   1   0   0   9   0]\n [  9  23  12   2   4  45   2   2   1   2]\n [  0  18   3  21   2   0  78   1   9   9]\n [  1  19   8   0   0   0   0  86  10  12]\n [  0   1   1   0   0   0   5  14  66   1]\n [  0  14   6   0   2   0   4   1  14 107]]\nTime usage: 0:00:05\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.6, Train Acc:   9.38%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.2, Train Acc:  26.56%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.4, Train Acc:  50.00%, Time: 0:00:37 *\nIter:    120, Train Loss:    2.6, Train Acc:  68.75%, Time: 0:00:47 *\nIter:    150, Train Loss:    2.7, Train Acc:  60.94%, Time: 0:00:53 \nIter:    180, Train Loss:    2.6, Train Acc:  60.94%, Time: 0:00:59 \nEpoch: 2\nIter:    210, Train Loss:    1.9, Train Acc:  73.44%, Time: 0:01:09 *\nIter:    240, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:01:15 \nIter:    270, Train Loss:    2.2, Train Acc:  68.75%, Time: 0:01:21 \nIter:    300, Train Loss:    1.8, Train Acc:  70.31%, Time: 0:01:26 \nIter:    330, Train Loss:    1.4, Train Acc:  84.38%, Time: 0:01:37 *\nIter:    360, Train Loss:    1.8, Train Acc:  67.19%, Time: 0:01:43 \nIter:    390, Train Loss:    2.1, Train Acc:  68.75%, Time: 0:01:48 \nEpoch: 3\nIter:    420, Train Loss:   0.97, Train Acc:  85.94%, Time: 0:01:58 *\nIter:    450, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:02:04 \nIter:    480, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:10 \nIter:    510, Train Loss:   0.95, Train Acc:  85.94%, Time: 0:02:16 \nIter:    540, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:21 \nIter:    570, Train Loss:    1.2, Train Acc:  82.81%, Time: 0:02:27 \nEpoch: 4\nIter:    600, Train Loss:   0.98, Train Acc:  90.62%, Time: 0:02:38 *\nIter:    630, Train Loss:    1.2, Train Acc:  73.44%, Time: 0:02:43 \nIter:    660, Train Loss:   0.46, Train Acc:  95.31%, Time: 0:02:54 *\nIter:    690, Train Loss:   0.98, Train Acc:  90.62%, Time: 0:03:00 \nIter:    720, Train Loss:   0.99, Train Acc:  92.19%, Time: 0:03:06 \nIter:    750, Train Loss:    1.0, Train Acc:  87.50%, Time: 0:03:12 \nIter:    780, Train Loss:   0.86, Train Acc:  87.50%, Time: 0:03:18 \nEpoch: 5\nIter:    810, Train Loss:   0.73, Train Acc:  85.94%, Time: 0:03:23 \nIter:    840, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:03:29 \nIter:    870, Train Loss:   0.69, Train Acc:  87.50%, Time: 0:03:35 \nIter:    900, Train Loss:    1.0, Train Acc:  84.38%, Time: 0:03:41 \nIter:    930, Train Loss:    0.8, Train Acc:  85.94%, Time: 0:03:47 \nIter:    960, Train Loss:   0.65, Train Acc:  90.62%, Time: 0:03:52 \nIter:    990, Train Loss:   0.22, Train Acc: 100.00%, Time: 0:04:02 *\nEpoch: 6\nIter:   1020, Train Loss:   0.57, Train Acc:  93.75%, Time: 0:04:08 \nIter:   1050, Train Loss:   0.51, Train Acc:  89.06%, Time: 0:04:14 \nIter:   1080, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:04:20 \nIter:   1110, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:04:26 \nIter:   1140, Train Loss:   0.57, Train Acc:  92.19%, Time: 0:04:32 \nIter:   1170, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:04:37 \nEpoch: 7\nIter:   1200, Train Loss:   0.55, Train Acc:  85.94%, Time: 0:04:43 \nIter:   1230, Train Loss:   0.44, Train Acc:  92.19%, Time: 0:04:49 \nIter:   1260, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:04:55 \nIter:   1290, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:05:00 \nIter:   1320, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:05:06 \nIter:   1350, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:05:12 \nIter:   1380, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:05:18 \nEpoch: 8\nIter:   1410, Train Loss:   0.38, Train Acc:  95.31%, Time: 0:05:23 \nIter:   1440, Train Loss:   0.49, Train Acc:  93.75%, Time: 0:05:29 \nIter:   1470, Train Loss:   0.54, Train Acc:  90.62%, Time: 0:05:35 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/7/7\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/7/7\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.4, Test Acc:  66.67%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.86      0.82      0.84       198\n                   Filter       0.59      0.73      0.65       173\n    Compute Derived Value       0.50      0.82      0.62       119\n            Find Extremum       0.52      0.61      0.56        92\n                     Sort       0.95      0.66      0.78       172\n          Determine Range       0.71      0.46      0.56       167\nCharacterize Distribution       0.69      0.56      0.62        72\n           Find Anomalies       0.68      0.62      0.65       147\n                  Cluster       0.65      0.72      0.68       109\n                Correlate       0.59      0.60      0.59       152\n\n                micro avg       0.67      0.67      0.67      1401\n                macro avg       0.67      0.66      0.66      1401\n             weighted avg       0.69      0.67      0.67      1401\n\nConfusion Matrix...\n[[163   9  23   1   0   0   1   1   0   0]\n [ 11 126   1   8   1   5   0  15   6   0]\n [  5   3  98   0   0   4   0   0   0   9]\n [  4  16   8  56   0   1   0   2   0   5]\n [  0   0   2  21 113  11   0   1  24   0]\n [  4  21  29  10   1  77   1   2   9  13]\n [  1   1   8   0   3   5  40   0   4  10]\n [  0  11   5   8   1   2   6  91   0  23]\n [  1   5   1   1   0   4  10   5  79   3]\n [  0  22  21   2   0   0   0  16   0  91]]\nTime usage: 0:00:05\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  23.44%, Time: 0:00:15 *\nIter:     60, Train Loss:    3.9, Train Acc:  45.31%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.4, Train Acc:  43.75%, Time: 0:00:31 \nIter:    120, Train Loss:    2.8, Train Acc:  54.69%, Time: 0:00:41 *\nIter:    150, Train Loss:    2.5, Train Acc:  59.38%, Time: 0:00:52 *\nIter:    180, Train Loss:    2.2, Train Acc:  65.62%, Time: 0:01:03 *\nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  65.62%, Time: 0:01:09 \nIter:    240, Train Loss:    2.2, Train Acc:  67.19%, Time: 0:01:21 *\nIter:    270, Train Loss:    1.8, Train Acc:  70.31%, Time: 0:01:33 *\nIter:    300, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:01:46 *\nIter:    330, Train Loss:    1.8, Train Acc:  65.62%, Time: 0:01:52 \nIter:    360, Train Loss:    1.8, Train Acc:  75.00%, Time: 0:01:58 \nIter:    390, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:10 *\nEpoch: 3\nIter:    420, Train Loss:    1.7, Train Acc:  73.44%, Time: 0:02:16 \nIter:    450, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:28 *\nIter:    480, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:02:33 \nIter:    510, Train Loss:    0.9, Train Acc:  90.62%, Time: 0:02:45 *\nIter:    540, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:50 \nIter:    570, Train Loss:    0.8, Train Acc:  85.94%, Time: 0:02:56 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:03:02 \nIter:    630, Train Loss:   0.84, Train Acc:  85.94%, Time: 0:03:08 \nIter:    660, Train Loss:    0.9, Train Acc:  89.06%, Time: 0:03:13 \nIter:    690, Train Loss:   0.92, Train Acc:  85.94%, Time: 0:03:19 \nIter:    720, Train Loss:    1.2, Train Acc:  82.81%, Time: 0:03:25 \nIter:    750, Train Loss:   0.66, Train Acc:  87.50%, Time: 0:03:31 \nIter:    780, Train Loss:   0.82, Train Acc:  85.94%, Time: 0:03:36 \nEpoch: 5\nIter:    810, Train Loss:   0.87, Train Acc:  84.38%, Time: 0:03:42 \nIter:    840, Train Loss:    0.5, Train Acc:  90.62%, Time: 0:03:48 \nIter:    870, Train Loss:   0.49, Train Acc:  92.19%, Time: 0:03:59 *\nIter:    900, Train Loss:   0.73, Train Acc:  90.62%, Time: 0:04:05 \nIter:    930, Train Loss:   0.72, Train Acc:  89.06%, Time: 0:04:11 \nIter:    960, Train Loss:   0.89, Train Acc:  87.50%, Time: 0:04:17 \nIter:    990, Train Loss:  0.057, Train Acc: 100.00%, Time: 0:04:29 *\nEpoch: 6\nIter:   1020, Train Loss:   0.62, Train Acc:  92.19%, Time: 0:04:35 \nIter:   1050, Train Loss:   0.47, Train Acc:  96.88%, Time: 0:04:41 \nIter:   1080, Train Loss:   0.52, Train Acc:  89.06%, Time: 0:04:47 \nIter:   1110, Train Loss:   0.81, Train Acc:  92.19%, Time: 0:04:53 \nIter:   1140, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:04:59 \nIter:   1170, Train Loss:   0.63, Train Acc:  89.06%, Time: 0:05:05 \nEpoch: 7\nIter:   1200, Train Loss:   0.44, Train Acc:  92.19%, Time: 0:05:11 \nIter:   1230, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:05:17 \nIter:   1260, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:05:23 \nIter:   1290, Train Loss:    0.6, Train Acc:  93.75%, Time: 0:05:30 \nIter:   1320, Train Loss:   0.65, Train Acc:  93.75%, Time: 0:05:36 \nIter:   1350, Train Loss:   0.62, Train Acc:  87.50%, Time: 0:05:41 \nIter:   1380, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:05:47 \nEpoch: 8\nIter:   1410, Train Loss:    0.4, Train Acc:  95.31%, Time: 0:05:53 \nIter:   1440, Train Loss:   0.37, Train Acc:  98.44%, Time: 0:05:59 \nIter:   1470, Train Loss:    0.2, Train Acc:  96.88%, Time: 0:06:05 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/8/8\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/8/8\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.7, Test Acc:  65.92%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.75      0.57        56\n                   Filter       0.54      0.60      0.57       165\n    Compute Derived Value       0.67      0.58      0.62       125\n            Find Extremum       0.80      0.71      0.75       258\n                     Sort       0.37      0.33      0.35        61\n          Determine Range       0.66      0.41      0.51       109\nCharacterize Distribution       0.80      0.82      0.81       169\n           Find Anomalies       0.67      0.52      0.58       151\n                  Cluster       0.73      0.88      0.80       185\n                Correlate       0.55      0.67      0.61       144\n\n                micro avg       0.66      0.66      0.66      1423\n                macro avg       0.63      0.63      0.62      1423\n             weighted avg       0.67      0.66      0.66      1423\n\nConfusion Matrix...\n[[ 42   1   0   0   0   0   0  13   0   0]\n [  9  99   5   4   2  11   1  23   6   5]\n [  0  16  73  13   0   0   4   1   1  17]\n [ 27   9  13 184  14   3   3   1   2   2]\n [  2   1   0  27  20   6   2   0   2   1]\n [  9  25   4   2   1  45   4   0   7  12]\n [  1   0   2   0   2   2 138   0  19   5]\n [  1  32   1   0   0   0   0  78   6  33]\n [  0   1   0   0  15   0   4   0 162   3]\n [  1   1  11   0   0   1  16   0  17  97]]\nTime usage: 0:00:05\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  18.75%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  31.25%, Time: 0:00:26 *\nIter:     90, Train Loss:    4.0, Train Acc:  34.38%, Time: 0:00:37 *\nIter:    120, Train Loss:    2.7, Train Acc:  56.25%, Time: 0:00:47 *\nIter:    150, Train Loss:    2.7, Train Acc:  60.94%, Time: 0:00:58 *\nIter:    180, Train Loss:    2.4, Train Acc:  59.38%, Time: 0:01:03 \nEpoch: 2\nIter:    210, Train Loss:    2.6, Train Acc:  59.38%, Time: 0:01:09 \nIter:    240, Train Loss:    1.8, Train Acc:  73.44%, Time: 0:01:21 *\nIter:    270, Train Loss:    1.7, Train Acc:  76.56%, Time: 0:01:33 *\nIter:    300, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:01:46 *\nIter:    330, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:52 \nIter:    360, Train Loss:    1.5, Train Acc:  70.31%, Time: 0:01:57 \nIter:    390, Train Loss:    1.4, Train Acc:  75.00%, Time: 0:02:03 \nEpoch: 3\nIter:    420, Train Loss:    1.9, Train Acc:  75.00%, Time: 0:02:09 \nIter:    450, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:15 \nIter:    480, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:20 \nIter:    510, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:32 *\nIter:    540, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:38 \nIter:    570, Train Loss:   0.98, Train Acc:  87.50%, Time: 0:02:50 *\nEpoch: 4\nIter:    600, Train Loss:   0.76, Train Acc:  89.06%, Time: 0:03:01 *\nIter:    630, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:03:13 *\nIter:    660, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:03:19 \nIter:    690, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:03:24 \nIter:    720, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:30 \nIter:    750, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:03:36 \nIter:    780, Train Loss:   0.77, Train Acc:  85.94%, Time: 0:03:42 \nEpoch: 5\nIter:    810, Train Loss:   0.53, Train Acc:  95.31%, Time: 0:03:54 *\nIter:    840, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:04:00 \nIter:    870, Train Loss:   0.63, Train Acc:  85.94%, Time: 0:04:05 \nIter:    900, Train Loss:   0.74, Train Acc:  92.19%, Time: 0:04:11 \nIter:    930, Train Loss:   0.92, Train Acc:  84.38%, Time: 0:04:17 \nIter:    960, Train Loss:   0.58, Train Acc:  93.75%, Time: 0:04:23 \nIter:    990, Train Loss:   0.51, Train Acc:  88.89%, Time: 0:04:29 \nEpoch: 6\nIter:   1020, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:04:35 \nIter:   1050, Train Loss:   0.82, Train Acc:  85.94%, Time: 0:04:40 \nIter:   1080, Train Loss:   0.44, Train Acc:  90.62%, Time: 0:04:46 \nIter:   1110, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:04:57 *\nIter:   1140, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:05:03 \nIter:   1170, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:05:09 \nEpoch: 7\nIter:   1200, Train Loss:   0.52, Train Acc:  90.62%, Time: 0:05:14 \nIter:   1230, Train Loss:   0.37, Train Acc:  92.19%, Time: 0:05:20 \nIter:   1260, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:05:26 \nIter:   1290, Train Loss:   0.33, Train Acc:  96.88%, Time: 0:05:32 \nIter:   1320, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:05:37 \nIter:   1350, Train Loss:   0.62, Train Acc:  92.19%, Time: 0:05:43 \nIter:   1380, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:05:49 \nEpoch: 8\nIter:   1410, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:05:55 \nIter:   1440, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:06:01 \nIter:   1470, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:06:06 \nIter:   1500, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:06:12 \nIter:   1530, Train Loss:   0.28, Train Acc:  93.75%, Time: 0:06:18 \nIter:   1560, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:06:30 *\nEpoch: 9\nIter:   1590, Train Loss:   0.25, Train Acc:  98.44%, Time: 0:06:37 \nIter:   1620, Train Loss:   0.14, Train Acc: 100.00%, Time: 0:06:43 \nIter:   1650, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:06:49 \nIter:   1680, Train Loss:    0.2, Train Acc:  98.44%, Time: 0:06:55 \nIter:   1710, Train Loss:   0.43, Train Acc:  95.31%, Time: 0:07:01 \nIter:   1740, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:07:07 \nIter:   1770, Train Loss:   0.58, Train Acc:  95.31%, Time: 0:07:13 \nEpoch: 10\nIter:   1800, Train Loss:   0.13, Train Acc: 100.00%, Time: 0:07:19 \nIter:   1830, Train Loss:   0.25, Train Acc:  95.31%, Time: 0:07:26 \nIter:   1860, Train Loss:   0.31, Train Acc:  98.44%, Time: 0:07:32 \nIter:   1890, Train Loss:  0.098, Train Acc: 100.00%, Time: 0:07:38 \nIter:   1920, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:07:44 \nIter:   1950, Train Loss:   0.21, Train Acc:  98.44%, Time: 0:07:50 \nIter:   1980, Train Loss:   0.15, Train Acc:  97.22%, Time: 0:07:56 \nEpoch: 11\nIter:   2010, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:08:02 \nIter:   2040, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:08:08 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/9/9\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/9/9\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.5, Test Acc:  69.52%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.62      0.74      0.68       151\n                   Filter       0.47      0.72      0.57        97\n    Compute Derived Value       0.72      0.39      0.51       186\n            Find Extremum       0.85      0.70      0.77       202\n                     Sort       0.70      0.55      0.62        87\n          Determine Range       0.62      0.59      0.61       135\nCharacterize Distribution       0.62      0.93      0.75        61\n           Find Anomalies       0.77      0.68      0.72       170\n                  Cluster       0.83      0.83      0.83       156\n                Correlate       0.71      0.96      0.82       146\n\n                micro avg       0.70      0.70      0.70      1391\n                macro avg       0.69      0.71      0.69      1391\n             weighted avg       0.71      0.70      0.69      1391\n\nConfusion Matrix...\n[[112  13  10   1   1   4   4   0   2   4]\n [  2  70   1   0   2   3  14   3   2   0]\n [ 33   3  73  21   1  28   0  13   3  11]\n [ 25   6   2 141   4   6   6   5   1   6]\n [  0   8   1   0  48   4   4   3  11   8]\n [  7  26  11   0   4  80   1   0   1   5]\n [  1   0   0   0   0   2  57   0   1   0]\n [  0  24   3   2   0   2   0 116   5  18]\n [  0   0   0   0   9   0   6   6 130   5]\n [  0   0   0   0   0   0   0   5   1 140]]\nTime usage: 0:00:06\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.6, Train Acc:  12.50%, Time: 0:00:17 *\nIter:     60, Train Loss:    4.2, Train Acc:  26.56%, Time: 0:00:27 *\nIter:     90, Train Loss:    3.2, Train Acc:  46.88%, Time: 0:00:37 *\nIter:    120, Train Loss:    2.8, Train Acc:  54.69%, Time: 0:00:49 *\nIter:    150, Train Loss:    3.2, Train Acc:  50.00%, Time: 0:00:55 \nIter:    180, Train Loss:    2.5, Train Acc:  57.81%, Time: 0:01:06 *\nEpoch: 2\nIter:    210, Train Loss:    1.8, Train Acc:  73.44%, Time: 0:01:18 *\nIter:    240, Train Loss:    2.2, Train Acc:  62.50%, Time: 0:01:23 \nIter:    270, Train Loss:    1.5, Train Acc:  70.31%, Time: 0:01:29 \nIter:    300, Train Loss:    1.9, Train Acc:  67.19%, Time: 0:01:35 \nIter:    330, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:01:47 *\nIter:    360, Train Loss:    1.6, Train Acc:  71.88%, Time: 0:01:52 \nIter:    390, Train Loss:    1.5, Train Acc:  79.69%, Time: 0:02:04 *\nEpoch: 3\nIter:    420, Train Loss:    1.1, Train Acc:  79.69%, Time: 0:02:10 \nIter:    450, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:22 *\nIter:    480, Train Loss:    2.1, Train Acc:  67.19%, Time: 0:02:28 \nIter:    510, Train Loss:    1.3, Train Acc:  79.69%, Time: 0:02:34 \nIter:    540, Train Loss:   0.92, Train Acc:  87.50%, Time: 0:02:47 *\nIter:    570, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:02:53 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:59 \nIter:    630, Train Loss:   0.66, Train Acc:  95.31%, Time: 0:03:10 *\nIter:    660, Train Loss:   0.93, Train Acc:  90.62%, Time: 0:03:16 \nIter:    690, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:03:22 \nIter:    720, Train Loss:   0.78, Train Acc:  90.62%, Time: 0:03:27 \nIter:    750, Train Loss:   0.79, Train Acc:  87.50%, Time: 0:03:33 \nIter:    780, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:39 \nEpoch: 5\nIter:    810, Train Loss:   0.41, Train Acc:  96.88%, Time: 0:03:50 *\nIter:    840, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:03:56 \nIter:    870, Train Loss:   0.81, Train Acc:  87.50%, Time: 0:04:02 \nIter:    900, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:04:08 \nIter:    930, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:04:13 \nIter:    960, Train Loss:   0.58, Train Acc:  92.19%, Time: 0:04:19 \nIter:    990, Train Loss:   0.57, Train Acc: 100.00%, Time: 0:04:32 *\nEpoch: 6\nIter:   1020, Train Loss:   0.39, Train Acc:  98.44%, Time: 0:04:38 \nIter:   1050, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:04:43 \nIter:   1080, Train Loss:   0.48, Train Acc:  92.19%, Time: 0:04:49 \nIter:   1110, Train Loss:    0.7, Train Acc:  93.75%, Time: 0:04:55 \nIter:   1140, Train Loss:   0.66, Train Acc:  87.50%, Time: 0:05:01 \nIter:   1170, Train Loss:   0.59, Train Acc:  90.62%, Time: 0:05:06 \nEpoch: 7\nIter:   1200, Train Loss:   0.43, Train Acc:  95.31%, Time: 0:05:12 \nIter:   1230, Train Loss:   0.22, Train Acc:  98.44%, Time: 0:05:18 \nIter:   1260, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:05:24 \nIter:   1290, Train Loss:   0.28, Train Acc:  98.44%, Time: 0:05:30 \nIter:   1320, Train Loss:   0.26, Train Acc:  98.44%, Time: 0:05:35 \nIter:   1350, Train Loss:    0.3, Train Acc:  95.31%, Time: 0:05:41 \nIter:   1380, Train Loss:   0.97, Train Acc:  82.81%, Time: 0:05:47 \nEpoch: 8\nIter:   1410, Train Loss:   0.54, Train Acc:  89.06%, Time: 0:05:53 \nIter:   1440, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:05:58 \nIter:   1470, Train Loss:    0.7, Train Acc:  90.62%, Time: 0:06:04 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/10/10\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/bundle/10/10\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.3, Test Acc:  68.29%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.56      0.50      0.53       164\n                   Filter       0.72      0.73      0.73       180\n    Compute Derived Value       0.42      0.58      0.48       148\n            Find Extremum       0.70      0.71      0.71       101\n                     Sort       0.97      0.89      0.93       152\n          Determine Range       0.73      0.77      0.75       119\nCharacterize Distribution       0.80      0.56      0.66       117\n           Find Anomalies       0.62      0.81      0.70       108\n                  Cluster       0.88      0.49      0.63       109\n                Correlate       0.71      0.74      0.73       218\n\n                micro avg       0.68      0.68      0.68      1416\n                macro avg       0.71      0.68      0.68      1416\n             weighted avg       0.71      0.68      0.69      1416\n\nConfusion Matrix...\n[[ 82  26  31  15   0   4   0   0   1   5]\n [  4 132  16  11   0   2   4   4   1   6]\n [ 29   6  86   0   0   8   2   7   2   8]\n [  0   2  22  72   0   0   0   5   0   0]\n [  3   7   0   0 136   3   1   1   1   0]\n [  1   3  13   3   0  92   0   3   0   4]\n [  0   1  28   0   1   1  65   1   1  19]\n [  3   1   3   0   2   3   0  87   1   8]\n [ 16   1   4   1   0  13   2   3  53  16]\n [  9   4   4   1   1   0   7  30   0 162]]\nTime usage: 0:00:05\n[0.7202549575070821, 0.617839014167108, 0.6435643564356436, 0.6953455572908452, 0.7251420454545454, 0.6409322645051237, 0.6666666674750095, 0.6591707661548971, 0.6951833222085498, 0.682909604519774]\n0.6747008555718579, 0.03343861350667859, 0.0352473934930581, 0.0011181408732490274\ntable\n37 322 826 41\nFold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  17.19%, Time: 0:00:14 *\nIter:     60, Train Loss:    3.8, Train Acc:  35.94%, Time: 0:00:24 *\nIter:     90, Train Loss:    3.6, Train Acc:  48.44%, Time: 0:00:36 *\nIter:    120, Train Loss:    3.2, Train Acc:  54.69%, Time: 0:00:48 *\nIter:    150, Train Loss:    2.7, Train Acc:  64.06%, Time: 0:01:00 *\nIter:    180, Train Loss:    2.4, Train Acc:  65.62%, Time: 0:01:11 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  64.06%, Time: 0:01:17 \nIter:    240, Train Loss:    2.1, Train Acc:  64.06%, Time: 0:01:23 \nIter:    270, Train Loss:    2.1, Train Acc:  68.75%, Time: 0:01:33 *\nIter:    300, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:01:45 *\nIter:    330, Train Loss:    1.8, Train Acc:  68.75%, Time: 0:01:50 \nIter:    360, Train Loss:    2.1, Train Acc:  60.94%, Time: 0:01:56 \nIter:    390, Train Loss:    1.5, Train Acc:  82.81%, Time: 0:02:07 *\nEpoch: 3\nIter:    420, Train Loss:    1.0, Train Acc:  89.06%, Time: 0:02:19 *\nIter:    450, Train Loss:    1.5, Train Acc:  78.12%, Time: 0:02:25 \nIter:    480, Train Loss:   0.93, Train Acc:  90.62%, Time: 0:02:35 *\nIter:    510, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:41 \nIter:    540, Train Loss:   0.92, Train Acc:  92.19%, Time: 0:02:51 *\nIter:    570, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:57 \nEpoch: 4\nIter:    600, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:03:03 \nIter:    630, Train Loss:    1.3, Train Acc:  78.12%, Time: 0:03:09 \nIter:    660, Train Loss:    1.2, Train Acc:  89.06%, Time: 0:03:14 \nIter:    690, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:03:20 \nIter:    720, Train Loss:   0.86, Train Acc:  90.62%, Time: 0:03:26 \nIter:    750, Train Loss:    1.2, Train Acc:  79.69%, Time: 0:03:32 \nIter:    780, Train Loss:   0.89, Train Acc:  82.81%, Time: 0:03:38 \nEpoch: 5\nIter:    810, Train Loss:    0.6, Train Acc:  95.31%, Time: 0:03:51 *\nIter:    840, Train Loss:   0.98, Train Acc:  87.50%, Time: 0:03:56 \nIter:    870, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:04:02 \nIter:    900, Train Loss:   0.64, Train Acc:  92.19%, Time: 0:04:08 \nIter:    930, Train Loss:   0.81, Train Acc:  87.50%, Time: 0:04:14 \nIter:    960, Train Loss:   0.51, Train Acc:  93.75%, Time: 0:04:20 \nIter:    990, Train Loss:   0.77, Train Acc:  90.62%, Time: 0:04:25 \nEpoch: 6\nIter:   1020, Train Loss:   0.49, Train Acc:  90.62%, Time: 0:04:31 \nIter:   1050, Train Loss:   0.34, Train Acc:  92.19%, Time: 0:04:37 \nIter:   1080, Train Loss:   0.51, Train Acc:  92.19%, Time: 0:04:43 \nIter:   1110, Train Loss:    0.3, Train Acc:  93.75%, Time: 0:04:48 \nIter:   1140, Train Loss:   0.77, Train Acc:  87.50%, Time: 0:04:54 \nIter:   1170, Train Loss:   0.35, Train Acc:  93.75%, Time: 0:05:00 \nEpoch: 7\nIter:   1200, Train Loss:   0.36, Train Acc:  96.88%, Time: 0:05:10 *\nIter:   1230, Train Loss:    0.6, Train Acc:  93.75%, Time: 0:05:16 \nIter:   1260, Train Loss:   0.42, Train Acc:  93.75%, Time: 0:05:21 \nIter:   1290, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:27 \nIter:   1320, Train Loss:    0.6, Train Acc:  90.62%, Time: 0:05:33 \nIter:   1350, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:05:39 \nIter:   1380, Train Loss:    0.4, Train Acc:  98.44%, Time: 0:05:51 *\nEpoch: 8\nIter:   1410, Train Loss:   0.45, Train Acc:  92.19%, Time: 0:05:57 \nIter:   1440, Train Loss:   0.33, Train Acc:  93.75%, Time: 0:06:02 \nIter:   1470, Train Loss:   0.46, Train Acc:  93.75%, Time: 0:06:08 \nIter:   1500, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:06:14 \nIter:   1530, Train Loss:   0.59, Train Acc:  95.31%, Time: 0:06:20 \nIter:   1560, Train Loss:   0.16, Train Acc:  98.44%, Time: 0:06:25 \nIter:   1590, Train Loss:   0.39, Train Acc:  93.75%, Time: 0:06:31 \nEpoch: 9\nIter:   1620, Train Loss:   0.21, Train Acc:  98.44%, Time: 0:06:37 \nIter:   1650, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:06:43 \nIter:   1680, Train Loss:   0.32, Train Acc:  93.75%, Time: 0:06:48 \nIter:   1710, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:06:54 \nIter:   1740, Train Loss:   0.11, Train Acc:  98.44%, Time: 0:07:00 \nIter:   1770, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:07:06 \nEpoch: 10\nIter:   1800, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:07:11 \nIter:   1830, Train Loss:   0.42, Train Acc:  93.75%, Time: 0:07:17 \nIter:   1860, Train Loss:   0.38, Train Acc:  93.75%, Time: 0:07:23 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/1/1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/1/1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.9, Test Acc:  63.36%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.56      0.74      0.64       178\n                   Filter       0.49      0.45      0.47       163\n    Compute Derived Value       0.68      0.58      0.63       146\n            Find Extremum       0.72      0.73      0.72       199\n                     Sort       0.90      0.69      0.78        93\n          Determine Range       0.53      0.24      0.33        80\nCharacterize Distribution       0.88      0.73      0.80       111\n           Find Anomalies       0.54      0.42      0.47       144\n                  Cluster       0.46      0.87      0.60        55\n                Correlate       0.67      0.83      0.74       163\n\n                micro avg       0.63      0.63      0.63      1332\n                macro avg       0.64      0.63      0.62      1332\n             weighted avg       0.64      0.63      0.63      1332\n\nConfusion Matrix...\n[[131   5  14   5   0   0   0  20   1   2]\n [ 44  74   6  13   0  10   1   8   7   0]\n [ 33  14  85   1   0   0   2   3   0   8]\n [ 12   2   3 145   5   0   0  15   3  14]\n [ 10   3   1   1  64   0   2   1   4   7]\n [  1  11   6  33   2  19   2   1   1   4]\n [  0   0   3   0   0   4  81   0   4  19]\n [  1  36   2   0   0   1   1  61  32  10]\n [  1   0   0   1   0   0   2   0  48   3]\n [  1   6   5   3   0   2   1   4   5 136]]\nTime usage: 0:00:05\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  14.06%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  35.94%, Time: 0:00:26 *\nIter:     90, Train Loss:    3.4, Train Acc:  42.19%, Time: 0:00:37 *\nIter:    120, Train Loss:    3.0, Train Acc:  50.00%, Time: 0:00:49 *\nIter:    150, Train Loss:    2.4, Train Acc:  64.06%, Time: 0:01:00 *\nIter:    180, Train Loss:    2.5, Train Acc:  65.62%, Time: 0:01:10 *\nEpoch: 2\nIter:    210, Train Loss:    2.3, Train Acc:  64.06%, Time: 0:01:16 \nIter:    240, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:26 *\nIter:    270, Train Loss:    1.5, Train Acc:  79.69%, Time: 0:01:32 \nIter:    300, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:38 \nIter:    330, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:01:43 \nIter:    360, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:01:49 \nEpoch: 3\nIter:    390, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:01:59 *\nIter:    420, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:02:11 *\nIter:    450, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:16 \nIter:    480, Train Loss:    1.3, Train Acc:  76.56%, Time: 0:02:22 \nIter:    510, Train Loss:   0.77, Train Acc:  85.94%, Time: 0:02:33 *\nIter:    540, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:02:38 \nEpoch: 4\nIter:    570, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:02:49 *\nIter:    600, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:55 \nIter:    630, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:03:01 \nIter:    660, Train Loss:   0.96, Train Acc:  84.38%, Time: 0:03:07 \nIter:    690, Train Loss:   0.87, Train Acc:  85.94%, Time: 0:03:13 \nIter:    720, Train Loss:   0.62, Train Acc:  93.75%, Time: 0:03:24 *\nEpoch: 5\nIter:    750, Train Loss:    0.5, Train Acc:  92.19%, Time: 0:03:30 \nIter:    780, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:03:36 \nIter:    810, Train Loss:   0.72, Train Acc:  92.19%, Time: 0:03:42 \nIter:    840, Train Loss:   0.65, Train Acc:  93.75%, Time: 0:03:47 \nIter:    870, Train Loss:   0.71, Train Acc:  87.50%, Time: 0:03:53 \nIter:    900, Train Loss:   0.58, Train Acc:  92.19%, Time: 0:03:59 \nIter:    930, Train Loss:   0.49, Train Acc: 100.00%, Time: 0:04:10 *\nEpoch: 6\nIter:    960, Train Loss:   0.84, Train Acc:  84.38%, Time: 0:04:16 \nIter:    990, Train Loss:   0.63, Train Acc:  85.94%, Time: 0:04:22 \nIter:   1020, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:04:28 \nIter:   1050, Train Loss:    0.6, Train Acc:  93.75%, Time: 0:04:34 \nIter:   1080, Train Loss:   0.19, Train Acc:  98.44%, Time: 0:04:39 \nIter:   1110, Train Loss:   0.33, Train Acc:  98.44%, Time: 0:04:45 \nEpoch: 7\nIter:   1140, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:04:51 \nIter:   1170, Train Loss:   0.75, Train Acc:  85.94%, Time: 0:04:56 \nIter:   1200, Train Loss:   0.34, Train Acc:  96.88%, Time: 0:05:02 \nIter:   1230, Train Loss:   0.39, Train Acc:  95.31%, Time: 0:05:08 \nIter:   1260, Train Loss:   0.31, Train Acc:  98.44%, Time: 0:05:14 \nIter:   1290, Train Loss:   0.36, Train Acc:  93.75%, Time: 0:05:19 \nEpoch: 8\nIter:   1320, Train Loss:   0.28, Train Acc:  96.88%, Time: 0:05:25 \nIter:   1350, Train Loss:   0.29, Train Acc:  93.75%, Time: 0:05:31 \nIter:   1380, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:05:37 \nIter:   1410, Train Loss:   0.51, Train Acc:  95.31%, Time: 0:05:43 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/2/2\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/2/2\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.0, Test Acc:  58.79%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.52      0.58      0.55       159\n                   Filter       0.50      0.58      0.54       242\n    Compute Derived Value       0.59      0.38      0.46       287\n            Find Extremum       0.59      0.73      0.65       289\n                     Sort       0.52      0.87      0.65       203\n          Determine Range       0.60      0.59      0.60       185\nCharacterize Distribution       0.73      0.63      0.68       219\n           Find Anomalies       0.59      0.70      0.64       197\n                  Cluster       0.79      0.50      0.62       183\n                Correlate       0.63      0.35      0.45       220\n\n                micro avg       0.59      0.59      0.59      2184\n                macro avg       0.61      0.59      0.58      2184\n             weighted avg       0.60      0.59      0.58      2184\n\nConfusion Matrix...\n[[ 93  32   8  17   5   0   3   1   0   0]\n [ 13 141  20  13   4   9   8  20   0  14]\n [ 33  10 109  71   8  24  15  13   3   1]\n [ 17  11   0 211  38   6   0   4   2   0]\n [  6  13   1   0 176   1   2   0   4   0]\n [  6  35   1   6  22 109   2   1   2   1]\n [  3   3  19   7  27   6 139   4   2   9]\n [  1  10   4   8  15   0   0 137   4  18]\n [  4  15   2   3  28  14   8  14  92   3]\n [  2  13  22  20  15  12  13  39   7  77]]\nTime usage: 0:00:07\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  23.44%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.1, Train Acc:  34.38%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.7, Train Acc:  39.06%, Time: 0:00:41 *\nIter:    120, Train Loss:    3.0, Train Acc:  51.56%, Time: 0:00:52 *\nIter:    150, Train Loss:    2.5, Train Acc:  54.69%, Time: 0:01:04 *\nIter:    180, Train Loss:    2.5, Train Acc:  67.19%, Time: 0:01:15 *\nEpoch: 2\nIter:    210, Train Loss:    1.9, Train Acc:  70.31%, Time: 0:01:27 *\nIter:    240, Train Loss:    2.0, Train Acc:  70.31%, Time: 0:01:32 \nIter:    270, Train Loss:    1.7, Train Acc:  75.00%, Time: 0:01:44 *\nIter:    300, Train Loss:    2.0, Train Acc:  62.50%, Time: 0:01:50 \nIter:    330, Train Loss:    1.5, Train Acc:  76.56%, Time: 0:02:01 *\nIter:    360, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:02:07 \nIter:    390, Train Loss:    1.6, Train Acc:  75.00%, Time: 0:02:13 \nEpoch: 3\nIter:    420, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:25 *\nIter:    450, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:02:30 \nIter:    480, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:42 *\nIter:    510, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:48 \nIter:    540, Train Loss:    0.9, Train Acc:  87.50%, Time: 0:02:59 *\nIter:    570, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:03:05 \nEpoch: 4\nIter:    600, Train Loss:    1.0, Train Acc:  85.94%, Time: 0:03:11 \nIter:    630, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:03:17 \nIter:    660, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:03:22 \nIter:    690, Train Loss:   0.94, Train Acc:  84.38%, Time: 0:03:28 \nIter:    720, Train Loss:   0.68, Train Acc:  89.06%, Time: 0:03:40 *\nIter:    750, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:03:51 *\nIter:    780, Train Loss:   0.48, Train Acc:  92.19%, Time: 0:04:05 *\nEpoch: 5\nIter:    810, Train Loss:   0.75, Train Acc:  84.38%, Time: 0:04:11 \nIter:    840, Train Loss:   0.78, Train Acc:  89.06%, Time: 0:04:17 \nIter:    870, Train Loss:   0.45, Train Acc:  96.88%, Time: 0:04:29 *\nIter:    900, Train Loss:   0.37, Train Acc:  95.31%, Time: 0:04:35 \nIter:    930, Train Loss:   0.69, Train Acc:  89.06%, Time: 0:04:41 \nIter:    960, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:04:47 \nEpoch: 6\nIter:    990, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:04:53 \nIter:   1020, Train Loss:   0.36, Train Acc:  96.88%, Time: 0:04:59 \nIter:   1050, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:05:05 \nIter:   1080, Train Loss:   0.54, Train Acc:  90.62%, Time: 0:05:10 \nIter:   1110, Train Loss:   0.54, Train Acc:  95.31%, Time: 0:05:16 \nIter:   1140, Train Loss:   0.48, Train Acc:  93.75%, Time: 0:05:22 \nIter:   1170, Train Loss:   0.34, Train Acc:  95.31%, Time: 0:05:28 \nEpoch: 7\nIter:   1200, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:05:33 \nIter:   1230, Train Loss:   0.43, Train Acc:  95.31%, Time: 0:05:39 \nIter:   1260, Train Loss:    0.2, Train Acc:  95.31%, Time: 0:05:45 \nIter:   1290, Train Loss:   0.24, Train Acc:  96.88%, Time: 0:05:51 \nIter:   1320, Train Loss:   0.45, Train Acc:  95.31%, Time: 0:05:56 \nIter:   1350, Train Loss:   0.38, Train Acc:  95.31%, Time: 0:06:02 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/3/3\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/3/3\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    2.7, Test Acc:  65.27%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.69      0.34      0.46       126\n                   Filter       0.72      0.74      0.73       185\n    Compute Derived Value       0.43      0.51      0.47       136\n            Find Extremum       0.58      0.79      0.67       150\n                     Sort       0.78      0.70      0.74       142\n          Determine Range       0.82      0.49      0.61       144\nCharacterize Distribution       0.82      0.69      0.75       172\n           Find Anomalies       0.79      0.61      0.69       158\n                  Cluster       0.83      0.76      0.80       151\n                Correlate       0.44      0.79      0.57       165\n\n                micro avg       0.65      0.65      0.65      1529\n                macro avg       0.69      0.64      0.65      1529\n             weighted avg       0.69      0.65      0.65      1529\n\nConfusion Matrix...\n[[ 43   6  57   2   0   3   4   0   1  10]\n [  3 136   6  18  10   1   1   4   2   4]\n [  6   7  70  21   8   1   0   4   0  19]\n [  0   8   9 118   1   4   0   2   3   5]\n [  0   0   7  22 100   1   1   1   2   8]\n [  6   4   6  11   3  70   1   2   1  40]\n [  1  24   3   2   1   4 119   6   7   5]\n [  2   1   0   4   0   0   0  97   1  53]\n [  1   0   0   2   6   0   3   4 115  20]\n [  0   3   3   3   0   1  16   3   6 130]]\nTime usage: 0:00:06\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.5, Train Acc:  20.31%, Time: 0:00:15 *\nIter:     60, Train Loss:    3.9, Train Acc:  39.06%, Time: 0:00:27 *\nIter:     90, Train Loss:    3.4, Train Acc:  51.56%, Time: 0:00:39 *\nIter:    120, Train Loss:    2.9, Train Acc:  46.88%, Time: 0:00:45 \nIter:    150, Train Loss:    2.5, Train Acc:  60.94%, Time: 0:00:58 *\nIter:    180, Train Loss:    2.3, Train Acc:  68.75%, Time: 0:01:08 *\nEpoch: 2\nIter:    210, Train Loss:    2.0, Train Acc:  71.88%, Time: 0:01:18 *\nIter:    240, Train Loss:    2.0, Train Acc:  70.31%, Time: 0:01:24 \nIter:    270, Train Loss:    1.9, Train Acc:  71.88%, Time: 0:01:30 \nIter:    300, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:40 *\nIter:    330, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:01:46 \nIter:    360, Train Loss:    1.8, Train Acc:  78.12%, Time: 0:01:57 *\nIter:    390, Train Loss:    1.8, Train Acc:  78.12%, Time: 0:02:02 \nEpoch: 3\nIter:    420, Train Loss:    1.2, Train Acc:  82.81%, Time: 0:02:14 *\nIter:    450, Train Loss:    1.6, Train Acc:  73.44%, Time: 0:02:20 \nIter:    480, Train Loss:   0.81, Train Acc:  89.06%, Time: 0:02:31 *\nIter:    510, Train Loss:   0.81, Train Acc:  89.06%, Time: 0:02:37 \nIter:    540, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:43 \nIter:    570, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:02:48 \nIter:    600, Train Loss:   0.91, Train Acc:  87.50%, Time: 0:02:54 \nEpoch: 4\nIter:    630, Train Loss:   0.87, Train Acc:  84.38%, Time: 0:03:00 \nIter:    660, Train Loss:   0.73, Train Acc:  89.06%, Time: 0:03:06 \nIter:    690, Train Loss:    1.1, Train Acc:  85.94%, Time: 0:03:11 \nIter:    720, Train Loss:   0.86, Train Acc:  89.06%, Time: 0:03:17 \nIter:    750, Train Loss:    1.0, Train Acc:  82.81%, Time: 0:03:23 \nIter:    780, Train Loss:   0.74, Train Acc:  89.06%, Time: 0:03:29 \nEpoch: 5\nIter:    810, Train Loss:   0.51, Train Acc:  95.31%, Time: 0:03:40 *\nIter:    840, Train Loss:   0.88, Train Acc:  85.94%, Time: 0:03:46 \nIter:    870, Train Loss:   0.91, Train Acc:  85.94%, Time: 0:03:52 \nIter:    900, Train Loss:   0.52, Train Acc:  90.62%, Time: 0:03:58 \nIter:    930, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:04:04 \nIter:    960, Train Loss:   0.62, Train Acc:  90.62%, Time: 0:04:09 \nIter:    990, Train Loss:   0.67, Train Acc:  90.62%, Time: 0:04:15 \nEpoch: 6\nIter:   1020, Train Loss:    0.9, Train Acc:  84.38%, Time: 0:04:21 \nIter:   1050, Train Loss:   0.38, Train Acc:  96.88%, Time: 0:04:32 *\nIter:   1080, Train Loss:   0.38, Train Acc:  95.31%, Time: 0:04:37 \nIter:   1110, Train Loss:   0.42, Train Acc:  92.19%, Time: 0:04:43 \nIter:   1140, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:04:49 \nIter:   1170, Train Loss:    0.4, Train Acc:  96.88%, Time: 0:04:55 \nIter:   1200, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:05:01 \nEpoch: 7\nIter:   1230, Train Loss:   0.23, Train Acc:  95.31%, Time: 0:05:06 \nIter:   1260, Train Loss:   0.57, Train Acc:  90.62%, Time: 0:05:12 \nIter:   1290, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:05:18 \nIter:   1320, Train Loss:   0.29, Train Acc:  96.88%, Time: 0:05:24 \nIter:   1350, Train Loss:   0.32, Train Acc:  98.44%, Time: 0:05:36 *\nIter:   1380, Train Loss:   0.64, Train Acc:  89.06%, Time: 0:05:42 \nIter:   1410, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:05:47 \nEpoch: 8\nIter:   1440, Train Loss:   0.41, Train Acc:  95.31%, Time: 0:05:53 \nIter:   1470, Train Loss:   0.64, Train Acc:  89.06%, Time: 0:05:59 \nIter:   1500, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:05 \nIter:   1530, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:06:11 \nIter:   1560, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:06:16 \nIter:   1590, Train Loss:   0.46, Train Acc:  90.62%, Time: 0:06:22 \nEpoch: 9\nIter:   1620, Train Loss:  0.071, Train Acc: 100.00%, Time: 0:06:32 *\nIter:   1650, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:06:38 \nIter:   1680, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:06:44 \nIter:   1710, Train Loss:   0.32, Train Acc:  98.44%, Time: 0:06:50 \nIter:   1740, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:06:56 \nIter:   1770, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:07:01 \nIter:   1800, Train Loss:   0.25, Train Acc:  93.75%, Time: 0:07:07 \nEpoch: 10\nIter:   1830, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:07:13 \nIter:   1860, Train Loss:   0.31, Train Acc:  96.88%, Time: 0:07:19 \nIter:   1890, Train Loss:   0.11, Train Acc: 100.00%, Time: 0:07:25 \nIter:   1920, Train Loss:   0.19, Train Acc:  98.44%, Time: 0:07:30 \nIter:   1950, Train Loss:   0.26, Train Acc:  93.75%, Time: 0:07:36 \nIter:   1980, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:07:42 \nIter:   2010, Train Loss:    0.3, Train Acc:  96.88%, Time: 0:07:48 \nEpoch: 11\nIter:   2040, Train Loss:   0.11, Train Acc:  98.44%, Time: 0:07:54 \nIter:   2070, Train Loss:  0.099, Train Acc: 100.00%, Time: 0:07:59 \nIter:   2100, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:08:05 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/4/4\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/4/4\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    3.2, Test Acc:  61.09%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.46      0.38      0.42        95\n                   Filter       0.37      0.71      0.48       106\n    Compute Derived Value       0.66      0.28      0.39       178\n            Find Extremum       0.82      0.72      0.77       229\n                     Sort       0.72      0.87      0.79        61\n          Determine Range       0.60      0.64      0.62       108\nCharacterize Distribution       0.57      0.72      0.64        72\n           Find Anomalies       0.56      0.91      0.69       108\n                  Cluster       0.85      0.65      0.74        72\n                Correlate       0.72      0.46      0.56       107\n\n                micro avg       0.61      0.61      0.61      1136\n                macro avg       0.63      0.63      0.61      1136\n             weighted avg       0.65      0.61      0.60      1136\n\nConfusion Matrix...\n[[ 36  25  17   7   1   5   0   2   0   2]\n [  8  75   1   0   1  16   0   4   1   0]\n [  6  16  50  23   8  15  35   9   1  15]\n [ 13  16   3 165   1   0   0  28   3   0]\n [  0   2   0   0  53   0   1   2   3   0]\n [  2  31   2   2   1  69   1   0   0   0]\n [  1   6   1   0   2   2  52   6   0   2]\n [  0   4   1   3   0   2   0  98   0   0]\n [  0  11   0   0   5   4   0   5  47   0]\n [ 12  18   1   0   2   2   2  21   0  49]]\nTime usage: 0:00:05\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  18.75%, Time: 0:00:15 *\nIter:     60, Train Loss:    4.0, Train Acc:  34.38%, Time: 0:00:25 *\nIter:     90, Train Loss:    3.3, Train Acc:  64.06%, Time: 0:00:35 *\nIter:    120, Train Loss:    2.9, Train Acc:  46.88%, Time: 0:00:41 \nIter:    150, Train Loss:    2.5, Train Acc:  56.25%, Time: 0:00:47 \nIter:    180, Train Loss:    2.5, Train Acc:  64.06%, Time: 0:00:53 \nEpoch: 2\nIter:    210, Train Loss:    2.4, Train Acc:  60.94%, Time: 0:00:59 \nIter:    240, Train Loss:    2.0, Train Acc:  64.06%, Time: 0:01:04 \nIter:    270, Train Loss:    2.4, Train Acc:  56.25%, Time: 0:01:10 \nIter:    300, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:01:21 *\nIter:    330, Train Loss:    2.0, Train Acc:  73.44%, Time: 0:01:27 \nIter:    360, Train Loss:    1.7, Train Acc:  68.75%, Time: 0:01:33 \nIter:    390, Train Loss:    1.3, Train Acc:  81.25%, Time: 0:01:38 \nEpoch: 3\nIter:    420, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:01:44 \nIter:    450, Train Loss:    1.2, Train Acc:  78.12%, Time: 0:01:50 \nIter:    480, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:01:56 \nIter:    510, Train Loss:   0.96, Train Acc:  82.81%, Time: 0:02:01 \nIter:    540, Train Loss:    1.6, Train Acc:  79.69%, Time: 0:02:07 \nIter:    570, Train Loss:    1.1, Train Acc:  82.81%, Time: 0:02:13 \nIter:    600, Train Loss:   0.84, Train Acc:  85.94%, Time: 0:02:25 *\nEpoch: 4\nIter:    630, Train Loss:    1.3, Train Acc:  82.81%, Time: 0:02:31 \nIter:    660, Train Loss:   0.85, Train Acc:  89.06%, Time: 0:02:44 *\nIter:    690, Train Loss:   0.85, Train Acc:  84.38%, Time: 0:02:50 \nIter:    720, Train Loss:    1.4, Train Acc:  78.12%, Time: 0:02:56 \nIter:    750, Train Loss:   0.65, Train Acc:  92.19%, Time: 0:03:09 *\nIter:    780, Train Loss:   0.98, Train Acc:  84.38%, Time: 0:03:15 \nEpoch: 5\nIter:    810, Train Loss:   0.94, Train Acc:  90.62%, Time: 0:03:21 \nIter:    840, Train Loss:   0.53, Train Acc:  90.62%, Time: 0:03:27 \nIter:    870, Train Loss:   0.65, Train Acc:  85.94%, Time: 0:03:33 \nIter:    900, Train Loss:   0.64, Train Acc:  90.62%, Time: 0:03:39 \nIter:    930, Train Loss:   0.75, Train Acc:  89.06%, Time: 0:03:45 \nIter:    960, Train Loss:   0.54, Train Acc:  92.19%, Time: 0:03:50 \nIter:    990, Train Loss:   0.91, Train Acc:  79.69%, Time: 0:03:56 \nEpoch: 6\nIter:   1020, Train Loss:   0.37, Train Acc:  93.75%, Time: 0:04:07 *\nIter:   1050, Train Loss:    0.7, Train Acc:  89.06%, Time: 0:04:13 \nIter:   1080, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:04:26 *\nIter:   1110, Train Loss:   0.25, Train Acc:  96.88%, Time: 0:04:32 \nIter:   1140, Train Loss:   0.72, Train Acc:  90.62%, Time: 0:04:38 \nIter:   1170, Train Loss:   0.39, Train Acc:  93.75%, Time: 0:04:44 \nIter:   1200, Train Loss:    0.5, Train Acc:  93.75%, Time: 0:04:51 \nEpoch: 7\nIter:   1230, Train Loss:   0.17, Train Acc: 100.00%, Time: 0:05:04 *\nIter:   1260, Train Loss:   0.51, Train Acc:  92.19%, Time: 0:05:10 \nIter:   1290, Train Loss:   0.62, Train Acc:  93.75%, Time: 0:05:16 \nIter:   1320, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:22 \nIter:   1350, Train Loss:   0.32, Train Acc:  96.88%, Time: 0:05:28 \nIter:   1380, Train Loss:   0.22, Train Acc:  96.88%, Time: 0:05:33 \nIter:   1410, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:05:39 \nEpoch: 8\nIter:   1440, Train Loss:   0.23, Train Acc:  98.44%, Time: 0:05:45 \nIter:   1470, Train Loss:   0.52, Train Acc:  93.75%, Time: 0:05:51 \nIter:   1500, Train Loss:   0.32, Train Acc:  95.31%, Time: 0:05:57 \nIter:   1530, Train Loss:   0.26, Train Acc:  95.31%, Time: 0:06:02 \nIter:   1560, Train Loss:   0.35, Train Acc:  95.31%, Time: 0:06:08 \nIter:   1590, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:06:14 \nEpoch: 9\nIter:   1620, Train Loss:   0.31, Train Acc:  98.44%, Time: 0:06:20 \nIter:   1650, Train Loss:  0.085, Train Acc: 100.00%, Time: 0:06:25 \nIter:   1680, Train Loss:   0.27, Train Acc:  96.88%, Time: 0:06:31 \nIter:   1710, Train Loss:   0.57, Train Acc:  93.75%, Time: 0:06:37 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0b742405f132>\", line 15, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-11-128725d8d774>\", line 18, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-11-128725d8d774>\", line 100, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-18-20e7decdf33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest_acc_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-12-07ac98523da2>\u001b[0m in \u001b[0;36mtrain_split_data\u001b[0;34m(model, train_data, split_type)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-12-07ac98523da2>\u001b[0m in \u001b[0;36mmodel_evaluate\u001b[0;34m(model, x_test, y_test, split_type, fold_id, categories, batch_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-12-07ac98523da2>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sess, model, x_pad, y_pad, loss1, acc1, batch_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mbatch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mfeed_dict1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mlossTmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0b742405f132>\", line 15, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-11-128725d8d774>\", line 18, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-11-128725d8d774>\", line 100, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n"]}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(train_data)", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "37"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_acc = []\ntest_acc = []\nfold_id = 0\n\nsplit_type = \"table\"\ntx = []\nty = []\nfor ti in train_data:\n    x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n    tx.append(x_train)\n    ty.append(y_train)\n\ntx = np.asarray(tx)\nty = np.asarray(ty)\n\nprint(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n\nfor train_i, test_i in kf.split(tx):\n    fold_id += 1\n    if fold_id >= 5:\n        print(\"Fold: \", fold_id)\n        train_x, train_y = mergeData(tx[train_i],ty[train_i])\n        test_x, test_y = mergeData(tx[test_i],ty[test_i])\n        train_acc.append(model_train(lstm,train_x, train_y,split_type,fold_id))\n        test_acc.append(model_evaluate(lstm,test_x, test_y,split_type,fold_id,categories))", "execution_count": 61, "outputs": [{"output_type": "stream", "text": "37 322 826 41\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    4.4, Train Acc:  17.19%, Time: 0:00:16 *\nIter:     60, Train Loss:    4.0, Train Acc:  23.44%, Time: 0:00:28 *\nIter:     90, Train Loss:    3.6, Train Acc:  40.62%, Time: 0:00:40 *\nIter:    120, Train Loss:    3.0, Train Acc:  45.31%, Time: 0:00:51 *\nIter:    150, Train Loss:    2.4, Train Acc:  65.62%, Time: 0:01:04 *\nIter:    180, Train Loss:    2.1, Train Acc:  73.44%, Time: 0:01:16 *\nEpoch: 2\nIter:    210, Train Loss:    2.2, Train Acc:  70.31%, Time: 0:01:21 \nIter:    240, Train Loss:    1.7, Train Acc:  71.88%, Time: 0:01:27 \nIter:    270, Train Loss:    1.8, Train Acc:  71.88%, Time: 0:01:33 \nIter:    300, Train Loss:    2.1, Train Acc:  67.19%, Time: 0:01:39 \nIter:    330, Train Loss:    1.8, Train Acc:  73.44%, Time: 0:01:44 \nIter:    360, Train Loss:    2.1, Train Acc:  70.31%, Time: 0:01:50 \nIter:    390, Train Loss:    1.3, Train Acc:  84.38%, Time: 0:02:01 *\nEpoch: 3\nIter:    420, Train Loss:    1.7, Train Acc:  68.75%, Time: 0:02:07 \nIter:    450, Train Loss:    1.2, Train Acc:  84.38%, Time: 0:02:13 \nIter:    480, Train Loss:    1.3, Train Acc:  78.12%, Time: 0:02:19 \nIter:    510, Train Loss:    1.5, Train Acc:  71.88%, Time: 0:02:25 \nIter:    540, Train Loss:    1.1, Train Acc:  81.25%, Time: 0:02:31 \nIter:    570, Train Loss:    1.4, Train Acc:  79.69%, Time: 0:02:37 \nIter:    600, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:02:43 \nEpoch: 4\nIter:    630, Train Loss:   0.92, Train Acc:  87.50%, Time: 0:02:55 *\nIter:    660, Train Loss:   0.65, Train Acc:  92.19%, Time: 0:03:07 *\nIter:    690, Train Loss:    1.2, Train Acc:  81.25%, Time: 0:03:13 \nIter:    720, Train Loss:   0.94, Train Acc:  84.38%, Time: 0:03:20 \nIter:    750, Train Loss:   0.83, Train Acc:  90.62%, Time: 0:03:26 \nIter:    780, Train Loss:    1.1, Train Acc:  84.38%, Time: 0:03:32 \nEpoch: 5\nIter:    810, Train Loss:   0.39, Train Acc:  92.19%, Time: 0:03:38 \nIter:    840, Train Loss:   0.35, Train Acc:  98.44%, Time: 0:03:50 *\nIter:    870, Train Loss:   0.76, Train Acc:  89.06%, Time: 0:03:56 \nIter:    900, Train Loss:   0.68, Train Acc:  90.62%, Time: 0:04:02 \nIter:    930, Train Loss:   0.67, Train Acc:  92.19%, Time: 0:04:08 \nIter:    960, Train Loss:   0.49, Train Acc:  95.31%, Time: 0:04:14 \nIter:    990, Train Loss:   0.67, Train Acc:  89.06%, Time: 0:04:20 \nEpoch: 6\nIter:   1020, Train Loss:   0.35, Train Acc:  96.88%, Time: 0:04:26 \nIter:   1050, Train Loss:   0.29, Train Acc:  95.31%, Time: 0:04:32 \nIter:   1080, Train Loss:    0.6, Train Acc:  92.19%, Time: 0:04:38 \nIter:   1110, Train Loss:   0.58, Train Acc:  92.19%, Time: 0:04:44 \nIter:   1140, Train Loss:   0.96, Train Acc:  84.38%, Time: 0:04:50 \nIter:   1170, Train Loss:   0.79, Train Acc:  87.50%, Time: 0:04:56 \nIter:   1200, Train Loss:   0.33, Train Acc:  95.31%, Time: 0:05:02 \nEpoch: 7\nIter:   1230, Train Loss:   0.41, Train Acc:  90.62%, Time: 0:05:08 \nIter:   1260, Train Loss:   0.59, Train Acc:  92.19%, Time: 0:05:14 \nIter:   1290, Train Loss:    0.4, Train Acc:  93.75%, Time: 0:05:20 \nIter:   1320, Train Loss:   0.23, Train Acc:  96.88%, Time: 0:05:26 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model/ad_biLSTM/table/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\n", "name": "stdout"}, {"output_type": "error", "ename": "InvalidArgumentError", "evalue": "In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0b742405f132>\", line 15, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-11-128725d8d774>\", line 18, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-11-128725d8d774>\", line 100, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", "\u001b[0;32m<ipython-input-61-cf3e2e43cc93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-12-07ac98523da2>\u001b[0m in \u001b[0;36mmodel_evaluate\u001b[0;34m(model, x_test, y_test, split_type, fold_id, categories, batch_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-12-07ac98523da2>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sess, model, x_pad, y_pad, loss1, acc1, batch_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mbatch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mfeed_dict1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropoutKeepProb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mlossTmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccTmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n\nCaused by op 'loss/Bi-LSTM/output/predictions/MatMul', defined at:\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0b742405f132>\", line 15, in <module>\n    lstm = AdversarailLSTM(embedding)\n  File \"<ipython-input-11-128725d8d774>\", line 18, in __init__\n    self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n  File \"<ipython-input-11-128725d8d774>\", line 100, in _Bi_LSTMAttention\n    predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2208, in xw_plus_b\n    mm = math_ops.matmul(x, weights)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2122, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4279, in mat_mul\n    name=name)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): In[0] is not a matrix\n\t [[Node: loss/Bi-LSTM/output/predictions/MatMul = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Bi-LSTM/Attention/dropout/mul, Bi-LSTM/outputW/read)]]\n"]}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(test_x),len(test_x[0]),len(test_y),len(test_y[0])", "execution_count": 40, "outputs": [{"output_type": "execute_result", "execution_count": 40, "data": {"text/plain": "(1136, 41, 1136, 10)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "test_x[0],test_x[1],test_y[0],test_y[1]", "execution_count": 43, "outputs": [{"output_type": "execute_result", "execution_count": 43, "data": {"text/plain": "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2237,\n        2867, 4249,   33,  356,  197,  109, 1145,  188], dtype=int32),\n array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,   102,    32,    77,  1145,\n           12,    32, 13773,   173,   188], dtype=int32),\n array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]))"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(test_x),len(test_x[0]),len(test_y),len(test_y[0])", "execution_count": 45, "outputs": [{"output_type": "execute_result", "execution_count": 45, "data": {"text/plain": "(1153, 41, 1153, 10)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "test_x[-1],test_y[-1],test_x[1152]", "execution_count": 53, "outputs": [{"output_type": "execute_result", "execution_count": 53, "data": {"text/plain": "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    6,    0,   62,  778,\n          42,  123,   40,    0,   96, 8786,  543,  188], dtype=int32),\n array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    6,    0,   62,  778,\n          42,  123,   40,    0,   96, 8786,  543,  188], dtype=int32))"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}