{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocabPath = \"s3://corpus-text-classification1/data/glove.6B.100d.txt\"\ntrainDataPath = \"s3://corpus-text-classification1/data/train_5500.label.txt\"\ntestDataPath = \"s3://corpus-text-classification1/data/TREC_10.label.txt\"\nsavePath = \"s3://corpus-2/model2/trec\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd\n\n\ndef readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    train_data = []\n    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n        for line in f.readlines():\n            word = line.strip().split()\n            label = word[0].split(\":\")[0]\n            content = word[1:]\n            train_data.append([content,label])\n    \n    np.random.shuffle(train_data)\n    return np.asarray(train_data)", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\nnum_classes = len(categories)\n\nvocab, embd = loadGloVe(vocabPath, 100)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)\nword_to_id = dict(zip(vocab, range(vocab_size)))\n\ncat_to_id = {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n\nlen(embedding),embedding_dim,vocab_size", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "(400000, 100, 400000)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "testData = readfile(testDataPath)\ntrainData = readfile(trainDataPath)\n\ntrainData = np.r_[trainData,testData]\nlen(trainData)", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "5952"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length = 0\nfor content in trainData[:,0]:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 35\n        \nseq_length", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "37"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, cat_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"Loading training and validation and testing data...\")\nstart_time = time.time()\nx_train, y_train = process_file(trainData[:,0], trainData[:,1], word_to_id,cat_to_id, num_classes, seq_length)\ntime_dif = get_time_dif(start_time)\nprint(\"Loading data Time usage:\", time_dif)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Loading training and validation and testing data...\nLoading data Time usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class rnn_cnn(object):\n    \n    def __init__(self, savePath, num_classes):\n        self.savePath = savePath\n        # \u8f93\u5165\u5185\u5bb9\u53ca\u5bf9\u5e94\u7684\u6807\u7b7e\n        self.input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n        # dropout\u7684\u635f\u5931\u7387\n        self.keep_prob = tf.placeholder(tf.float64, name='keep_prob')\n        # \u8bcd\u5411\u91cf\u6620\u5c04;\u5b9e\u9645\u4e0a\u6b64\u5904\u7684\u8bcd\u5411\u91cf\u5e76\u4e0d\u662f\u7528\u7684\u9884\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u800c\u662f\u672a\u7ecf\u4efb\u4f55\u8bad\u7ec3\u76f4\u63a5\u751f\u6210\u4e86\u4e00\u4e2a\u77e9\u9635\uff0c\u5c06\u6b64\u77e9\u9635\u4f5c\u4e3a\u8bcd\u5411\u91cf\u77e9\u9635\u4f7f\u7528\uff0c\u6548\u679c\u4e5f\u8fd8\u4e0d\u9519\u3002\n        # \u82e5\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u6216\u8bb8\u8bad\u7ec3\u6b64\u6b21\u6587\u672c\u5206\u7c7b\u7684\u6a21\u578b\u65f6\u4f1a\u66f4\u5feb\uff0c\u66f4\u597d\u3002\n        # embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])\n        embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n\n        num_filters = 256\n        kernel_size = 5\n        hidden_dim = 128\n        learning_rate = 1e-3\n\n        # CNN layer\n        conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 \u8fd9\u662f\u4e2a\u5565\n        ''' https://blog.csdn.net/khy19940520/article/details/89934335\n        tf.layers.conv1d\uff1a\u4e00\u7ef4\u5377\u79ef\u4e00\u822c\u7528\u4e8e\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u5e38\u7528\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u8f93\u5165\u4e00\u822c\u662f\u6587\u672c\u7ecf\u8fc7embedding\u7684\u4e8c\u7ef4\u6570\u636e\u3002\n            inputs\uff1a \u8f93\u5165tensor\uff0c \u7ef4\u5ea6(batch_size, seq_length, embedding_dim) \u662f\u4e00\u4e2a\u4e09\u7ef4\u7684tensor\uff1b\u5176\u4e2d\uff0c\n                batch_size\u6307\u6bcf\u6b21\u8f93\u5165\u7684\u6587\u672c\u6570\u91cf\uff1b\n                seq_length\u6307\u6bcf\u4e2a\u6587\u672c\u7684\u8bcd\u8bed\u6570\u6216\u8005\u5355\u5b57\u6570\uff1b\n                embedding_dim\u6307\u6bcf\u4e2a\u8bcd\u8bed\u6216\u8005\u6bcf\u4e2a\u5b57\u7684\u5411\u91cf\u957f\u5ea6\uff1b\n                \u4f8b\u5982\u6bcf\u6b21\u8bad\u7ec3\u8f93\u51652\u7bc7\u6587\u672c\uff0c\u6bcf\u7bc7\u6587\u672c\u6709100\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u957f\u5ea6\u4e3a20\uff0c\u90a3input\u7ef4\u5ea6\u5373\u4e3a(2, 100, 20)\u3002\n            filters\uff1a\u8fc7\u6ee4\u5668\uff08\u5377\u79ef\u6838\uff09\u7684\u6570\u76ee\n            kernel_size\uff1a\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u5377\u79ef\u6838\u672c\u8eab\u5e94\u8be5\u662f\u4e8c\u7ef4\u7684\uff0c\u8fd9\u91cc\u53ea\u9700\u8981\u6307\u5b9a\u4e00\u7ef4\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5373\u957f\u5ea6\u4e0e\u8bcd\u5411\u91cf\u7684\u957f\u5ea6\u4e00\u81f4\uff0c\u5377\u79ef\u6838\u53ea\u80fd\u4ece\u4e0a\u5f80\u4e0b\u8d70\uff0c\u4e0d\u80fd\u4ece\u5de6\u5f80\u53f3\u8d70\uff0c\u5373\u53ea\u80fd\u6309\u7167\u6587\u672c\u4e2d\u8bcd\u7684\u987a\u5e8f\uff0c\u4e5f\u662f\u5217\u7684\u987a\u5e8f\u3002\n        '''\n        # global max pooling layer\n        gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n\n        # \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b\n        fc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim\uff1a128\n        ''' https://blog.csdn.net/yangfengling1023/article/details/81774580\n        dense \uff1a\u5168\u8fde\u63a5\u5c42  inputs\uff1a\u8f93\u5165\u8be5\u7f51\u7edc\u5c42\u7684\u6570\u636e\uff1bunits\uff1a\u8f93\u51fa\u7684\u7ef4\u5ea6\u5927\u5c0f\uff0c\u6539\u53d8inputs\u7684\u6700\u540e\u4e00\u7ef4\n        '''\n        fc = tf.nn.dropout(fc, self.keep_prob)\n        fc = tf.nn.relu(fc)\n\n        # \u5206\u7c7b\u5668\n        logits = tf.layers.dense(fc, num_classes, name='fc2')\n        self.y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n\n        # \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.input_y)\n        self.loss = tf.reduce_mean(cross_entropy)\n        # \u4f18\u5316\u5668\n        self.optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n\n        # \u51c6\u786e\u7387\n        correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n        self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n        self.saver = tf.train.Saver()\n        \n    \n    def train(self, x_train, y_train, split_type, fold_id, num_epochs=20, dropout_keep_prob=0.5, print_per_batch=30, batch_size=64):\n        \n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        # \u521b\u5efasession\n        session = tf.Session()\n        session.run(tf.global_variables_initializer())\n        \n        print('Training and evaluating...')\n        start_time = time.time()\n        total_batch = 0  # \u603b\u6279\u6b21\n        best_acc_train = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n        last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n        require_improvement = 500  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n        flag = False\n\n        for epoch in range(num_epochs):  # 20\n            print('Epoch:', epoch + 1)\n            batch_train = rnn_cnn.batch_iter(x_train, y_train, batch_size)\n            for x_batch, y_batch in batch_train:\n                feed_dict = {self.input_x: x_batch, self.input_y: y_batch, self.keep_prob: dropout_keep_prob}\n                session.run(self.optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n                total_batch += 1\n\n                if total_batch % print_per_batch == 0:\n                    # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                    feed_dict[self.keep_prob] = 1.0\n                    loss_train, acc_train = session.run([self.loss, self.acc], feed_dict=feed_dict)\n                    # loss_val, acc_val = evaluate(session, x_dev, y_dev, loss, acc)\n                    if acc_train > best_acc_train:\n                        # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                        best_acc_train = acc_train\n                        last_improved = total_batch\n                        self.saver.save(sess=session, save_path=savePath)\n                        improved_str = '*'\n                    else:\n                        improved_str = ''\n\n                    time_dif = get_time_dif(start_time)\n                    msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, Time: {3} {4}'\n                    print(msg.format(total_batch, loss_train, acc_train, time_dif, improved_str))\n\n                if total_batch - last_improved > require_improvement:\n                    # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                    print(\"No optimization for a long time, auto-stopping...\")\n                    flag = True\n                    break  # \u8df3\u51fa\u5faa\u73af\n            if flag:  # \u540c\u4e0a\n                break\n                \n        session.close()\n        return best_acc_train\n        \n        \n    def evaluate_model(self, x_test, y_test, split_type, fold_id, categories, batch_size=64):\n        \n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n        session = tf.Session()\n        self.saver.restore(sess=session, save_path=savePath)\n        start_time = time.time()\n        print('Testing...')\n        loss_test, acc_test = self.evaluate(session, x_test, y_test, self.loss, self.acc, batch_size)\n        msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n        print(msg.format(loss_test, acc_test))\n\n        test_data_len = len(x_test)\n        test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n        y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n        y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n        for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n            start_id = i * batch_size\n            end_id = min((i + 1) * batch_size, test_data_len)\n            feed_dict = {\n                self.input_x: x_test[start_id:end_id],\n                self.keep_prob: 1.0\n            }\n            y_test_pred_cls[start_id:end_id] = session.run(self.y_pred_cls, feed_dict=feed_dict)\n\n        # \u8bc4\u4f30\n        print(\"Precision, Recall and F1-Score...\")\n        print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n        '''\n        sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n            y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n            y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n            labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n            target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n            \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n        '''\n\n        # \u6df7\u6dc6\u77e9\u9635\n        print(\"Confusion Matrix...\")\n        cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n        '''\n        \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n        \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n        https://blog.csdn.net/u011734144/article/details/80277225\n        '''\n        print(cm)\n\n        time_dif = get_time_dif(start_time)\n        print(\"Time usage:\", time_dif)\n        session.close()\n        \n        return acc_test\n        \n        \n    def predict(self, predict_sentences, word_to_id, pad_max_length, split_type, fold_id):\n        \"\"\"\n        \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n        \"\"\"\n        savePath = \"%s/%s/%s/%s\" % (self.savePath, split_type, fold_id, fold_id)\n        session = tf.Session()\n        self.saver.restore(sess=session, save_path=savePath)\n        \n        data_id = []\n        # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n        for i in range(len(predict_sentences)):\n            data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n\n        # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n        x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n        ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n        pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n            sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n            maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n            dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n            padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n            truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n            value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n        '''\n        feed_dict = {\n            self.input_x: x_pad,\n            self.keep_prob: 1.0\n        }\n        predict_result = session.run(self.y_pred_cls, feed_dict=feed_dict)\n        predict_result = [i+1 for i in predict_result]\n        session.close()\n        \n        return predict_result\n    \n    \n    def evaluate(self, sess, x_pad, y_pad, loss1, acc1, batch_size):\n        \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n        data_len = len(x_pad)\n        batch_eval = rnn_cnn.batch_iter(x_pad, y_pad, batch_size)  \n        total_loss = 0.0\n        total_acc = 0.0\n        # print(dropout_keep_prob)\n        for x_batch1, y_batch1 in batch_eval:\n            batch_len = len(x_batch1)\n            feed_dict1 = {self.input_x: x_batch1, self.input_y: y_batch1, self.keep_prob: 1.0}\n            lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n            total_loss += lossTmp * batch_len\n            total_acc += accTmp * batch_len\n\n        return total_loss / data_len, total_acc / data_len\n    \n    \n    def batch_iter(x_pad, y_pad, batch_size=64):  # 128\n        \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n        data_len = len(x_pad)\n        num_batch = int((data_len - 1) / batch_size) + 1\n        # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n        # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n        indices = np.random.permutation(np.arange(data_len))\n        x_shuffle = x_pad[indices]\n        y_shuffle = y_pad[indices]\n\n        # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n        for i in range(num_batch):\n            start_id = i * batch_size\n            end_id = min((i + 1) * batch_size, data_len)\n            yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n            \n            ", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "kf = KFold(n_splits=10)\nmodel = rnn_cnn(savePath, 6)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-9-10b7ccfe3726>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-9-10b7ccfe3726>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "fold_id = 0\nmodel_train_acc = []\nmodel_test_acc = []\nfor train_i, test_i in kf.split(x_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    model_train_acc.append(model.train(x_train[train_i], y_train[train_i],\"cnn\",fold_id))\n    model_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],\"cnn\",fold_id,categories))", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Fold:  1\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.3, Train Acc:  48.44%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  48.44%, Time: 0:00:08 \nEpoch: 2\nIter:     90, Train Loss:   0.85, Train Acc:  68.75%, Time: 0:00:13 *\nIter:    120, Train Loss:   0.75, Train Acc:  70.31%, Time: 0:00:19 *\nIter:    150, Train Loss:   0.72, Train Acc:  76.56%, Time: 0:00:25 *\nEpoch: 3\nIter:    180, Train Loss:    0.7, Train Acc:  76.56%, Time: 0:00:26 \nIter:    210, Train Loss:   0.76, Train Acc:  71.88%, Time: 0:00:26 \nIter:    240, Train Loss:    0.5, Train Acc:  84.38%, Time: 0:00:32 *\nEpoch: 4\nIter:    270, Train Loss:   0.43, Train Acc:  85.94%, Time: 0:00:39 *\nIter:    300, Train Loss:   0.47, Train Acc:  85.94%, Time: 0:00:39 \nIter:    330, Train Loss:   0.34, Train Acc:  85.94%, Time: 0:00:40 \nEpoch: 5\nIter:    360, Train Loss:   0.51, Train Acc:  84.38%, Time: 0:00:40 \nIter:    390, Train Loss:   0.26, Train Acc:  96.88%, Time: 0:00:46 *\nIter:    420, Train Loss:   0.27, Train Acc:  90.91%, Time: 0:00:47 \nEpoch: 6\nIter:    450, Train Loss:   0.19, Train Acc:  95.31%, Time: 0:00:47 \nIter:    480, Train Loss:   0.23, Train Acc:  93.75%, Time: 0:00:48 \nEpoch: 7\nIter:    510, Train Loss:   0.23, Train Acc:  92.19%, Time: 0:00:48 \nIter:    540, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:49 \nIter:    570, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:00:50 \nEpoch: 8\nIter:    600, Train Loss:   0.17, Train Acc:  95.31%, Time: 0:00:50 \nIter:    630, Train Loss:   0.13, Train Acc:  93.75%, Time: 0:00:51 \nIter:    660, Train Loss:  0.095, Train Acc: 100.00%, Time: 0:00:57 *\nEpoch: 9\nIter:    690, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:00:58 \nIter:    720, Train Loss:    0.1, Train Acc:  95.31%, Time: 0:00:58 \nIter:    750, Train Loss:  0.049, Train Acc:  98.44%, Time: 0:00:59 \nEpoch: 10\nIter:    780, Train Loss:  0.059, Train Acc:  96.88%, Time: 0:01:00 \nIter:    810, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:01:00 \nIter:    840, Train Loss:   0.16, Train Acc:  95.45%, Time: 0:01:01 \nEpoch: 11\nIter:    870, Train Loss:  0.048, Train Acc:  98.44%, Time: 0:01:01 \nIter:    900, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:01:02 \nEpoch: 12\nIter:    930, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:01:03 \nIter:    960, Train Loss:    0.2, Train Acc:  92.19%, Time: 0:01:03 \nIter:    990, Train Loss:  0.086, Train Acc:  96.88%, Time: 0:01:04 \nEpoch: 13\nIter:   1020, Train Loss:  0.048, Train Acc:  96.88%, Time: 0:01:04 \nIter:   1050, Train Loss:   0.12, Train Acc:  93.75%, Time: 0:01:05 \nIter:   1080, Train Loss:   0.14, Train Acc:  95.31%, Time: 0:01:05 \nEpoch: 14\nIter:   1110, Train Loss:  0.099, Train Acc:  96.88%, Time: 0:01:06 \nIter:   1140, Train Loss:  0.064, Train Acc:  96.88%, Time: 0:01:07 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/1/1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/1/1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.93, Test Acc:  73.83%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.40      0.57        10\n        DESC       0.65      0.77      0.70       126\n        ENTY       0.60      0.79      0.68       136\n         HUM       0.88      0.71      0.79       137\n         LOC       0.82      0.70      0.76        88\n         NUM       0.94      0.74      0.82        99\n\n   micro avg       0.74      0.74      0.74       596\n   macro avg       0.81      0.68      0.72       596\nweighted avg       0.77      0.74      0.74       596\n\nConfusion Matrix...\n[[  4   6   0   0   0   0]\n [  0  97  20   3   5   1]\n [  0  19 107   6   4   0]\n [  0   9  26  97   2   3]\n [  0  12  11   2  62   1]\n [  0   7  14   2   3  73]]\nTime usage: 0:00:02\nFold:  2\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.3, Train Acc:  48.44%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  50.00%, Time: 0:00:13 *\nEpoch: 2\nIter:     90, Train Loss:    0.9, Train Acc:  70.31%, Time: 0:00:18 *\nIter:    120, Train Loss:   0.95, Train Acc:  71.88%, Time: 0:00:23 *\nIter:    150, Train Loss:   0.74, Train Acc:  73.44%, Time: 0:00:27 *\nEpoch: 3\nIter:    180, Train Loss:   0.74, Train Acc:  71.88%, Time: 0:00:27 \nIter:    210, Train Loss:   0.65, Train Acc:  76.56%, Time: 0:00:32 *\nIter:    240, Train Loss:   0.73, Train Acc:  70.31%, Time: 0:00:32 \nEpoch: 4\nIter:    270, Train Loss:   0.57, Train Acc:  81.25%, Time: 0:00:36 *\nIter:    300, Train Loss:   0.49, Train Acc:  87.50%, Time: 0:00:40 *\nIter:    330, Train Loss:    0.5, Train Acc:  82.81%, Time: 0:00:41 \nEpoch: 5\nIter:    360, Train Loss:   0.43, Train Acc:  82.81%, Time: 0:00:41 \nIter:    390, Train Loss:   0.45, Train Acc:  81.25%, Time: 0:00:42 \nIter:    420, Train Loss:   0.39, Train Acc:  95.45%, Time: 0:00:47 *\nEpoch: 6\nIter:    450, Train Loss:   0.19, Train Acc:  93.75%, Time: 0:00:48 \nIter:    480, Train Loss:   0.24, Train Acc:  90.62%, Time: 0:00:48 \nEpoch: 7\nIter:    510, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:00:52 *\nIter:    540, Train Loss:   0.22, Train Acc:  92.19%, Time: 0:00:53 \nIter:    570, Train Loss:   0.24, Train Acc:  92.19%, Time: 0:00:54 \nEpoch: 8\nIter:    600, Train Loss:   0.16, Train Acc:  96.88%, Time: 0:00:54 \nIter:    630, Train Loss:   0.24, Train Acc:  90.62%, Time: 0:00:55 \nIter:    660, Train Loss:   0.32, Train Acc:  89.06%, Time: 0:00:55 \nEpoch: 9\nIter:    690, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:56 \nIter:    720, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:00:57 \nIter:    750, Train Loss:   0.11, Train Acc:  93.75%, Time: 0:00:57 \nEpoch: 10\nIter:    780, Train Loss:   0.16, Train Acc:  95.31%, Time: 0:00:58 \nIter:    810, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:00:58 \nIter:    840, Train Loss:   0.16, Train Acc:  90.91%, Time: 0:00:59 \nEpoch: 11\nIter:    870, Train Loss:   0.11, Train Acc:  95.31%, Time: 0:01:00 \nIter:    900, Train Loss:   0.02, Train Acc: 100.00%, Time: 0:01:04 *\nEpoch: 12\nIter:    930, Train Loss:  0.045, Train Acc:  98.44%, Time: 0:01:05 \nIter:    960, Train Loss:   0.11, Train Acc:  98.44%, Time: 0:01:05 \nIter:    990, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:06 \nEpoch: 13\nIter:   1020, Train Loss:  0.055, Train Acc:  96.88%, Time: 0:01:06 \nIter:   1050, Train Loss:  0.024, Train Acc: 100.00%, Time: 0:01:07 \nIter:   1080, Train Loss:   0.19, Train Acc:  92.19%, Time: 0:01:08 \nEpoch: 14\nIter:   1110, Train Loss:   0.11, Train Acc:  95.31%, Time: 0:01:08 \nIter:   1140, Train Loss:  0.015, Train Acc: 100.00%, Time: 0:01:09 \nIter:   1170, Train Loss:   0.14, Train Acc:  93.75%, Time: 0:01:09 \nEpoch: 15\nIter:   1200, Train Loss:  0.041, Train Acc:  98.44%, Time: 0:01:10 \nIter:   1230, Train Loss:  0.035, Train Acc:  98.44%, Time: 0:01:11 \nIter:   1260, Train Loss:  0.022, Train Acc: 100.00%, Time: 0:01:11 \nEpoch: 16\nIter:   1290, Train Loss:  0.055, Train Acc:  96.88%, Time: 0:01:12 \nIter:   1320, Train Loss:    0.1, Train Acc:  93.75%, Time: 0:01:12 \nEpoch: 17\nIter:   1350, Train Loss:  0.057, Train Acc:  98.44%, Time: 0:01:13 \nIter:   1380, Train Loss:  0.044, Train Acc:  98.44%, Time: 0:01:13 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/2/2\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/2/2\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.89, Test Acc:  76.85%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.67      0.80         6\n        DESC       0.71      0.77      0.74       138\n        ENTY       0.70      0.76      0.73       139\n         HUM       0.84      0.73      0.78       147\n         LOC       0.74      0.84      0.78        80\n         NUM       0.93      0.79      0.86        86\n\n   micro avg       0.77      0.77      0.77       596\n   macro avg       0.82      0.76      0.78       596\nweighted avg       0.78      0.77      0.77       596\n\nConfusion Matrix...\n[[  4   1   0   0   1   0]\n [  0 106  15   6  10   1]\n [  0  17 105  10   4   3]\n [  0  11  22 108   6   0]\n [  0   9   2   1  67   1]\n [  0   5   6   4   3  68]]\nTime usage: 0:00:02\nFold:  3\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.3, Train Acc:  51.56%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.1, Train Acc:  53.12%, Time: 0:00:12 *\nEpoch: 2\nIter:     90, Train Loss:    0.9, Train Acc:  60.94%, Time: 0:00:16 *\nIter:    120, Train Loss:   0.83, Train Acc:  67.19%, Time: 0:00:20 *\nIter:    150, Train Loss:   0.68, Train Acc:  78.12%, Time: 0:00:25 *\nEpoch: 3\nIter:    180, Train Loss:   0.75, Train Acc:  73.44%, Time: 0:00:26 \nIter:    210, Train Loss:   0.67, Train Acc:  76.56%, Time: 0:00:26 \nIter:    240, Train Loss:   0.72, Train Acc:  78.12%, Time: 0:00:27 \nEpoch: 4\nIter:    270, Train Loss:   0.44, Train Acc:  87.50%, Time: 0:00:32 *\nIter:    300, Train Loss:   0.47, Train Acc:  79.69%, Time: 0:00:32 \nIter:    330, Train Loss:   0.41, Train Acc:  85.94%, Time: 0:00:33 \nEpoch: 5\nIter:    360, Train Loss:   0.47, Train Acc:  87.50%, Time: 0:00:34 \nIter:    390, Train Loss:   0.32, Train Acc:  89.06%, Time: 0:00:38 *\nIter:    420, Train Loss:   0.26, Train Acc:  93.33%, Time: 0:00:43 *\nEpoch: 6\nIter:    450, Train Loss:   0.24, Train Acc:  92.19%, Time: 0:00:43 \nIter:    480, Train Loss:   0.35, Train Acc:  87.50%, Time: 0:00:44 \nEpoch: 7\nIter:    510, Train Loss:   0.21, Train Acc:  92.19%, Time: 0:00:44 \nIter:    540, Train Loss:   0.34, Train Acc:  85.94%, Time: 0:00:45 \nIter:    570, Train Loss:   0.19, Train Acc:  92.19%, Time: 0:00:45 \nEpoch: 8\nIter:    600, Train Loss:   0.19, Train Acc:  93.75%, Time: 0:00:50 *\nIter:    630, Train Loss:   0.25, Train Acc:  89.06%, Time: 0:00:50 \nIter:    660, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:00:55 *\nEpoch: 9\nIter:    690, Train Loss:  0.075, Train Acc: 100.00%, Time: 0:00:59 *\nIter:    720, Train Loss:   0.18, Train Acc:  90.62%, Time: 0:00:59 \nIter:    750, Train Loss:    0.2, Train Acc:  90.62%, Time: 0:01:00 \nEpoch: 10\nIter:    780, Train Loss:   0.07, Train Acc: 100.00%, Time: 0:01:00 \nIter:    810, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:01:01 \nIter:    840, Train Loss:   0.13, Train Acc:  95.56%, Time: 0:01:02 \nEpoch: 11\nIter:    870, Train Loss:  0.078, Train Acc:  98.44%, Time: 0:01:02 \nIter:    900, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:01:03 \nEpoch: 12\nIter:    930, Train Loss:  0.022, Train Acc: 100.00%, Time: 0:01:03 \nIter:    960, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:01:04 \nIter:    990, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:01:04 \nEpoch: 13\nIter:   1020, Train Loss:  0.051, Train Acc:  96.88%, Time: 0:01:05 \nIter:   1050, Train Loss:   0.18, Train Acc:  96.88%, Time: 0:01:06 \nIter:   1080, Train Loss:   0.17, Train Acc:  93.75%, Time: 0:01:06 \nEpoch: 14\nIter:   1110, Train Loss:  0.052, Train Acc:  98.44%, Time: 0:01:07 \nIter:   1140, Train Loss:   0.14, Train Acc:  95.31%, Time: 0:01:08 \nIter:   1170, Train Loss:  0.047, Train Acc:  98.44%, Time: 0:01:08 \nEpoch: 15\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/3/3\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/3/3\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.73, Test Acc:  76.81%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.50      0.67         8\n        DESC       0.73      0.74      0.73       126\n        ENTY       0.69      0.71      0.70       142\n         HUM       0.77      0.86      0.81       130\n         LOC       0.83      0.76      0.80        92\n         NUM       0.89      0.79      0.84        97\n\n   micro avg       0.77      0.77      0.77       595\n   macro avg       0.82      0.73      0.76       595\nweighted avg       0.77      0.77      0.77       595\n\nConfusion Matrix...\n[[  4   1   2   1   0   0]\n [  0  93  18   7   6   2]\n [  0  23 101  12   2   4]\n [  0   6  10 112   2   0]\n [  0   3   8   7  70   4]\n [  0   2   8   6   4  77]]\nTime usage: 0:00:02\nFold:  4\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.2, Train Acc:  51.56%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  56.25%, Time: 0:00:11 *\nEpoch: 2\nIter:     90, Train Loss:   0.87, Train Acc:  73.44%, Time: 0:00:16 *\nIter:    120, Train Loss:   0.76, Train Acc:  78.12%, Time: 0:00:20 *\nIter:    150, Train Loss:   0.76, Train Acc:  71.88%, Time: 0:00:21 \nEpoch: 3\nIter:    180, Train Loss:   0.51, Train Acc:  87.50%, Time: 0:00:25 *\nIter:    210, Train Loss:   0.59, Train Acc:  78.12%, Time: 0:00:25 \nIter:    240, Train Loss:   0.46, Train Acc:  84.38%, Time: 0:00:26 \nEpoch: 4\nIter:    270, Train Loss:   0.51, Train Acc:  82.81%, Time: 0:00:27 \nIter:    300, Train Loss:   0.41, Train Acc:  89.06%, Time: 0:00:31 *\nIter:    330, Train Loss:   0.46, Train Acc:  82.81%, Time: 0:00:31 \nEpoch: 5\nIter:    360, Train Loss:   0.43, Train Acc:  82.81%, Time: 0:00:32 \nIter:    390, Train Loss:   0.41, Train Acc:  85.94%, Time: 0:00:32 \nIter:    420, Train Loss:    0.3, Train Acc:  86.67%, Time: 0:00:33 \nEpoch: 6\nIter:    450, Train Loss:   0.31, Train Acc:  87.50%, Time: 0:00:34 \nIter:    480, Train Loss:   0.33, Train Acc:  87.50%, Time: 0:00:34 \nEpoch: 7\nIter:    510, Train Loss:   0.18, Train Acc:  98.44%, Time: 0:00:39 *\nIter:    540, Train Loss:   0.21, Train Acc:  95.31%, Time: 0:00:39 \nIter:    570, Train Loss:   0.16, Train Acc:  93.75%, Time: 0:00:40 \nEpoch: 8\nIter:    600, Train Loss:    0.2, Train Acc:  93.75%, Time: 0:00:40 \nIter:    630, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:41 \nIter:    660, Train Loss:   0.26, Train Acc:  90.62%, Time: 0:00:42 \nEpoch: 9\nIter:    690, Train Loss:   0.12, Train Acc:  96.88%, Time: 0:00:42 \nIter:    720, Train Loss:  0.073, Train Acc:  98.44%, Time: 0:00:43 \nIter:    750, Train Loss:   0.21, Train Acc:  93.75%, Time: 0:00:43 \nEpoch: 10\nIter:    780, Train Loss:   0.14, Train Acc:  93.75%, Time: 0:00:44 \nIter:    810, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:44 \nIter:    840, Train Loss:  0.069, Train Acc:  97.78%, Time: 0:00:45 \nEpoch: 11\nIter:    870, Train Loss:  0.046, Train Acc: 100.00%, Time: 0:00:50 *\nIter:    900, Train Loss:  0.056, Train Acc:  98.44%, Time: 0:00:50 \nEpoch: 12\nIter:    930, Train Loss:  0.087, Train Acc:  98.44%, Time: 0:00:51 \nIter:    960, Train Loss:  0.063, Train Acc:  98.44%, Time: 0:00:51 \nIter:    990, Train Loss:  0.091, Train Acc:  96.88%, Time: 0:00:52 \nEpoch: 13\nIter:   1020, Train Loss:  0.096, Train Acc:  95.31%, Time: 0:00:53 \nIter:   1050, Train Loss:  0.074, Train Acc:  98.44%, Time: 0:00:53 \nIter:   1080, Train Loss:  0.049, Train Acc:  98.44%, Time: 0:00:54 \nEpoch: 14\nIter:   1110, Train Loss:   0.16, Train Acc:  95.31%, Time: 0:00:54 \nIter:   1140, Train Loss:   0.11, Train Acc:  93.75%, Time: 0:00:55 \nIter:   1170, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:56 \nEpoch: 15\nIter:   1200, Train Loss:  0.078, Train Acc:  95.31%, Time: 0:00:56 \nIter:   1230, Train Loss:  0.063, Train Acc:  98.44%, Time: 0:00:57 \nIter:   1260, Train Loss:   0.14, Train Acc:  95.56%, Time: 0:00:57 \nEpoch: 16\nIter:   1290, Train Loss:  0.051, Train Acc:  98.44%, Time: 0:00:58 \nIter:   1320, Train Loss:  0.061, Train Acc:  96.88%, Time: 0:00:59 \nEpoch: 17\nIter:   1350, Train Loss:  0.023, Train Acc: 100.00%, Time: 0:00:59 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/4/4\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/4/4\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:    0.8, Test Acc:  78.66%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.83      0.45      0.59        11\n        DESC       0.76      0.76      0.76       141\n        ENTY       0.70      0.80      0.75       132\n         HUM       0.82      0.76      0.79       123\n         LOC       0.88      0.79      0.83        89\n         NUM       0.84      0.87      0.86        99\n\n   micro avg       0.79      0.79      0.79       595\n   macro avg       0.80      0.74      0.76       595\nweighted avg       0.79      0.79      0.79       595\n\nConfusion Matrix...\n[[  5   4   2   0   0   0]\n [  1 107  17   6   3   7]\n [  0  11 106  11   2   2]\n [  0   6  19  94   2   2]\n [  0   9   4   1  70   5]\n [  0   4   3   3   3  86]]\nTime usage: 0:00:02\nFold:  5\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.4, Train Acc:  48.44%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  56.25%, Time: 0:00:11 *\nEpoch: 2\nIter:     90, Train Loss:   0.81, Train Acc:  71.88%, Time: 0:00:15 *\nIter:    120, Train Loss:   0.71, Train Acc:  71.88%, Time: 0:00:16 \nIter:    150, Train Loss:   0.74, Train Acc:  68.75%, Time: 0:00:17 \nEpoch: 3\nIter:    180, Train Loss:   0.48, Train Acc:  90.62%, Time: 0:00:21 *\nIter:    210, Train Loss:   0.68, Train Acc:  76.56%, Time: 0:00:22 \nIter:    240, Train Loss:   0.65, Train Acc:  78.12%, Time: 0:00:22 \nEpoch: 4\nIter:    270, Train Loss:   0.38, Train Acc:  89.06%, Time: 0:00:23 \nIter:    300, Train Loss:   0.37, Train Acc:  85.94%, Time: 0:00:23 \nIter:    330, Train Loss:   0.36, Train Acc:  89.06%, Time: 0:00:24 \nEpoch: 5\nIter:    360, Train Loss:   0.64, Train Acc:  75.00%, Time: 0:00:24 \nIter:    390, Train Loss:    0.3, Train Acc:  90.62%, Time: 0:00:25 \nIter:    420, Train Loss:   0.29, Train Acc:  88.89%, Time: 0:00:26 \nEpoch: 6\nIter:    450, Train Loss:   0.26, Train Acc:  90.62%, Time: 0:00:26 \nIter:    480, Train Loss:   0.31, Train Acc:  92.19%, Time: 0:00:31 *\nEpoch: 7\nIter:    510, Train Loss:   0.15, Train Acc:  95.31%, Time: 0:00:36 *\nIter:    540, Train Loss:   0.16, Train Acc:  93.75%, Time: 0:00:36 \nIter:    570, Train Loss:   0.18, Train Acc:  95.31%, Time: 0:00:37 \nEpoch: 8\nIter:    600, Train Loss:   0.14, Train Acc:  95.31%, Time: 0:00:37 \nIter:    630, Train Loss:   0.18, Train Acc:  92.19%, Time: 0:00:38 \nIter:    660, Train Loss:   0.17, Train Acc:  93.75%, Time: 0:00:38 \nEpoch: 9\nIter:    690, Train Loss:   0.12, Train Acc:  96.88%, Time: 0:00:43 *\nIter:    720, Train Loss:   0.21, Train Acc:  92.19%, Time: 0:00:43 \nIter:    750, Train Loss:   0.18, Train Acc:  93.75%, Time: 0:00:44 \nEpoch: 10\nIter:    780, Train Loss:  0.079, Train Acc:  98.44%, Time: 0:00:49 *\nIter:    810, Train Loss:   0.24, Train Acc:  93.75%, Time: 0:00:50 \nIter:    840, Train Loss:  0.082, Train Acc: 100.00%, Time: 0:00:54 *\nEpoch: 11\nIter:    870, Train Loss:   0.17, Train Acc:  90.62%, Time: 0:00:55 \nIter:    900, Train Loss:  0.072, Train Acc:  98.44%, Time: 0:00:55 \nEpoch: 12\nIter:    930, Train Loss:  0.061, Train Acc:  98.44%, Time: 0:00:56 \nIter:    960, Train Loss:   0.13, Train Acc:  96.88%, Time: 0:00:57 \nIter:    990, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:00:57 \nEpoch: 13\nIter:   1020, Train Loss:   0.15, Train Acc:  92.19%, Time: 0:00:58 \nIter:   1050, Train Loss:  0.073, Train Acc:  98.44%, Time: 0:00:58 \nIter:   1080, Train Loss:  0.037, Train Acc: 100.00%, Time: 0:00:59 \nEpoch: 14\nIter:   1110, Train Loss:  0.037, Train Acc:  98.44%, Time: 0:01:00 \nIter:   1140, Train Loss:  0.065, Train Acc:  98.44%, Time: 0:01:00 \nIter:   1170, Train Loss:  0.046, Train Acc:  96.88%, Time: 0:01:01 \nEpoch: 15\nIter:   1200, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:01 \nIter:   1230, Train Loss:  0.063, Train Acc:  98.44%, Time: 0:01:02 \nIter:   1260, Train Loss:   0.11, Train Acc:  95.56%, Time: 0:01:02 \nEpoch: 16\nIter:   1290, Train Loss:  0.062, Train Acc:  98.44%, Time: 0:01:03 \nIter:   1320, Train Loss:  0.069, Train Acc:  96.88%, Time: 0:01:04 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/5/5\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/5/5\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.74, Test Acc:  80.50%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.22      0.36         9\n        DESC       0.75      0.80      0.77       111\n        ENTY       0.74      0.71      0.73       114\n         HUM       0.79      0.89      0.84       152\n         LOC       0.85      0.75      0.80        93\n         NUM       0.91      0.88      0.89       116\n\n   micro avg       0.81      0.81      0.81       595\n   macro avg       0.84      0.71      0.73       595\nweighted avg       0.81      0.81      0.80       595\n\nConfusion Matrix...\n[[  2   4   3   0   0   0]\n [  0  89  10   8   4   0]\n [  0  11  81  14   4   4]\n [  0   4   8 135   2   3]\n [  0   8   4   8  70   3]\n [  0   3   3   6   2 102]]\nTime usage: 0:00:02\nFold:  6\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.4, Train Acc:  45.31%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  53.12%, Time: 0:00:11 *\nEpoch: 2\nIter:     90, Train Loss:   0.81, Train Acc:  79.69%, Time: 0:00:15 *\nIter:    120, Train Loss:    0.9, Train Acc:  76.56%, Time: 0:00:16 \nIter:    150, Train Loss:   0.81, Train Acc:  71.88%, Time: 0:00:16 \nEpoch: 3\nIter:    180, Train Loss:   0.68, Train Acc:  76.56%, Time: 0:00:17 \nIter:    210, Train Loss:   0.69, Train Acc:  79.69%, Time: 0:00:18 \nIter:    240, Train Loss:   0.61, Train Acc:  76.56%, Time: 0:00:18 \nEpoch: 4\nIter:    270, Train Loss:    0.3, Train Acc:  92.19%, Time: 0:00:23 *\nIter:    300, Train Loss:   0.48, Train Acc:  79.69%, Time: 0:00:24 \nIter:    330, Train Loss:   0.56, Train Acc:  84.38%, Time: 0:00:24 \nEpoch: 5\nIter:    360, Train Loss:   0.46, Train Acc:  82.81%, Time: 0:00:25 \nIter:    390, Train Loss:   0.34, Train Acc:  84.38%, Time: 0:00:26 \nIter:    420, Train Loss:   0.45, Train Acc:  84.44%, Time: 0:00:26 \nEpoch: 6\nIter:    450, Train Loss:   0.18, Train Acc:  93.75%, Time: 0:00:31 *\nIter:    480, Train Loss:   0.22, Train Acc:  93.75%, Time: 0:00:32 \nEpoch: 7\nIter:    510, Train Loss:    0.2, Train Acc:  93.75%, Time: 0:00:32 \nIter:    540, Train Loss:    0.1, Train Acc: 100.00%, Time: 0:00:37 *\nIter:    570, Train Loss:   0.22, Train Acc:  93.75%, Time: 0:00:37 \nEpoch: 8\nIter:    600, Train Loss:  0.051, Train Acc: 100.00%, Time: 0:00:38 \nIter:    630, Train Loss:   0.12, Train Acc:  96.88%, Time: 0:00:39 \nIter:    660, Train Loss:   0.18, Train Acc:  95.31%, Time: 0:00:39 \nEpoch: 9\nIter:    690, Train Loss:   0.11, Train Acc:  98.44%, Time: 0:00:40 \nIter:    720, Train Loss:  0.096, Train Acc:  96.88%, Time: 0:00:40 \nIter:    750, Train Loss:   0.19, Train Acc:  89.06%, Time: 0:00:41 \nEpoch: 10\nIter:    780, Train Loss:   0.12, Train Acc:  98.44%, Time: 0:00:42 \nIter:    810, Train Loss:  0.089, Train Acc: 100.00%, Time: 0:00:42 \nIter:    840, Train Loss:   0.08, Train Acc:  97.78%, Time: 0:00:43 \nEpoch: 11\nIter:    870, Train Loss:   0.12, Train Acc:  96.88%, Time: 0:00:43 \nIter:    900, Train Loss:  0.076, Train Acc:  95.31%, Time: 0:00:44 \nEpoch: 12\nIter:    930, Train Loss:  0.053, Train Acc: 100.00%, Time: 0:00:45 \nIter:    960, Train Loss:  0.056, Train Acc:  98.44%, Time: 0:00:45 \nIter:    990, Train Loss:   0.14, Train Acc:  93.75%, Time: 0:00:46 \nEpoch: 13\nIter:   1020, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:00:46 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/6/6\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/6/6\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.74, Test Acc:  75.13%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.86      1.00      0.92         6\n        DESC       0.62      0.78      0.69       120\n        ENTY       0.81      0.65      0.72       150\n         HUM       0.74      0.81      0.78       134\n         LOC       0.88      0.63      0.73        97\n         NUM       0.80      0.90      0.84        88\n\n   micro avg       0.75      0.75      0.75       595\n   macro avg       0.78      0.80      0.78       595\nweighted avg       0.77      0.75      0.75       595\n\nConfusion Matrix...\n[[  6   0   0   0   0   0]\n [  0  94  12   7   3   4]\n [  0  23  98  19   2   8]\n [  1  12   5 109   2   5]\n [  0  21   3   9  61   3]\n [  0   2   3   3   1  79]]\nTime usage: 0:00:02\nFold:  7\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.4, Train Acc:  45.31%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.1, Train Acc:  60.94%, Time: 0:00:11 *\nEpoch: 2\nIter:     90, Train Loss:   0.82, Train Acc:  76.56%, Time: 0:00:15 *\nIter:    120, Train Loss:   0.87, Train Acc:  71.88%, Time: 0:00:15 \nIter:    150, Train Loss:   0.76, Train Acc:  65.62%, Time: 0:00:16 \nEpoch: 3\nIter:    180, Train Loss:   0.56, Train Acc:  82.81%, Time: 0:00:20 *\nIter:    210, Train Loss:   0.63, Train Acc:  81.25%, Time: 0:00:21 \nIter:    240, Train Loss:   0.39, Train Acc:  89.06%, Time: 0:00:25 *\nEpoch: 4\nIter:    270, Train Loss:   0.43, Train Acc:  90.62%, Time: 0:00:28 *\nIter:    300, Train Loss:   0.67, Train Acc:  76.56%, Time: 0:00:29 \nIter:    330, Train Loss:   0.51, Train Acc:  84.38%, Time: 0:00:30 \nEpoch: 5\nIter:    360, Train Loss:   0.25, Train Acc:  93.75%, Time: 0:00:33 *\nIter:    390, Train Loss:    0.4, Train Acc:  87.50%, Time: 0:00:34 \nIter:    420, Train Loss:   0.43, Train Acc:  77.78%, Time: 0:00:35 \nEpoch: 6\nIter:    450, Train Loss:   0.17, Train Acc:  96.88%, Time: 0:00:38 *\nIter:    480, Train Loss:   0.38, Train Acc:  85.94%, Time: 0:00:39 \nEpoch: 7\nIter:    510, Train Loss:   0.25, Train Acc:  89.06%, Time: 0:00:40 \nIter:    540, Train Loss:   0.14, Train Acc:  96.88%, Time: 0:00:40 \nIter:    570, Train Loss:   0.25, Train Acc:  95.31%, Time: 0:00:41 \nEpoch: 8\nIter:    600, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:00:41 \nIter:    630, Train Loss:   0.11, Train Acc:  95.31%, Time: 0:00:42 \nIter:    660, Train Loss:  0.096, Train Acc:  96.88%, Time: 0:00:43 \nEpoch: 9\nIter:    690, Train Loss:   0.15, Train Acc:  95.31%, Time: 0:00:43 \nIter:    720, Train Loss:  0.084, Train Acc:  98.44%, Time: 0:00:48 *\nIter:    750, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:00:49 \nEpoch: 10\nIter:    780, Train Loss:  0.085, Train Acc:  98.44%, Time: 0:00:49 \nIter:    810, Train Loss:  0.061, Train Acc:  98.44%, Time: 0:00:50 \nIter:    840, Train Loss:   0.12, Train Acc:  95.56%, Time: 0:00:50 \nEpoch: 11\nIter:    870, Train Loss:   0.18, Train Acc:  93.75%, Time: 0:00:51 \nIter:    900, Train Loss:   0.19, Train Acc:  92.19%, Time: 0:00:52 \nEpoch: 12\nIter:    930, Train Loss:   0.09, Train Acc:  96.88%, Time: 0:00:52 \nIter:    960, Train Loss:   0.13, Train Acc:  96.88%, Time: 0:00:53 \nIter:    990, Train Loss:   0.06, Train Acc:  96.88%, Time: 0:00:53 \nEpoch: 13\nIter:   1020, Train Loss:  0.054, Train Acc:  98.44%, Time: 0:00:54 \nIter:   1050, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:00:55 \nIter:   1080, Train Loss:  0.069, Train Acc:  96.88%, Time: 0:00:55 \nEpoch: 14\nIter:   1110, Train Loss:  0.059, Train Acc:  98.44%, Time: 0:00:56 \nIter:   1140, Train Loss:  0.072, Train Acc:  98.44%, Time: 0:00:56 \nIter:   1170, Train Loss:  0.032, Train Acc: 100.00%, Time: 0:01:01 *\nEpoch: 15\nIter:   1200, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:01:02 \nIter:   1230, Train Loss:  0.053, Train Acc:  98.44%, Time: 0:01:02 \nIter:   1260, Train Loss:  0.062, Train Acc:  97.78%, Time: 0:01:03 \nEpoch: 16\nIter:   1290, Train Loss:  0.051, Train Acc:  96.88%, Time: 0:01:04 \nIter:   1320, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:04 \nEpoch: 17\nIter:   1350, Train Loss:  0.033, Train Acc: 100.00%, Time: 0:01:05 \nIter:   1380, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:01:05 \nIter:   1410, Train Loss:  0.079, Train Acc:  95.31%, Time: 0:01:06 \nEpoch: 18\nIter:   1440, Train Loss:  0.084, Train Acc:  95.31%, Time: 0:01:07 \nIter:   1470, Train Loss:  0.035, Train Acc:  98.44%, Time: 0:01:07 \nIter:   1500, Train Loss:  0.042, Train Acc: 100.00%, Time: 0:01:08 \nEpoch: 19\nIter:   1530, Train Loss:   0.02, Train Acc: 100.00%, Time: 0:01:08 \nIter:   1560, Train Loss:    0.1, Train Acc:  95.31%, Time: 0:01:09 \nIter:   1590, Train Loss:  0.066, Train Acc:  98.44%, Time: 0:01:09 \nEpoch: 20\nIter:   1620, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:01:10 \nIter:   1650, Train Loss:  0.046, Train Acc:  96.88%, Time: 0:01:11 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/7/7\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/7/7\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.89, Test Acc:  75.97%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.86      0.60      0.71        10\n        DESC       0.72      0.70      0.71       115\n        ENTY       0.65      0.73      0.69       139\n         HUM       0.80      0.76      0.78       136\n         LOC       0.88      0.75      0.81        89\n         NUM       0.81      0.88      0.84       106\n\n   micro avg       0.76      0.76      0.76       595\n   macro avg       0.79      0.74      0.76       595\nweighted avg       0.77      0.76      0.76       595\n\nConfusion Matrix...\n[[  6   3   1   0   0   0]\n [  1  80  23   2   4   5]\n [  0  10 102  15   2  10]\n [  0   6  22 104   2   2]\n [  0   6   5   6  67   5]\n [  0   6   3   3   1  93]]\nTime usage: 0:00:02\nFold:  8\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.3, Train Acc:  43.75%, Time: 0:00:08 *\nIter:     60, Train Loss:    1.1, Train Acc:  60.94%, Time: 0:00:13 *\nEpoch: 2\nIter:     90, Train Loss:   0.81, Train Acc:  78.12%, Time: 0:00:17 *\nIter:    120, Train Loss:   0.94, Train Acc:  64.06%, Time: 0:00:17 \nIter:    150, Train Loss:    0.7, Train Acc:  79.69%, Time: 0:00:22 *\nEpoch: 3\nIter:    180, Train Loss:   0.64, Train Acc:  71.88%, Time: 0:00:23 \nIter:    210, Train Loss:   0.77, Train Acc:  68.75%, Time: 0:00:23 \nIter:    240, Train Loss:   0.68, Train Acc:  78.12%, Time: 0:00:24 \nEpoch: 4\nIter:    270, Train Loss:   0.52, Train Acc:  81.25%, Time: 0:00:28 *\nIter:    300, Train Loss:   0.48, Train Acc:  81.25%, Time: 0:00:28 \nIter:    330, Train Loss:   0.47, Train Acc:  82.81%, Time: 0:00:34 *\nEpoch: 5\nIter:    360, Train Loss:   0.49, Train Acc:  85.94%, Time: 0:00:39 *\nIter:    390, Train Loss:   0.43, Train Acc:  84.38%, Time: 0:00:40 \nIter:    420, Train Loss:   0.27, Train Acc:  95.56%, Time: 0:00:44 *\nEpoch: 6\nIter:    450, Train Loss:   0.24, Train Acc:  87.50%, Time: 0:00:44 \nIter:    480, Train Loss:    0.4, Train Acc:  84.38%, Time: 0:00:45 \nEpoch: 7\nIter:    510, Train Loss:   0.23, Train Acc:  93.75%, Time: 0:00:46 \nIter:    540, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:00:50 *\nIter:    570, Train Loss:   0.26, Train Acc:  92.19%, Time: 0:00:51 \nEpoch: 8\nIter:    600, Train Loss:  0.078, Train Acc:  98.44%, Time: 0:00:55 *\nIter:    630, Train Loss:   0.14, Train Acc:  95.31%, Time: 0:00:56 \nIter:    660, Train Loss:   0.33, Train Acc:  89.06%, Time: 0:00:57 \nEpoch: 9\nIter:    690, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:00:58 \nIter:    720, Train Loss:   0.18, Train Acc:  92.19%, Time: 0:00:59 \nIter:    750, Train Loss:  0.065, Train Acc:  98.44%, Time: 0:01:00 \nEpoch: 10\nIter:    780, Train Loss:  0.072, Train Acc:  96.88%, Time: 0:01:01 \nIter:    810, Train Loss:   0.13, Train Acc:  93.75%, Time: 0:01:02 \nIter:    840, Train Loss:   0.12, Train Acc:  95.56%, Time: 0:01:02 \nEpoch: 11\nIter:    870, Train Loss:  0.083, Train Acc:  96.88%, Time: 0:01:03 \nIter:    900, Train Loss:   0.18, Train Acc:  95.31%, Time: 0:01:04 \nEpoch: 12\nIter:    930, Train Loss:  0.091, Train Acc:  98.44%, Time: 0:01:04 \nIter:    960, Train Loss:  0.055, Train Acc:  96.88%, Time: 0:01:05 \nIter:    990, Train Loss:  0.052, Train Acc:  98.44%, Time: 0:01:05 \nEpoch: 13\nIter:   1020, Train Loss:  0.028, Train Acc: 100.00%, Time: 0:01:11 *\nIter:   1050, Train Loss:  0.043, Train Acc:  98.44%, Time: 0:01:12 \nIter:   1080, Train Loss:  0.029, Train Acc:  98.44%, Time: 0:01:13 \nEpoch: 14\nIter:   1110, Train Loss:  0.044, Train Acc:  98.44%, Time: 0:01:13 \nIter:   1140, Train Loss:  0.037, Train Acc: 100.00%, Time: 0:01:14 \nIter:   1170, Train Loss:  0.087, Train Acc:  96.88%, Time: 0:01:14 \nEpoch: 15\nIter:   1200, Train Loss:  0.063, Train Acc:  96.88%, Time: 0:01:15 \nIter:   1230, Train Loss:  0.032, Train Acc:  98.44%, Time: 0:01:16 \nIter:   1260, Train Loss:  0.087, Train Acc:  97.78%, Time: 0:01:16 \nEpoch: 16\nIter:   1290, Train Loss:  0.069, Train Acc:  98.44%, Time: 0:01:17 \nIter:   1320, Train Loss:   0.18, Train Acc:  95.31%, Time: 0:01:17 \nEpoch: 17\nIter:   1350, Train Loss:  0.025, Train Acc:  98.44%, Time: 0:01:18 \nIter:   1380, Train Loss:  0.078, Train Acc:  95.31%, Time: 0:01:18 \nIter:   1410, Train Loss:  0.051, Train Acc:  98.44%, Time: 0:01:19 \nEpoch: 18\nIter:   1440, Train Loss:  0.061, Train Acc:  96.88%, Time: 0:01:20 \nIter:   1470, Train Loss:  0.037, Train Acc:  98.44%, Time: 0:01:20 \nIter:   1500, Train Loss:   0.01, Train Acc: 100.00%, Time: 0:01:21 \nEpoch: 19\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/8/8\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/8/8\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.65, Test Acc:  80.84%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.88      0.64      0.74        11\n        DESC       0.79      0.69      0.74       135\n        ENTY       0.75      0.81      0.78       135\n         HUM       0.79      0.90      0.84       124\n         LOC       0.81      0.84      0.83        93\n         NUM       0.95      0.85      0.90        97\n\n   micro avg       0.81      0.81      0.81       595\n   macro avg       0.83      0.79      0.80       595\nweighted avg       0.81      0.81      0.81       595\n\nConfusion Matrix...\n[[  7   2   2   0   0   0]\n [  1  93  21   8   9   3]\n [  0   7 110  15   2   1]\n [  0   4   6 111   3   0]\n [  0   7   4   4  78   0]\n [  0   5   4   2   4  82]]\nTime usage: 0:00:02\nFold:  9\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.2, Train Acc:  62.50%, Time: 0:00:08 *\nIter:     60, Train Loss:    1.1, Train Acc:  57.81%, Time: 0:00:09 \nEpoch: 2\nIter:     90, Train Loss:    0.9, Train Acc:  65.62%, Time: 0:00:14 *\nIter:    120, Train Loss:    0.9, Train Acc:  62.50%, Time: 0:00:15 \nIter:    150, Train Loss:   0.94, Train Acc:  62.50%, Time: 0:00:15 \nEpoch: 3\nIter:    180, Train Loss:   0.56, Train Acc:  82.81%, Time: 0:00:21 *\nIter:    210, Train Loss:   0.56, Train Acc:  79.69%, Time: 0:00:21 \nIter:    240, Train Loss:   0.57, Train Acc:  82.81%, Time: 0:00:22 \nEpoch: 4\nIter:    270, Train Loss:   0.67, Train Acc:  71.88%, Time: 0:00:23 \nIter:    300, Train Loss:   0.48, Train Acc:  82.81%, Time: 0:00:23 \nIter:    330, Train Loss:   0.34, Train Acc:  92.19%, Time: 0:00:29 *\nEpoch: 5\nIter:    360, Train Loss:   0.29, Train Acc:  92.19%, Time: 0:00:30 \nIter:    390, Train Loss:   0.44, Train Acc:  81.25%, Time: 0:00:31 \nIter:    420, Train Loss:   0.32, Train Acc:  88.89%, Time: 0:00:31 \nEpoch: 6\nIter:    450, Train Loss:   0.31, Train Acc:  92.19%, Time: 0:00:32 \nIter:    480, Train Loss:   0.25, Train Acc:  93.75%, Time: 0:00:36 *\nEpoch: 7\nIter:    510, Train Loss:   0.28, Train Acc:  90.62%, Time: 0:00:37 \nIter:    540, Train Loss:   0.15, Train Acc:  93.75%, Time: 0:00:38 \nIter:    570, Train Loss:   0.24, Train Acc:  92.19%, Time: 0:00:38 \nEpoch: 8\nIter:    600, Train Loss:   0.16, Train Acc:  95.31%, Time: 0:00:43 *\nIter:    630, Train Loss:   0.17, Train Acc:  95.31%, Time: 0:00:43 \nIter:    660, Train Loss:   0.26, Train Acc:  87.50%, Time: 0:00:44 \nEpoch: 9\nIter:    690, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:00:48 *\nIter:    720, Train Loss:    0.1, Train Acc:  95.31%, Time: 0:00:49 \nIter:    750, Train Loss:   0.21, Train Acc:  93.75%, Time: 0:00:49 \nEpoch: 10\nIter:    780, Train Loss:  0.051, Train Acc: 100.00%, Time: 0:00:54 *\nIter:    810, Train Loss:   0.21, Train Acc:  92.19%, Time: 0:00:55 \nIter:    840, Train Loss:  0.094, Train Acc:  93.33%, Time: 0:00:55 \nEpoch: 11\nIter:    870, Train Loss:   0.11, Train Acc:  95.31%, Time: 0:00:56 \nIter:    900, Train Loss:   0.13, Train Acc:  95.31%, Time: 0:00:57 \nEpoch: 12\nIter:    930, Train Loss:  0.098, Train Acc:  98.44%, Time: 0:00:57 \nIter:    960, Train Loss:  0.046, Train Acc:  98.44%, Time: 0:00:58 \nIter:    990, Train Loss:   0.09, Train Acc:  96.88%, Time: 0:00:58 \nEpoch: 13\nIter:   1020, Train Loss:  0.086, Train Acc:  96.88%, Time: 0:00:59 \nIter:   1050, Train Loss:  0.067, Train Acc:  98.44%, Time: 0:01:00 \nIter:   1080, Train Loss:  0.065, Train Acc:  98.44%, Time: 0:01:00 \nEpoch: 14\nIter:   1110, Train Loss:  0.073, Train Acc:  98.44%, Time: 0:01:01 \nIter:   1140, Train Loss:  0.059, Train Acc:  98.44%, Time: 0:01:02 \nIter:   1170, Train Loss:  0.028, Train Acc: 100.00%, Time: 0:01:03 \nEpoch: 15\nIter:   1200, Train Loss:  0.099, Train Acc:  95.31%, Time: 0:01:04 \nIter:   1230, Train Loss:  0.048, Train Acc:  98.44%, Time: 0:01:05 \nIter:   1260, Train Loss:    0.1, Train Acc:  97.78%, Time: 0:01:06 \nEpoch: 16\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/9/9\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/9/9\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.74, Test Acc:  77.31%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.71      0.83        14\n        DESC       0.79      0.63      0.70       131\n        ENTY       0.78      0.74      0.76       141\n         HUM       0.75      0.88      0.81       120\n         LOC       0.75      0.79      0.77        97\n         NUM       0.79      0.86      0.82        92\n\n   micro avg       0.77      0.77      0.77       595\n   macro avg       0.81      0.77      0.78       595\nweighted avg       0.78      0.77      0.77       595\n\nConfusion Matrix...\n[[ 10   2   2   0   0   0]\n [  0  83  17   9  12  10]\n [  0   7 105  17   6   6]\n [  0   3   7 106   2   2]\n [  0   8   2   7  77   3]\n [  0   2   2   3   6  79]]\nTime usage: 0:00:02\nFold:  10\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.5, Train Acc:  50.00%, Time: 0:00:09 *\nIter:     60, Train Loss:    1.1, Train Acc:  65.62%, Time: 0:00:14 *\nEpoch: 2\nIter:     90, Train Loss:   0.88, Train Acc:  62.50%, Time: 0:00:15 \nIter:    120, Train Loss:   0.77, Train Acc:  70.31%, Time: 0:00:20 *\nIter:    150, Train Loss:   0.63, Train Acc:  78.12%, Time: 0:00:26 *\nEpoch: 3\nIter:    180, Train Loss:   0.65, Train Acc:  73.44%, Time: 0:00:26 \nIter:    210, Train Loss:   0.39, Train Acc:  87.50%, Time: 0:00:31 *\nIter:    240, Train Loss:   0.68, Train Acc:  76.56%, Time: 0:00:31 \nEpoch: 4\nIter:    270, Train Loss:   0.51, Train Acc:  81.25%, Time: 0:00:32 \nIter:    300, Train Loss:   0.52, Train Acc:  84.38%, Time: 0:00:32 \nIter:    330, Train Loss:   0.36, Train Acc:  84.38%, Time: 0:00:33 \nEpoch: 5\nIter:    360, Train Loss:   0.28, Train Acc:  93.75%, Time: 0:00:39 *\nIter:    390, Train Loss:   0.16, Train Acc:  96.88%, Time: 0:00:45 *\nIter:    420, Train Loss:   0.33, Train Acc:  86.67%, Time: 0:00:45 \nEpoch: 6\nIter:    450, Train Loss:   0.26, Train Acc:  93.75%, Time: 0:00:46 \nIter:    480, Train Loss:   0.23, Train Acc:  95.31%, Time: 0:00:46 \nEpoch: 7\nIter:    510, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:00:52 *\nIter:    540, Train Loss:   0.23, Train Acc:  93.75%, Time: 0:00:52 \nIter:    570, Train Loss:   0.14, Train Acc:  95.31%, Time: 0:00:53 \nEpoch: 8\nIter:    600, Train Loss:   0.33, Train Acc:  89.06%, Time: 0:00:54 \nIter:    630, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:00:54 \nIter:    660, Train Loss:   0.19, Train Acc:  92.19%, Time: 0:00:55 \nEpoch: 9\nIter:    690, Train Loss:   0.15, Train Acc:  96.88%, Time: 0:00:55 \nIter:    720, Train Loss:   0.17, Train Acc:  95.31%, Time: 0:00:56 \nIter:    750, Train Loss:   0.19, Train Acc:  96.88%, Time: 0:00:57 \nEpoch: 10\nIter:    780, Train Loss:   0.12, Train Acc:  95.31%, Time: 0:00:57 \nIter:    810, Train Loss:  0.082, Train Acc:  98.44%, Time: 0:00:58 \nIter:    840, Train Loss:   0.15, Train Acc:  95.56%, Time: 0:00:58 \nEpoch: 11\nIter:    870, Train Loss:  0.095, Train Acc:  96.88%, Time: 0:00:59 \nIter:    900, Train Loss:  0.076, Train Acc:  98.44%, Time: 0:01:00 \nEpoch: 12\nIter:    930, Train Loss:  0.032, Train Acc: 100.00%, Time: 0:01:04 *\nIter:    960, Train Loss:  0.069, Train Acc:  96.88%, Time: 0:01:05 \nIter:    990, Train Loss:  0.079, Train Acc:  96.88%, Time: 0:01:05 \nEpoch: 13\nIter:   1020, Train Loss:  0.019, Train Acc: 100.00%, Time: 0:01:06 \nIter:   1050, Train Loss:  0.042, Train Acc:  98.44%, Time: 0:01:07 \nIter:   1080, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:01:07 \nEpoch: 14\nIter:   1110, Train Loss:  0.012, Train Acc: 100.00%, Time: 0:01:08 \nIter:   1140, Train Loss:  0.049, Train Acc:  98.44%, Time: 0:01:08 \nIter:   1170, Train Loss:   0.16, Train Acc:  93.75%, Time: 0:01:09 \nEpoch: 15\nIter:   1200, Train Loss:  0.039, Train Acc:  98.44%, Time: 0:01:10 \nIter:   1230, Train Loss:  0.065, Train Acc:  98.44%, Time: 0:01:10 \nIter:   1260, Train Loss:  0.082, Train Acc: 100.00%, Time: 0:01:11 \nEpoch: 16\nIter:   1290, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:01:11 \nIter:   1320, Train Loss:  0.039, Train Acc:  98.44%, Time: 0:01:12 \nEpoch: 17\nIter:   1350, Train Loss:  0.081, Train Acc:  96.88%, Time: 0:01:13 \nIter:   1380, Train Loss:  0.057, Train Acc:  98.44%, Time: 0:01:13 \nIter:   1410, Train Loss:  0.029, Train Acc: 100.00%, Time: 0:01:14 \nEpoch: 18\nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/10/10\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/cnn/10/10\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.65, Test Acc:  81.18%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       1.00      0.80      0.89        10\n        DESC       0.69      0.96      0.80       157\n        ENTY       0.86      0.68      0.76       116\n         HUM       0.90      0.86      0.88        85\n         LOC       0.83      0.63      0.72        98\n         NUM       0.91      0.86      0.88       129\n\n   micro avg       0.81      0.81      0.81       595\n   macro avg       0.86      0.80      0.82       595\nweighted avg       0.83      0.81      0.81       595\n\nConfusion Matrix...\n[[  8   2   0   0   0   0]\n [  0 150   4   0   2   1]\n [  0  26  79   4   4   3]\n [  0   6   2  73   1   3]\n [  0  26   4   2  62   4]\n [  0   7   3   2   6 111]]\nTime usage: 0:00:02\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_test_acc", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "[0.738255033557047,\n 0.7684563766389885,\n 0.7680672272914598,\n 0.7865546227503224,\n 0.8050420177083055,\n 0.7512605046023841,\n 0.7596638652456909,\n 0.8084033608436585,\n 0.7731092445990618,\n 0.8117647060827047,\n 0.7567114085959108]"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(model_test_acc),np.std(model_test_acc),np.std(model_test_acc,ddof=1),np.var(model_test_acc)", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "(0.7752080334468666,\n 0.023511829348229672,\n 0.024659414657089925,\n 0.0005528061193002741)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "split_type = \"random\"\nfor train_i, test_i in kf.split(x_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    model_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    model_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Fold:  11\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.4, Train Acc:  45.31%, Time: 0:00:08 *\nIter:     60, Train Loss:    1.2, Train Acc:  54.69%, Time: 0:00:13 *\nEpoch: 2\nIter:     90, Train Loss:    1.1, Train Acc:  60.94%, Time: 0:00:19 *\nIter:    120, Train Loss:   0.77, Train Acc:  75.00%, Time: 0:00:24 *\nIter:    150, Train Loss:   0.81, Train Acc:  76.56%, Time: 0:00:30 *\nEpoch: 3\nIter:    180, Train Loss:   0.65, Train Acc:  78.12%, Time: 0:00:36 *\nIter:    210, Train Loss:   0.68, Train Acc:  71.88%, Time: 0:00:36 \nIter:    240, Train Loss:   0.58, Train Acc:  71.88%, Time: 0:00:37 \nEpoch: 4\nIter:    270, Train Loss:   0.46, Train Acc:  85.94%, Time: 0:00:43 *\nIter:    300, Train Loss:    0.5, Train Acc:  79.69%, Time: 0:00:43 \nIter:    330, Train Loss:   0.42, Train Acc:  84.38%, Time: 0:00:44 \nEpoch: 5\nIter:    360, Train Loss:   0.41, Train Acc:  85.94%, Time: 0:00:45 \nIter:    390, Train Loss:   0.27, Train Acc:  92.19%, Time: 0:00:50 *\nIter:    420, Train Loss:   0.62, Train Acc:  81.82%, Time: 0:00:51 \nEpoch: 6\nIter:    450, Train Loss:    0.3, Train Acc:  87.50%, Time: 0:00:51 \nIter:    480, Train Loss:   0.27, Train Acc:  92.19%, Time: 0:00:52 \nEpoch: 7\nIter:    510, Train Loss:   0.25, Train Acc:  92.19%, Time: 0:00:53 \nIter:    540, Train Loss:   0.25, Train Acc:  89.06%, Time: 0:00:53 \nIter:    570, Train Loss:   0.38, Train Acc:  87.50%, Time: 0:00:54 \nEpoch: 8\nIter:    600, Train Loss:   0.14, Train Acc:  98.44%, Time: 0:01:00 *\nIter:    630, Train Loss:   0.13, Train Acc:  98.44%, Time: 0:01:00 \nIter:    660, Train Loss:   0.13, Train Acc:  93.75%, Time: 0:01:01 \nEpoch: 9\nIter:    690, Train Loss:    0.1, Train Acc:  96.88%, Time: 0:01:02 \nIter:    720, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:02 \nIter:    750, Train Loss:   0.18, Train Acc:  92.19%, Time: 0:01:03 \nEpoch: 10\nIter:    780, Train Loss:    0.1, Train Acc:  95.31%, Time: 0:01:03 \nIter:    810, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:04 \nIter:    840, Train Loss:   0.15, Train Acc:  95.45%, Time: 0:01:05 \nEpoch: 11\nIter:    870, Train Loss:  0.054, Train Acc:  98.44%, Time: 0:01:05 \nIter:    900, Train Loss:  0.049, Train Acc: 100.00%, Time: 0:01:11 *\nEpoch: 12\nIter:    930, Train Loss:  0.092, Train Acc:  95.31%, Time: 0:01:12 \nIter:    960, Train Loss:   0.13, Train Acc:  93.75%, Time: 0:01:12 \nIter:    990, Train Loss:  0.058, Train Acc:  98.44%, Time: 0:01:13 \nEpoch: 13\nIter:   1020, Train Loss:  0.061, Train Acc:  96.88%, Time: 0:01:14 \nIter:   1050, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:14 \nIter:   1080, Train Loss:  0.051, Train Acc:  98.44%, Time: 0:01:15 \nEpoch: 14\nIter:   1110, Train Loss:  0.038, Train Acc:  98.44%, Time: 0:01:15 \nIter:   1140, Train Loss:  0.065, Train Acc:  98.44%, Time: 0:01:16 \nIter:   1170, Train Loss:  0.064, Train Acc:  96.88%, Time: 0:01:17 \nEpoch: 15\nIter:   1200, Train Loss:   0.06, Train Acc:  96.88%, Time: 0:01:17 \nIter:   1230, Train Loss:  0.059, Train Acc:  96.88%, Time: 0:01:18 \nIter:   1260, Train Loss:  0.019, Train Acc: 100.00%, Time: 0:01:18 \nEpoch: 16\nIter:   1290, Train Loss:   0.11, Train Acc:  96.88%, Time: 0:01:19 \nIter:   1320, Train Loss:  0.006, Train Acc: 100.00%, Time: 0:01:20 \nEpoch: 17\nIter:   1350, Train Loss:  0.048, Train Acc:  98.44%, Time: 0:01:20 \nIter:   1380, Train Loss:   0.03, Train Acc: 100.00%, Time: 0:01:21 \nNo optimization for a long time, auto-stopping...\nINFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/random/11/11\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-2/model2/trec/random/11/11\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.92, Test Acc:  75.67%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.60      0.60      0.60        10\n        DESC       0.65      0.79      0.71       126\n        ENTY       0.70      0.73      0.71       136\n         HUM       0.84      0.74      0.79       137\n         LOC       0.80      0.72      0.75        88\n         NUM       0.90      0.83      0.86        99\n\n   micro avg       0.76      0.76      0.76       596\n   macro avg       0.75      0.73      0.74       596\nweighted avg       0.77      0.76      0.76       596\n\nConfusion Matrix...\n[[  6   3   1   0   0   0]\n [  4  99  13   5   4   1]\n [  0  21  99  10   4   2]\n [  0  10  18 102   4   3]\n [  0  13   5   4  63   3]\n [  0   6   6   1   4  82]]\nTime usage: 0:00:02\nFold:  12\nTraining and evaluating...\nEpoch: 1\nIter:     30, Train Loss:    1.4, Train Acc:  31.25%, Time: 0:00:07 *\nIter:     60, Train Loss:    1.2, Train Acc:  54.69%, Time: 0:00:13 *\nEpoch: 2\nIter:     90, Train Loss:   0.97, Train Acc:  62.50%, Time: 0:00:18 *\nIter:    120, Train Loss:    1.0, Train Acc:  60.94%, Time: 0:00:19 \nIter:    150, Train Loss:    0.9, Train Acc:  59.38%, Time: 0:00:19 \nEpoch: 3\nIter:    180, Train Loss:   0.83, Train Acc:  65.62%, Time: 0:00:25 *\nIter:    210, Train Loss:   0.76, Train Acc:  73.44%, Time: 0:00:31 *\nIter:    240, Train Loss:   0.55, Train Acc:  79.69%, Time: 0:00:37 *\nEpoch: 4\nIter:    270, Train Loss:   0.36, Train Acc:  85.94%, Time: 0:00:41 *\nIter:    300, Train Loss:   0.42, Train Acc:  87.50%, Time: 0:00:46 *\nIter:    330, Train Loss:    0.6, Train Acc:  71.88%, Time: 0:00:47 \nEpoch: 5\nIter:    360, Train Loss:   0.27, Train Acc:  93.75%, Time: 0:00:52 *\nIter:    390, Train Loss:   0.32, Train Acc:  89.06%, Time: 0:00:52 \nIter:    420, Train Loss:   0.34, Train Acc:  88.64%, Time: 0:00:53 \nEpoch: 6\nIter:    450, Train Loss:   0.23, Train Acc:  95.31%, Time: 0:00:58 *\nIter:    480, Train Loss:   0.22, Train Acc:  92.19%, Time: 0:00:58 \nEpoch: 7\nIter:    510, Train Loss:   0.26, Train Acc:  90.62%, Time: 0:00:59 \nIter:    540, Train Loss:   0.24, Train Acc:  93.75%, Time: 0:01:00 \n", "name": "stdout"}, {"output_type": "error", "ename": "SystemError", "evalue": "<built-in function WritableFile_Close> returned a result with an error set", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: ", "\nThe above exception was the direct cause of the following exception:\n", "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)", "\u001b[0;32m<ipython-input-12-1cef6d0d3ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfold_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fold: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel_train_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_test_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-9-10b7ccfe3726>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, split_type, fold_id, num_epochs, dropout_keep_prob, print_per_batch, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0mbest_acc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                         \u001b[0mlast_improved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                         \u001b[0mimproved_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m           self.export_meta_graph(\n\u001b[0;32m-> 1728\u001b[0;31m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   2052\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[1;32m     72\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents, overwrite)\u001b[0m\n\u001b[1;32m    427\u001b[0m   \"\"\"\n\u001b[1;32m    428\u001b[0m   \u001b[0mtemp_pathname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[0;34m(filename, file_content)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \"\"\"\n\u001b[1;32m    308\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mret_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSet_TF_Status_from_Status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/anaconda3/envs/TensorFlow-1.8/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mClose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWritableFile_Close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mFlush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mSystemError\u001b[0m: <built-in function WritableFile_Close> returned a result with an error set"]}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(model_test_acc),np.std(model_test_acc),np.std(model_test_acc,ddof=1),np.var(model_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def expert_split():\n    train_data = [[] for i in range(20)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0]\n            index = int(info.split(\":\")[4]) - 1\n            label = int(word[0].split(\":\")[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(20):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)\n        \n    return np.asarray(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_expert = expert_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_expert[0][:2]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(train_data_expert)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(x_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    expert_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(expert_test_acc),np.std(expert_test_acc),np.std(expert_test_acc,ddof=1),np.var(expert_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "ex_train = []\ney_train = []\nfor ti in train_data_expert:\n    x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n    ex_train.append(x_train)\n    ey_train.append(y_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "ex_train = np.asarray(ex_train)\ney_train = np.asarray(ey_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(ex_train[0]),len(ex_train[1]),len(ex_train[0][0]),len(ex_train)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(ex_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    train_x, train_y = mergeData(ex_train[train_i],ey_train[train_i])\n    test_x, test_y = mergeData(ex_train[test_i],ey_train[test_i])\n    expert_train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(expert_test_acc),np.std(expert_test_acc),np.std(expert_test_acc,ddof=1),np.var(expert_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def bundle_split():\n    train_data = [[] for i in range(920)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0].split(\":\")\n            index = int(info[1]) - 1\n            label = int(info[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(920):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)   \n    \n    return train_data\n\ntrain_data_bundle = bundle_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_data_bundle[0][:5,1]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def mergeData(data_x, data_y):\n    merge_x = data_x[0]\n    merge_y = data_y[0]\n    for i in range(1,len(data_x)):\n        merge_x = np.r_[merge_x,data_x[i]]\n        merge_y = np.r_[merge_y,data_y[i]]\n        \n    return merge_x, merge_y\n\n\ndef train_split_data(train_data, split_type):\n    \n    print(split_type)\n    \n    tx = []\n    ty = []\n    for ti in train_data:\n        x_train, y_train = process_file(ti[:,0], ti[:,1], word_to_id, num_classes, seq_length)\n        tx.append(x_train)\n        ty.append(y_train)\n\n    tx = np.asarray(tx)\n    ty = np.asarray(ty)\n    \n    print(len(tx),len(tx[0]),len(tx[1]),len(tx[0][0]))\n    \n    train_acc = []\n    test_acc = []\n    fold_id = 0\n    \n    for train_i, test_i in kf.split(tx):\n        fold_id += 1\n        print(\"Fold: \", fold_id)\n        train_x, train_y = mergeData(tx[train_i],ty[train_i])\n        test_x, test_y = mergeData(tx[test_i],ty[test_i])\n        train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n        test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))\n        \n    print(test_acc)\n    print(np.mean(test_acc),np.std(test_acc),np.std(test_acc,ddof=1),np.var(test_acc))\n    \n    return test_acc", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "bundle_test_acc = train_split_data(train_data_bundle, \"bundle\")", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def table_split():\n    train_data = [[] for i in range(37)]\n    with open(trainDataPath, \"r\", encoding='utf-8') as fp:\n        for line in fp.readlines():\n            word = line.split()\n            info = word[0].split(\":\")\n            index = int(info[3]) - 1\n            label = int(info[0])\n            content = word[1:]\n            train_data[index].append([content,label])\n            \n    for i in range(37):\n        np.random.shuffle(train_data[i])\n        train_data[i] = np.asarray(train_data[i])\n        \n    np.random.shuffle(train_data)   \n    \n    return train_data\n\ntrain_data_table = table_split()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "table_test_acc = train_split_data(train_data_table, \"table\")", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "expert_train_acc = []\nexpert_test_acc = []\nsplit_type = \"expert\"\nfold_id = 0\nfor train_i, test_i in kf.split(ex_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    train_x, train_y = mergeData(ex_train[train_i],ey_train[train_i])\n    test_x, test_y = mergeData(ex_train[test_i],ey_train[test_i])\n    expert_train_acc.append(model.train(train_x, train_y,split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(test_x, test_y,split_type,fold_id,categories))\n\n\nbundle_train_acc = []\nbundle_test_acc = []\nsplit_type = \"bundle\"\nfold_id = 0\nfor train_i, test_i in kf.split(bx_train):\n    fold_id += 1\n    print(\"Fold: \", fold_id)\n    expert_train_acc.append(model.train(x_train[train_i], y_train[train_i],split_type,fold_id))\n    expert_test_acc.append(model.evaluate_model(x_train[test_i], y_train[test_i],split_type,fold_id,categories))", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "np.mean(model_test_acc),np.std(model_test_acc),np.std(model_test_acc,ddof=1),np.var(model_test_acc)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  #  7\n                    \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n                    \"List center - backs by what their transfer _ fee was .\"]  # 5\npredict(predict_sentences, word_to_id, seq_length)"}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}