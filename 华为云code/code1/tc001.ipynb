{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\n",
      "INFO:root:Using OBS-Python-SDK-3.1.2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "import moxing as mox\n",
    "mox.file.shift('os', 'mox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"s3://corpus-text-classification1/data/train_5500.label.txt\"\n",
    "testDataPath = \"s3://corpus-text-classification1/data/TREC_10.label.txt\"\n",
    "vocabPath = \"s3://corpus-text-classification1/data/vocab.txt\"\n",
    "savePath = \"s3://corpus-text-classification1/saveModel/saveModel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "num_classes = 10\n",
    "embedding_dim = 300\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "hiddenSizes = [128]\n",
    "epsilon = 5\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "print_per_batch = 50  # 每多少轮输出一次结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filePath):\n",
    "    \"\"\"读取文件内容，返回文本和标签列表\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                word = line.strip().split()\n",
    "                label = word[0].split(\":\")[0]\n",
    "                content = word[1:]\n",
    "                \n",
    "                contents.append(content)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def readCategory():\n",
    "    \"\"\"读取分类目录，固定id\"\"\"\n",
    "    categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def read_vocab(vocabPath):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    with open(vocabPath, 'r', encoding='utf-8', errors='ignore') as fp:\n",
    "        words = [_.strip() for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def buildVocab(contents_train, contents_test, vocabPath):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    # extend都可以用来合并两个列表，不同点在于extend是在原列表修改，而 + 是生成新的列表\n",
    "    contents_all = contents_train + contents_test\n",
    "    all_data = []\n",
    "    for content in contents_all:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    # 遍历得到每个单词及其出现的次数，组成字典返回\n",
    "    counter = Counter(all_data)   # Counter({'?': 2, ',': 2, 'NUM:dist': 1, 'How': 1})\n",
    "    # 统计得到出现频率最高的前    \n",
    "    count_pairs = counter.most_common()  # [('?', 2), (',', 2), ('NUM:dist', 1), ('How', 1)]\n",
    "    words, _ = zip(*count_pairs)  # zip(*) 可以看做是解压，即与zip()相反   ('?', ',', 'NUM:dist', 'How')\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = list(words)   # list(Counter(all_data).keys())，但要保证顺序，只能这样了 Counter(a).keys()会改变原来的顺序\n",
    "    open(vocabPath, 'w', encoding='utf-8', errors='ignore').write('\\n'.join(words) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_test, labels_test = readfile(testDataPath)\n",
    "contents_val, labels_val = contents_train[-452:], labels_train[-452:]\n",
    "contents_train, labels_train = contents_train[:-452], labels_train[:-452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 5000, 500)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_val),len(labels_train),len(contents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_test, labels_test = readfile(testDataPath)\n",
    "contents_train, labels_train = readfile(trainDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 6\n",
      "5452 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'},\n",
       " {'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_test = set(labels_test)\n",
    "category_train = set(labels_train)\n",
    "print(len(labels_test),len(category_test))\n",
    "print(len(labels_train),len(category_train))\n",
    "category_test,category_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======build vocab=======\n"
     ]
    }
   ],
   "source": [
    "# 如果不存在词汇表，则重建\n",
    "if not os.path.exists(vocabPath):\n",
    "    print('======build vocab=======')\n",
    "    buildVocab(contents_train, contents_test, vocabPath)\n",
    "categories, cat_to_id = readCategory()  # cat_to_id {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n",
    "words, word_to_id = read_vocab(vocabPath)\n",
    "vocab_size = len(words)\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_all = contents_train + contents_test\n",
    "seq_length = 0\n",
    "for content in contents_all:\n",
    "    if seq_length < len(content):\n",
    "        seq_length = len(content)   # seq_length = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length += 1  # seq_length = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(contents, labels, word_to_id, cat_to_id, pad_max_length):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "    # contents, labels = readfile(filePath)\n",
    "    data_id, label_id = [], []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n",
    "    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n",
    "    pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        sequences：浮点数或整数构成的两层嵌套列表\n",
    "        maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "        dtype：返回的numpy array的数据类型\n",
    "        padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "        truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "        value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    '''\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "    ''' https://blog.csdn.net/nima1994/article/details/82468965\n",
    "    to_categorical(y, num_classes=None, dtype='float32')\n",
    "        将整型标签转为onehot。y为int数组，num_classes为标签类别总数，大于max(y)（标签从0开始的）。\n",
    "        返回：如果num_classes=None，返回len(y) * [max(y)+1]（维度，m*n表示m行n列矩阵，下同），否则为len(y) * num_classes。\n",
    "    '''\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation and testing data...\n",
      "Loading data Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation and testing data...\")\n",
    "start_time = time.time()\n",
    "x_train, y_train = process_file(contents_train, labels_train, word_to_id, cat_to_id, seq_length)  # seq_length = 600\n",
    "x_val, y_val = process_file(contents_val, labels_val, word_to_id, cat_to_id, seq_length)\n",
    "x_test, y_test = process_file(contents_test, labels_test, word_to_id, cat_to_id, seq_length)\n",
    "time_dif = get_time_dif(start_time)\n",
    "print(\"Loading data Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What', 'is', 'a', 'person', \"'s\", 'socioeconomic', 'position', '?'],\n",
       " ['What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'say',\n",
       "  'to',\n",
       "  'a',\n",
       "  'friend',\n",
       "  'who',\n",
       "  'ignores',\n",
       "  'you',\n",
       "  'for',\n",
       "  'other',\n",
       "  'friends',\n",
       "  '?'],\n",
       " ['How', 'many', 'yards', 'are', 'in', '1', 'mile', '?']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_val[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    2,    3,    6,\n",
       "         231,    8, 9015, 1409,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    2,   20,   27,  175,   10,    6,  805,   78, 9016,\n",
       "          27,   14,  379, 9017,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    7,   21, 3362,\n",
       "          13,    5,  216,  522,    0]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length,num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================CNN Model Start===============================================\n",
    "# 输入内容及对应的标签\n",
    "input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "# dropout的损失率\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量映射;实际上此处的词向量并不是用的预训练好的词向量，而是未经任何训练直接生成了一个矩阵，将此矩阵作为词向量矩阵使用，效果也还不错。\n",
    "# 若使用训练好的词向量，或许训练此次文本分类的模型时会更快，更好。\n",
    "embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])  # embedding_dim 300\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding, input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 256\n",
    "kernel_size = 5\n",
    "hidden_dim = 128\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "print_per_batch = 100  # 每多少轮输出一次结果\n",
    "save_per_batch = 10  # 每多少轮存入tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN layer\n",
    "conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 这是个啥\n",
    "''' https://blog.csdn.net/khy19940520/article/details/89934335\n",
    "tf.layers.conv1d：一维卷积一般用于处理文本数据，常用语自然语言处理中，输入一般是文本经过embedding的二维数据。\n",
    "    inputs： 输入tensor， 维度(batch_size, seq_length, embedding_dim) 是一个三维的tensor；其中，\n",
    "        batch_size指每次输入的文本数量；\n",
    "        seq_length指每个文本的词语数或者单字数；\n",
    "        embedding_dim指每个词语或者每个字的向量长度；\n",
    "        例如每次训练输入2篇文本，每篇文本有100个词，每个词的向量长度为20，那input维度即为(2, 100, 20)。\n",
    "    filters：过滤器（卷积核）的数目\n",
    "    kernel_size：卷积核的大小，卷积核本身应该是二维的，这里只需要指定一维，因为第二个维度即长度与词向量的长度一致，卷积核只能从上往下走，不能从左往右走，即只能按照文本中词的顺序，也是列的顺序。\n",
    "'''\n",
    "# global max pooling layer\n",
    "gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n",
    "\n",
    "# 全连接层，后面接dropout以及relu激活\n",
    "fc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim：128\n",
    "''' https://blog.csdn.net/yangfengling1023/article/details/81774580\n",
    "dense ：全连接层  inputs：输入该网络层的数据；units：输出的维度大小，改变inputs的最后一维\n",
    "'''\n",
    "fc = tf.nn.dropout(fc, keep_prob)\n",
    "fc = tf.nn.relu(fc)\n",
    "\n",
    "# 分类器\n",
    "logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # 预测类别 tf.argmax：返回每一行或每一列的最大值 1为里面（每一行），0为外面（每一列）\n",
    "\n",
    "# 损失函数，交叉熵\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# 优化器\n",
    "optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 准确率\n",
    "correct_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# ======================================================CNN Model End============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建session\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Iter:    100, Train Loss:  0.025, Train Acc: 100.00%, Val Loss:   0.39, Val Acc:  86.06%, Time: 0:00:15 *\n",
      "Epoch: 3\n",
      "Iter:    200, Train Loss: 0.0063, Train Acc: 100.00%, Val Loss:   0.45, Val Acc:  86.50%, Time: 0:00:29 *\n",
      "Epoch: 4\n",
      "Iter:    300, Train Loss: 0.0035, Train Acc: 100.00%, Val Loss:   0.47, Val Acc:  86.28%, Time: 0:00:43 \n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Iter:    400, Train Loss: 0.00073, Train Acc: 100.00%, Val Loss:   0.47, Val Acc:  86.95%, Time: 0:00:58 *\n",
      "Epoch: 7\n",
      "Iter:    500, Train Loss: 0.00026, Train Acc: 100.00%, Val Loss:   0.51, Val Acc:  86.95%, Time: 0:01:11 \n",
      "Epoch: 8\n",
      "Iter:    600, Train Loss: 0.00033, Train Acc: 100.00%, Val Loss:   0.55, Val Acc:  85.84%, Time: 0:01:24 \n",
      "Epoch: 9\n",
      "Iter:    700, Train Loss: 3.2e-05, Train Acc: 100.00%, Val Loss:   0.55, Val Acc:  86.95%, Time: 0:01:38 \n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Iter:    800, Train Loss: 5.1e-05, Train Acc: 100.00%, Val Loss:   0.57, Val Acc:  88.27%, Time: 0:01:52 *\n",
      "Epoch: 12\n",
      "Iter:    900, Train Loss: 3.2e-05, Train Acc: 100.00%, Val Loss:   0.61, Val Acc:  87.17%, Time: 0:02:05 \n",
      "Epoch: 13\n",
      "Iter:   1000, Train Loss: 4.2e-05, Train Acc: 100.00%, Val Loss:   0.62, Val Acc:  86.73%, Time: 0:02:19 \n",
      "Epoch: 14\n",
      "Iter:   1100, Train Loss: 8.7e-06, Train Acc: 100.00%, Val Loss:   0.64, Val Acc:  87.17%, Time: 0:02:32 \n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Iter:   1200, Train Loss: 2.3e-05, Train Acc: 100.00%, Val Loss:    0.7, Val Acc:  86.06%, Time: 0:02:46 \n",
      "Epoch: 17\n",
      "Iter:   1300, Train Loss: 2.6e-05, Train Acc: 100.00%, Val Loss:   0.66, Val Acc:  87.17%, Time: 0:03:00 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "# ======================================================Train Start===============================================\n",
    "# 训练模型的代码，如果要重新训练则打开注释即可。因为后面调用了已训练好的模型，所以此处先注释掉。\n",
    "print('Training and evaluating...')\n",
    "start_time = time.time()\n",
    "total_batch = 0  # 总批次\n",
    "best_acc_val = 0.0  # 最佳验证集准确率\n",
    "last_improved = 0  # 记录上一次提升批次\n",
    "require_improvement = 500  # 如果超过1000轮未提升，提前结束训练\n",
    "flag = False\n",
    "\n",
    "for epoch in range(num_epochs):  # 20\n",
    "    print('Epoch:', epoch + 1)\n",
    "    batch_train = batch_iter(x_train, y_train, batch_size)\n",
    "    for x_batch, y_batch in batch_train:\n",
    "        feed_dict = {input_x: x_batch, input_y: y_batch, keep_prob: dropout_keep_prob}\n",
    "        session.run(optim, feed_dict=feed_dict)  # 运行优化\n",
    "        total_batch += 1\n",
    "\n",
    "        if total_batch % print_per_batch == 0:\n",
    "            # 每多少轮次输出在训练集和验证集上的性能\n",
    "            feed_dict[keep_prob] = 1.0\n",
    "            loss_train, acc_train = session.run([loss, acc], feed_dict=feed_dict)\n",
    "            loss_val, acc_val = evaluate(session, x_val, y_val, loss, acc)\n",
    "            if acc_val > best_acc_val:\n",
    "                # 保存最好结果\n",
    "                best_acc_val = acc_val\n",
    "                last_improved = total_batch\n",
    "                saver.save(sess=session, save_path=savePath)\n",
    "                improved_str = '*'\n",
    "            else:\n",
    "                improved_str = ''\n",
    "\n",
    "            time_dif = get_time_dif(start_time)\n",
    "            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                  + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "            print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "        if total_batch - last_improved > require_improvement:\n",
    "            # 验证集正确率长期不提升，提前结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            flag = True\n",
    "            break  # 跳出循环\n",
    "    if flag:  # 同上\n",
    "        break\n",
    "# ======================================================Train End==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x_pad, y_pad, batch_size):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x_pad)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    # np.arange()生成0到data_len的等差数列，默认等差为1；np.random.permutation()打乱生成的等差序列的顺序\n",
    "    # 下面三句语句是为了将训练或测试文本的顺序打乱，因为原文本中每个分类的样本全部挨在一起，这样每个batch训练的都是同一个分类，不太好，打乱后每个batch可包含不同分类\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x_pad[indices]\n",
    "    y_shuffle = y_pad[indices]\n",
    "\n",
    "    # 返回所有batch的数据\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "        \n",
    "        \n",
    "def evaluate(sess, x_pad, y_pad, loss1, acc1):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_pad)\n",
    "    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    # print(dropout_keep_prob)\n",
    "    for x_batch1, y_batch1 in batch_eval:\n",
    "        batch_len = len(x_batch1)\n",
    "        feed_dict1 = {input_x: x_batch1, input_y: y_batch1, keep_prob: 1.0}\n",
    "        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n",
    "        total_loss += lossTmp * batch_len\n",
    "        total_acc += accTmp * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    # 读取保存的模型\n",
    "    saver.restore(sess=session, save_path=savePath)\n",
    "    start_time = time.time()\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test, loss, acc)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    test_data_len = len(x_test)\n",
    "    test_num_batch = int((test_data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)  # 获得类别\n",
    "    y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果  len(x_test) 表示有多少个文本\n",
    "\n",
    "    for i in range(test_num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, test_data_len)\n",
    "        feed_dict = {\n",
    "            input_x: x_test[start_id:end_id],\n",
    "            keep_prob: 1.0\n",
    "        }\n",
    "        y_test_pred_cls[start_id:end_id] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n",
    "    '''\n",
    "    sklearn中的classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息。\n",
    "        y_true：1维数组，或标签指示器数组/稀疏矩阵，目标值。 \n",
    "        y_pred：1维数组，或标签指示器数组/稀疏矩阵，分类器返回的估计值。 \n",
    "        labels：array，shape = [n_labels]，报表中包含的标签索引的可选列表。 \n",
    "        target_names：字符串列表，与标签匹配的可选显示名称（相同顺序）。 \n",
    "        原文链接：https://blog.csdn.net/akadiao/article/details/78788864\n",
    "    '''\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n",
    "    '''\n",
    "    混淆矩阵是机器学习中总结分类模型预测结果的情形分析表，以矩阵形式将数据集中的记录按照真实的类别与分类模型作出的分类判断两个标准进行汇总。\n",
    "    这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）\n",
    "    https://blog.csdn.net/u011734144/article/details/80277225\n",
    "    '''\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/saveModel/saveModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/saveModel/saveModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Test Loss:   0.46, Test Acc:  90.80%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.78      0.78      0.78         9\n",
      "        DESC       0.86      0.99      0.92       138\n",
      "        ENTY       0.91      0.80      0.85        94\n",
      "         HUM       0.91      0.92      0.92        65\n",
      "         LOC       0.92      0.90      0.91        81\n",
      "         NUM       0.97      0.91      0.94       113\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       500\n",
      "   macro avg       0.89      0.88      0.89       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n",
      "Confusion Matrix...\n",
      "[[  7   2   0   0   0   0]\n",
      " [  1 136   1   0   0   0]\n",
      " [  1   8  75   5   3   2]\n",
      " [  0   1   3  60   0   1]\n",
      " [  0   4   3   1  73   0]\n",
      " [  0   7   0   0   3 103]]\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentences, word_to_id, cat_to_id, pad_max_length):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "    \n",
    "    data_id = []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for i in range(len(predict_sentences)):\n",
    "        data_id.append([word_to_id[x] for x in predict_sentences[i].strip().split() if x in word_to_id])\n",
    "        \n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n",
    "    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n",
    "    pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        sequences：浮点数或整数构成的两层嵌套列表\n",
    "        maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "        dtype：返回的numpy array的数据类型\n",
    "        padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "        truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "        value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    '''\n",
    "    feed_dict = {\n",
    "        input_x: x_pad,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "    predict_result = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 5])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LOC:mount Where are the Rocky Mountains ?\n",
    "DESC:def What are invertebrates ?\n",
    "NUM:temp What is the temperature at the center of the earth ?\n",
    "'''\n",
    "predict([\"Where are the Rocky Mountains ?\",\"What are invertebrates ?\",\"What is the temperature at the center of the earth ?\"],word_to_id, cat_to_id, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.8",
   "language": "python",
   "name": "tensorflow-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
