{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-text-classification1/data/train_5500.label.txt\"\ntestDataPath = \"s3://corpus-text-classification1/data/TREC_10.label.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/vocab_dp.txt\"\nsavePath = \"s3://corpus-text-classification1/saveModel_dp1/saveModel_dp1\"", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length = 20\nnum_classes = 10\nembedding_dim = 300\n\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nhiddenSizes = [128]\nepsilon = 5\n\nnum_epochs = 10\nbatch_size = 128\nprint_per_batch = 50  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import string\nprint(string.punctuation)\ntrantab = str.maketrans({key: None for key in string.punctuation})\n\"Hello ,. I's gg\".translate(trantab)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "'Hello  Is gg'"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    contents, labels = [], []\n    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n        for line in f:\n            try:\n                word = line.strip().split()\n                label = word[0].split(\":\")[0]\n                content = word[1:]\n                punc = []\n                for wi in content:\n                    if wi in string.punctuation:\n                        # content.remove(content[wi])  \u4e0d\u80fd\u76f4\u63a5\u79fb\u52a8\n                        punc.append(wi)\n                for pi in punc:\n                    content.remove(pi)  # \u79fb\u9664\u6807\u70b9\u7b26\u53f7\n                \n                contents.append(content) \n                labels.append(label)\n            except Exception as e:\n                print(repr(e))\n                pass\n    return contents, labels\n\n\ndef readCategory():\n    \"\"\"\u8bfb\u53d6\u5206\u7c7b\u76ee\u5f55\uff0c\u56fa\u5b9aid\"\"\"\n    categories = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n    cat_to_id = dict(zip(categories, range(len(categories))))\n    return categories, cat_to_id\n\n\ndef read_vocab(vocabPath):\n    \"\"\"\u8bfb\u53d6\u8bcd\u6c47\u8868\"\"\"\n    with open(vocabPath, 'r', encoding='utf-8', errors='ignore') as fp:\n        words = [_.strip() for _ in fp.readlines()]\n    word_to_id = dict(zip(words, range(len(words))))\n    return words, word_to_id\n\n\ndef buildVocab(contents_train, contents_test, vocabPath):\n    \"\"\"\u6839\u636e\u8bad\u7ec3\u96c6\u6784\u5efa\u8bcd\u6c47\u8868\uff0c\u5b58\u50a8\"\"\"\n    # extend\u90fd\u53ef\u4ee5\u7528\u6765\u5408\u5e76\u4e24\u4e2a\u5217\u8868\uff0c\u4e0d\u540c\u70b9\u5728\u4e8eextend\u662f\u5728\u539f\u5217\u8868\u4fee\u6539\uff0c\u800c + \u662f\u751f\u6210\u65b0\u7684\u5217\u8868\n    contents_all = contents_train + contents_test\n    all_data = []\n    for content in contents_all:\n        all_data.extend(content)\n\n    # \u904d\u5386\u5f97\u5230\u6bcf\u4e2a\u5355\u8bcd\u53ca\u5176\u51fa\u73b0\u7684\u6b21\u6570\uff0c\u7ec4\u6210\u5b57\u5178\u8fd4\u56de\n    counter = Counter(all_data)   # Counter({'?': 2, ',': 2, 'NUM:dist': 1, 'How': 1})\n    # \u7edf\u8ba1\u5f97\u5230\u51fa\u73b0\u9891\u7387\u6700\u9ad8\u7684\u524d    \n    count_pairs = counter.most_common()  # [('?', 2), (',', 2), ('NUM:dist', 1), ('How', 1)]\n    words, _ = zip(*count_pairs)  # zip(*) \u53ef\u4ee5\u770b\u505a\u662f\u89e3\u538b\uff0c\u5373\u4e0ezip()\u76f8\u53cd   ('?', ',', 'NUM:dist', 'How')\n    # \u6dfb\u52a0\u4e00\u4e2a <PAD> \u6765\u5c06\u6240\u6709\u6587\u672cpad\u4e3a\u540c\u4e00\u957f\u5ea6\n    words = list(words)   # list(Counter(all_data).keys())\uff0c\u4f46\u8981\u4fdd\u8bc1\u987a\u5e8f\uff0c\u53ea\u80fd\u8fd9\u6837\u4e86 Counter(a).keys()\u4f1a\u6539\u53d8\u539f\u6765\u7684\u987a\u5e8f\n    open(vocabPath, 'w', encoding='utf-8', errors='ignore').write('\\n'.join(words) + '\\n')", "execution_count": 11, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_train, labels_train = readfile(trainDataPath)\ncontents_test, labels_test = readfile(testDataPath)\ncontents_val, labels_val = contents_train[-452:], labels_train[-452:]\ncontents_train, labels_train = contents_train[:-452], labels_train[:-452]", "execution_count": 12, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(contents_val),len(labels_train),len(contents_test)", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "(452, 5000, 500)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_train[:2]", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "[['How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia'],\n ['What', 'films', 'featured', 'the', 'character', 'Popeye', 'Doyle']]"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "category_test = set(labels_test)\ncategory_train = set(labels_train)\nprint(len(labels_test),len(category_test))\nprint(len(labels_train),len(category_train))\ncategory_test,category_train", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "500 6\n5000 6\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "({'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'},\n {'ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'})"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u5982\u679c\u4e0d\u5b58\u5728\u8bcd\u6c47\u8868\uff0c\u5219\u91cd\u5efa\nif not os.path.exists(vocabPath):\n    print('======build vocab=======')\n    buildVocab(contents_train, contents_test, vocabPath)\ncategories, cat_to_id = readCategory()  # cat_to_id {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\nwords, word_to_id = read_vocab(vocabPath)\nvocab_size = len(words)\nnum_classes = len(categories)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "======build vocab=======\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "num_classes = len(categories)", "execution_count": 17, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_all = contents_train + contents_test\nseq_length = 0\nfor content in contents_all:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 37", "execution_count": 18, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "33"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "cat_to_id", "execution_count": 20, "outputs": [{"output_type": "execute_result", "execution_count": 20, "data": {"text/plain": "{'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, cat_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 21, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"Loading training and validation and testing data...\")\nstart_time = time.time()\nx_train, y_train = process_file(contents_train, labels_train, word_to_id, cat_to_id, seq_length)  # seq_length = 600\nx_val, y_val = process_file(contents_val, labels_val, word_to_id, cat_to_id, seq_length)\nx_test, y_test = process_file(contents_test, labels_test, word_to_id, cat_to_id, seq_length)\ntime_dif = get_time_dif(start_time)\nprint(\"Loading data Time usage:\", time_dif)", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "Loading training and validation and testing data...\nLoading data Time usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_val[:3]", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "[['What', 'is', 'a', 'person', \"'s\", 'socioeconomic', 'position'],\n ['What',\n  'do',\n  'you',\n  'say',\n  'to',\n  'a',\n  'friend',\n  'who',\n  'ignores',\n  'you',\n  'for',\n  'other',\n  'friends'],\n ['How', 'many', 'yards', 'are', 'in', '1', 'mile']]"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "x_val[:3]", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    1,    2,    5,  248,    7, 1737],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           1,   18,   26,  188,    9,    5,  884,   80,   26,   12,  379],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    6,   19, 6459,   11,    4,  210,  527]],\n      dtype=int32)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length,num_classes", "execution_count": 25, "outputs": [{"output_type": "execute_result", "execution_count": 25, "data": {"text/plain": "(33, 6)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "embedding_dim = 300", "execution_count": 26, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# ======================================================CNN Model Start===============================================\n# \u8f93\u5165\u5185\u5bb9\u53ca\u5bf9\u5e94\u7684\u6807\u7b7e\ninput_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\ninput_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n# dropout\u7684\u635f\u5931\u7387\nkeep_prob = tf.placeholder(tf.float32, name='keep_prob')", "execution_count": 27, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u8bcd\u5411\u91cf\u6620\u5c04;\u5b9e\u9645\u4e0a\u6b64\u5904\u7684\u8bcd\u5411\u91cf\u5e76\u4e0d\u662f\u7528\u7684\u9884\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u800c\u662f\u672a\u7ecf\u4efb\u4f55\u8bad\u7ec3\u76f4\u63a5\u751f\u6210\u4e86\u4e00\u4e2a\u77e9\u9635\uff0c\u5c06\u6b64\u77e9\u9635\u4f5c\u4e3a\u8bcd\u5411\u91cf\u77e9\u9635\u4f7f\u7528\uff0c\u6548\u679c\u4e5f\u8fd8\u4e0d\u9519\u3002\n# \u82e5\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u6216\u8bb8\u8bad\u7ec3\u6b64\u6b21\u6587\u672c\u5206\u7c7b\u7684\u6a21\u578b\u65f6\u4f1a\u66f4\u5feb\uff0c\u66f4\u597d\u3002\nembedding = tf.get_variable('embedding', [vocab_size, embedding_dim])  # embedding_dim 350\nembedding_inputs = tf.nn.embedding_lookup(embedding, input_x)", "execution_count": 28, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "num_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 20\nbatch_size = 64\nprint_per_batch = 20  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c", "execution_count": 29, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# CNN layer\nconv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 \u8fd9\u662f\u4e2a\u5565\n''' https://blog.csdn.net/khy19940520/article/details/89934335\ntf.layers.conv1d\uff1a\u4e00\u7ef4\u5377\u79ef\u4e00\u822c\u7528\u4e8e\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u5e38\u7528\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u8f93\u5165\u4e00\u822c\u662f\u6587\u672c\u7ecf\u8fc7embedding\u7684\u4e8c\u7ef4\u6570\u636e\u3002\n    inputs\uff1a \u8f93\u5165tensor\uff0c \u7ef4\u5ea6(batch_size, seq_length, embedding_dim) \u662f\u4e00\u4e2a\u4e09\u7ef4\u7684tensor\uff1b\u5176\u4e2d\uff0c\n        batch_size\u6307\u6bcf\u6b21\u8f93\u5165\u7684\u6587\u672c\u6570\u91cf\uff1b\n        seq_length\u6307\u6bcf\u4e2a\u6587\u672c\u7684\u8bcd\u8bed\u6570\u6216\u8005\u5355\u5b57\u6570\uff1b\n        embedding_dim\u6307\u6bcf\u4e2a\u8bcd\u8bed\u6216\u8005\u6bcf\u4e2a\u5b57\u7684\u5411\u91cf\u957f\u5ea6\uff1b\n        \u4f8b\u5982\u6bcf\u6b21\u8bad\u7ec3\u8f93\u51652\u7bc7\u6587\u672c\uff0c\u6bcf\u7bc7\u6587\u672c\u6709100\u4e2a\u8bcd\uff0c\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u957f\u5ea6\u4e3a20\uff0c\u90a3input\u7ef4\u5ea6\u5373\u4e3a(2, 100, 20)\u3002\n    filters\uff1a\u8fc7\u6ee4\u5668\uff08\u5377\u79ef\u6838\uff09\u7684\u6570\u76ee\n    kernel_size\uff1a\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u5377\u79ef\u6838\u672c\u8eab\u5e94\u8be5\u662f\u4e8c\u7ef4\u7684\uff0c\u8fd9\u91cc\u53ea\u9700\u8981\u6307\u5b9a\u4e00\u7ef4\uff0c\u56e0\u4e3a\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u5373\u957f\u5ea6\u4e0e\u8bcd\u5411\u91cf\u7684\u957f\u5ea6\u4e00\u81f4\uff0c\u5377\u79ef\u6838\u53ea\u80fd\u4ece\u4e0a\u5f80\u4e0b\u8d70\uff0c\u4e0d\u80fd\u4ece\u5de6\u5f80\u53f3\u8d70\uff0c\u5373\u53ea\u80fd\u6309\u7167\u6587\u672c\u4e2d\u8bcd\u7684\u987a\u5e8f\uff0c\u4e5f\u662f\u5217\u7684\u987a\u5e8f\u3002\n'''\n# global max pooling layer\ngmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n\n# \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b\nfc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim\uff1a128\n''' https://blog.csdn.net/yangfengling1023/article/details/81774580\ndense \uff1a\u5168\u8fde\u63a5\u5c42  inputs\uff1a\u8f93\u5165\u8be5\u7f51\u7edc\u5c42\u7684\u6570\u636e\uff1bunits\uff1a\u8f93\u51fa\u7684\u7ef4\u5ea6\u5927\u5c0f\uff0c\u6539\u53d8inputs\u7684\u6700\u540e\u4e00\u7ef4\n'''\nfc = tf.nn.dropout(fc, keep_prob)\nfc = tf.nn.relu(fc)\n\n# \u5206\u7c7b\u5668\nlogits = tf.layers.dense(fc, num_classes, name='fc2')\ny_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n\n# \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\nloss = tf.reduce_mean(cross_entropy)\n# \u4f18\u5316\u5668\noptim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n# \u51c6\u786e\u7387\ncorrect_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\nacc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n# ======================================================CNN Model End============================================", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-30-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-30-5cfed776675f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, x_pad, y_pad, loss1, acc1):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    # print(dropout_keep_prob)\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {input_x: x_batch1, input_y: y_batch1, keep_prob: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len", "execution_count": 31, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u521b\u5efasession\nsession = tf.Session()\nsaver = tf.train.Saver()\nsession.run(tf.global_variables_initializer())", "execution_count": 32, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# ======================================================Train Start===============================================\n# \u8bad\u7ec3\u6a21\u578b\u7684\u4ee3\u7801\uff0c\u5982\u679c\u8981\u91cd\u65b0\u8bad\u7ec3\u5219\u6253\u5f00\u6ce8\u91ca\u5373\u53ef\u3002\u56e0\u4e3a\u540e\u9762\u8c03\u7528\u4e86\u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u6240\u4ee5\u6b64\u5904\u5148\u6ce8\u91ca\u6389\u3002\nprint('Training and evaluating...')\nstart_time = time.time()\ntotal_batch = 0  # \u603b\u6279\u6b21\nbest_acc_val = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\nlast_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\nrequire_improvement = 600  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\nflag = False\n\nfor epoch in range(num_epochs):  # 20\n    print('Epoch:', epoch + 1)\n    batch_train = batch_iter(x_train, y_train, batch_size)\n    for x_batch, y_batch in batch_train:\n        feed_dict = {input_x: x_batch, input_y: y_batch, keep_prob: dropout_keep_prob}\n        session.run(optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n        total_batch += 1\n\n        if total_batch % print_per_batch == 0:\n            # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n            feed_dict[keep_prob] = 1.0\n            loss_train, acc_train = session.run([loss, acc], feed_dict=feed_dict)\n            loss_val, acc_val = evaluate(session, x_val, y_val, loss, acc)\n            if acc_val > best_acc_val:\n                # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                best_acc_val = acc_val\n                last_improved = total_batch\n                saver.save(sess=session, save_path=savePath)\n                improved_str = '*'\n            else:\n                improved_str = ''\n\n            time_dif = get_time_dif(start_time)\n            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n                  + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n            print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n\n        if total_batch - last_improved > require_improvement:\n            # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n            print(\"No optimization for a long time, auto-stopping...\")\n            flag = True\n            break  # \u8df3\u51fa\u5faa\u73af\n    if flag:  # \u540c\u4e0a\n        break\n# ======================================================Train End===============================================", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter:     20, Train Loss:    1.5, Train Acc:  40.62%, Val Loss:    1.5, Val Acc:  40.27%, Time: 0:00:04 *\nIter:     40, Train Loss:    0.9, Train Acc:  70.31%, Val Loss:    1.1, Val Acc:  57.96%, Time: 0:00:08 *\nIter:     60, Train Loss:   0.72, Train Acc:  82.81%, Val Loss:    0.8, Val Acc:  74.56%, Time: 0:00:12 *\nEpoch: 2\nIter:     80, Train Loss:   0.53, Train Acc:  84.38%, Val Loss:   0.61, Val Acc:  80.53%, Time: 0:00:16 *\nIter:    100, Train Loss:    0.2, Train Acc:  95.31%, Val Loss:   0.56, Val Acc:  82.08%, Time: 0:00:20 *\nIter:    120, Train Loss:   0.25, Train Acc:  90.62%, Val Loss:   0.52, Val Acc:  82.96%, Time: 0:00:24 *\nIter:    140, Train Loss:   0.21, Train Acc:  93.75%, Val Loss:   0.48, Val Acc:  83.41%, Time: 0:00:28 *\nEpoch: 3\nIter:    160, Train Loss:  0.017, Train Acc: 100.00%, Val Loss:   0.49, Val Acc:  84.29%, Time: 0:00:31 *\nIter:    180, Train Loss:  0.065, Train Acc:  98.44%, Val Loss:   0.45, Val Acc:  86.06%, Time: 0:00:34 *\nIter:    200, Train Loss:  0.018, Train Acc: 100.00%, Val Loss:   0.56, Val Acc:  84.73%, Time: 0:00:37 \nIter:    220, Train Loss:  0.009, Train Acc: 100.00%, Val Loss:   0.49, Val Acc:  84.96%, Time: 0:00:39 \nEpoch: 4\nIter:    240, Train Loss: 0.0063, Train Acc: 100.00%, Val Loss:   0.52, Val Acc:  85.40%, Time: 0:00:42 \nIter:    260, Train Loss: 0.0039, Train Acc: 100.00%, Val Loss:   0.61, Val Acc:  84.29%, Time: 0:00:45 \nIter:    280, Train Loss: 0.0048, Train Acc: 100.00%, Val Loss:   0.52, Val Acc:  85.84%, Time: 0:00:47 \nIter:    300, Train Loss: 0.0043, Train Acc: 100.00%, Val Loss:   0.53, Val Acc:  86.28%, Time: 0:00:51 *\nEpoch: 5\nIter:    320, Train Loss: 0.0024, Train Acc: 100.00%, Val Loss:   0.72, Val Acc:  84.51%, Time: 0:00:54 \nIter:    340, Train Loss: 0.0036, Train Acc: 100.00%, Val Loss:   0.55, Val Acc:  85.84%, Time: 0:00:56 \nIter:    360, Train Loss: 0.0067, Train Acc: 100.00%, Val Loss:   0.62, Val Acc:  85.62%, Time: 0:00:59 \nIter:    380, Train Loss:  0.015, Train Acc:  98.44%, Val Loss:   0.65, Val Acc:  85.40%, Time: 0:01:02 \nEpoch: 6\nIter:    400, Train Loss: 0.00075, Train Acc: 100.00%, Val Loss:    0.6, Val Acc:  86.50%, Time: 0:01:05 *\nIter:    420, Train Loss: 0.00044, Train Acc: 100.00%, Val Loss:    0.6, Val Acc:  86.95%, Time: 0:01:09 *\nIter:    440, Train Loss: 0.00024, Train Acc: 100.00%, Val Loss:   0.62, Val Acc:  86.73%, Time: 0:01:11 \nIter:    460, Train Loss: 0.0003, Train Acc: 100.00%, Val Loss:   0.61, Val Acc:  87.17%, Time: 0:01:15 *\nEpoch: 7\nIter:    480, Train Loss: 0.00079, Train Acc: 100.00%, Val Loss:   0.64, Val Acc:  86.50%, Time: 0:01:17 \nIter:    500, Train Loss: 0.00027, Train Acc: 100.00%, Val Loss:   0.66, Val Acc:  85.84%, Time: 0:01:20 \nIter:    520, Train Loss: 9.5e-05, Train Acc: 100.00%, Val Loss:   0.67, Val Acc:  86.06%, Time: 0:01:22 \nIter:    540, Train Loss: 0.00038, Train Acc: 100.00%, Val Loss:   0.65, Val Acc:  86.95%, Time: 0:01:25 \nEpoch: 8\nIter:    560, Train Loss: 0.0001, Train Acc: 100.00%, Val Loss:   0.64, Val Acc:  87.17%, Time: 0:01:27 \nIter:    580, Train Loss: 9.7e-05, Train Acc: 100.00%, Val Loss:   0.65, Val Acc:  87.39%, Time: 0:01:34 *\nIter:    600, Train Loss: 0.00011, Train Acc: 100.00%, Val Loss:   0.67, Val Acc:  86.73%, Time: 0:01:36 \nIter:    620, Train Loss: 0.0004, Train Acc: 100.00%, Val Loss:   0.69, Val Acc:  86.28%, Time: 0:01:39 \nEpoch: 9\nIter:    640, Train Loss: 0.00015, Train Acc: 100.00%, Val Loss:    0.7, Val Acc:  86.28%, Time: 0:01:41 \nIter:    660, Train Loss: 8.7e-05, Train Acc: 100.00%, Val Loss:   0.69, Val Acc:  87.17%, Time: 0:01:44 \nIter:    680, Train Loss: 0.00014, Train Acc: 100.00%, Val Loss:    0.7, Val Acc:  86.73%, Time: 0:01:46 \nIter:    700, Train Loss:  7e-05, Train Acc: 100.00%, Val Loss:   0.71, Val Acc:  86.28%, Time: 0:01:48 \nEpoch: 10\nIter:    720, Train Loss: 0.00019, Train Acc: 100.00%, Val Loss:   0.71, Val Acc:  86.95%, Time: 0:01:51 \nIter:    740, Train Loss: 0.00012, Train Acc: 100.00%, Val Loss:   0.72, Val Acc:  86.73%, Time: 0:01:53 \nIter:    760, Train Loss: 9.1e-05, Train Acc: 100.00%, Val Loss:   0.74, Val Acc:  86.95%, Time: 0:01:56 \nIter:    780, Train Loss: 6.7e-05, Train Acc: 100.00%, Val Loss:   0.74, Val Acc:  86.50%, Time: 0:01:58 \nEpoch: 11\nIter:    800, Train Loss: 9.1e-05, Train Acc: 100.00%, Val Loss:   0.75, Val Acc:  86.06%, Time: 0:02:01 \nIter:    820, Train Loss: 4.5e-05, Train Acc: 100.00%, Val Loss:   0.75, Val Acc:  86.06%, Time: 0:02:03 \nIter:    840, Train Loss: 4.2e-05, Train Acc: 100.00%, Val Loss:   0.76, Val Acc:  86.06%, Time: 0:02:06 \nIter:    860, Train Loss: 0.00018, Train Acc: 100.00%, Val Loss:   0.77, Val Acc:  86.06%, Time: 0:02:08 \nEpoch: 12\nIter:    880, Train Loss: 4.6e-05, Train Acc: 100.00%, Val Loss:   0.76, Val Acc:  86.28%, Time: 0:02:11 \nIter:    900, Train Loss: 2.3e-05, Train Acc: 100.00%, Val Loss:   0.78, Val Acc:  86.28%, Time: 0:02:13 \nIter:    920, Train Loss:  0.002, Train Acc: 100.00%, Val Loss:   0.86, Val Acc:  84.51%, Time: 0:02:16 \nIter:    940, Train Loss: 5.5e-05, Train Acc: 100.00%, Val Loss:   0.76, Val Acc:  86.73%, Time: 0:02:18 \nEpoch: 13\nIter:    960, Train Loss: 1.9e-05, Train Acc: 100.00%, Val Loss:    0.8, Val Acc:  85.62%, Time: 0:02:21 \nIter:    980, Train Loss: 8.4e-05, Train Acc: 100.00%, Val Loss:   0.78, Val Acc:  86.28%, Time: 0:02:23 \nIter:   1000, Train Loss: 6.7e-05, Train Acc: 100.00%, Val Loss:   0.78, Val Acc:  85.62%, Time: 0:02:26 \nIter:   1020, Train Loss: 2.6e-05, Train Acc: 100.00%, Val Loss:   0.77, Val Acc:  86.50%, Time: 0:02:28 \nEpoch: 14\nIter:   1040, Train Loss: 0.00012, Train Acc: 100.00%, Val Loss:   0.91, Val Acc:  83.85%, Time: 0:02:31 \nIter:   1060, Train Loss: 0.00013, Train Acc: 100.00%, Val Loss:   0.75, Val Acc:  85.62%, Time: 0:02:33 \nIter:   1080, Train Loss: 0.00019, Train Acc: 100.00%, Val Loss:   0.72, Val Acc:  86.06%, Time: 0:02:36 \nIter:   1100, Train Loss: 0.00011, Train Acc: 100.00%, Val Loss:   0.75, Val Acc:  86.28%, Time: 0:02:38 \nEpoch: 15\nIter:   1120, Train Loss: 9.7e-05, Train Acc: 100.00%, Val Loss:   0.77, Val Acc:  86.73%, Time: 0:02:40 \nIter:   1140, Train Loss: 3.3e-05, Train Acc: 100.00%, Val Loss:   0.82, Val Acc:  85.62%, Time: 0:02:43 \nIter:   1160, Train Loss: 0.00029, Train Acc: 100.00%, Val Loss:   0.74, Val Acc:  87.17%, Time: 0:02:45 \nIter:   1180, Train Loss: 0.0077, Train Acc: 100.00%, Val Loss:   0.73, Val Acc:  86.73%, Time: 0:02:48 \nNo optimization for a long time, auto-stopping...\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def evaluate_model():\n    # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n    saver.restore(sess=session, save_path=savePath)\n    start_time = time.time()\n    print('Testing...')\n    loss_test, acc_test = evaluate(session, x_test, y_test, loss, acc)\n    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n    print(msg.format(loss_test, acc_test))\n\n    test_data_len = len(x_test)\n    test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, test_data_len)\n        feed_dict = {\n            input_x: x_test[start_id:end_id],\n            keep_prob: 1.0\n        }\n        y_test_pred_cls[start_id:end_id] = session.run(y_pred_cls, feed_dict=feed_dict)\n\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n    '''\n    \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n    \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n    https://blog.csdn.net/u011734144/article/details/80277225\n    '''\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)", "execution_count": 34, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "evaluate_model()", "execution_count": 37, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/saveModel_dp1/saveModel_dp1\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/saveModel_dp1/saveModel_dp1\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.37, Test Acc:  89.40%\nPrecision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n        ABBR       0.78      0.78      0.78         9\n        DESC       0.84      0.99      0.91       138\n        ENTY       0.87      0.73      0.80        94\n         HUM       0.91      0.91      0.91        65\n         LOC       0.90      0.90      0.90        81\n         NUM       0.98      0.91      0.94       113\n\n   micro avg       0.89      0.89      0.89       500\n   macro avg       0.88      0.87      0.87       500\nweighted avg       0.90      0.89      0.89       500\n\nConfusion Matrix...\n[[  7   2   0   0   0   0]\n [  1 136   1   0   0   0]\n [  0  15  69   4   4   2]\n [  0   2   3  59   1   0]\n [  1   1   4   2  73   0]\n [  0   5   2   0   3 103]]\nTime usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(predict_sentences, word_to_id, cat_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    \n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(predict_sentences)):\n        data_id.append([word_to_id[x] for x in predict_sentences[i].strip().split() if x in word_to_id])\n        \n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    feed_dict = {\n        input_x: x_pad,\n        keep_prob: 1.0\n    }\n    predict_result = session.run(y_pred_cls, feed_dict=feed_dict)\n    return predict_result", "execution_count": 38, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "cat_to_id", "execution_count": 39, "outputs": [{"output_type": "execute_result", "execution_count": 39, "data": {"text/plain": "{'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "'''\nLOC:mount Where are the Rocky Mountains ?\nDESC:def What are invertebrates ?\nNUM:temp What is the temperature at the center of the earth ?\n'''\npredict([\"Where are the Rocky Mountains ?\",\"What are invertebrates ?\",\"What is the temperature at the center of the earth ?\"],word_to_id, cat_to_id, seq_length)", "execution_count": 40, "outputs": [{"output_type": "execute_result", "execution_count": 40, "data": {"text/plain": "array([4, 1, 5])"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}