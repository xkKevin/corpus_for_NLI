{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from collections import Counter\nimport numpy as np\nimport tensorflow.contrib.keras as kr\nimport tensorflow as tf\nimport time\nfrom datetime import timedelta\nimport os\nfrom sklearn import metrics\n\nimport moxing as mox\nmox.file.shift('os', 'mox')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.1-ddfd6c9a\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainDataPath = \"s3://corpus-text-classification1/data/c1_train.txt\"\ndevDataPath = \"s3://corpus-text-classification1/data/c1_dev.txt\"\ntestDataPath = \"s3://corpus-text-classification1/data/c1_test.txt\"\nvocabPath = \"s3://corpus-text-classification1/data/glove.6B.50d.txt\"\nsavePath = \"s3://corpus-text-classification1/c1_biLSTM/c1_biLSTM\"", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "raw", "source": "trainDataPath = \"c1_train.txt\"\ndevDataPath = \"c1_dev.txt\"\ntestDataPath = \"c1_test.txt\"\nvocabPath = r'D:\\lab_data\\\u4e4b\u6c5flab\\\u5b9e\u9a8c\u6a21\u578b\\glove.6B\\glove.6B.100d.txt'\nsavePath = \"c1_cnn\\c1_cnn\""}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filePath):\n    \"\"\"\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\uff0c\u8fd4\u56de\u6587\u672c\u548c\u6807\u7b7e\u5217\u8868\"\"\"\n    contents, labels = [], []\n    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n        for line in f:\n            try:\n                word = line.lower().strip().split()\n                label = int(word[0].split(\":\")[0])\n                content = word[1:]\n                \n                contents.append(content)\n                labels.append(label)\n            except:\n                pass\n    return contents, labels\n\n\ndef readCategory():\n    \"\"\"\u8bfb\u53d6\u5206\u7c7b\u76ee\u5f55\uff0c\u56fa\u5b9aid\"\"\"\n    '''\n    Retrieve Value\n    Filter\n    Compute Derived Value\n    Find Extremum\n    Sort\n    Determine Range\n    Characterize Distribution\n    Find Anomalies\n    Cluster\n    Correlate\n    '''\n    categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n    cat_to_id = dict(zip(categories, range(1,len(categories)+1)))\n    id_to_cat = dict(zip(range(1,len(categories)+1), categories))\n    return id_to_cat, cat_to_id\n\n\ndef loadGloVe(filename, emb_size=50):\n    vocab = []\n    embd = []\n    print('Loading GloVe!')\n    # vocab.append('unk') #\u88c5\u8f7d\u4e0d\u8ba4\u8bc6\u7684\u8bcd\n    # embd.append([0] * emb_size) #\u8fd9\u4e2aemb_size\u53ef\u80fd\u9700\u8981\u6307\u5b9a\n    file = open(filename,'r',encoding='utf-8')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        vocab.append(row[0])\n        embd.append([float(ei) for ei in row[1:]])\n    file.close()\n    print('Completed!')\n    return vocab,embd", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_train, labels_train = readfile(trainDataPath)\ncontents_dev, labels_dev = readfile(devDataPath)\ncontents_test, labels_test = readfile(testDataPath)", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "id_to_cat, cat_to_id = readCategory()\nnum_classes = len(id_to_cat)", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "num_classes, labels_train[0]", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "(10, 1)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_all = contents_train + contents_dev + contents_test\nseq_length = 0\nfor content in contents_all:\n    if seq_length < len(content):\n        seq_length = len(content)   # seq_length = 35", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "seq_length", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "35"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "vocab, embd = loadGloVe(vocabPath, 50)\nvocab_size = len(vocab)\nembedding_dim = len(embd[0])\nembedding = np.asarray(embd)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Loading GloVe!\nCompleted!\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id = dict(zip(vocab, range(vocab_size)))", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "len(embedding),embedding_dim,vocab_size", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "(400000, 50, 400000)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    # contents, labels = readfile(filePath)\n    data_id, label_id = [], []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(labels[i] - 1)\n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # \u5c06\u6807\u7b7e\u8f6c\u6362\u4e3aone-hot\u8868\u793a\n    ''' https://blog.csdn.net/nima1994/article/details/82468965\n    to_categorical(y, num_classes=None, dtype='float32')\n        \u5c06\u6574\u578b\u6807\u7b7e\u8f6c\u4e3aonehot\u3002y\u4e3aint\u6570\u7ec4\uff0cnum_classes\u4e3a\u6807\u7b7e\u7c7b\u522b\u603b\u6570\uff0c\u5927\u4e8emax(y)\uff08\u6807\u7b7e\u4ece0\u5f00\u59cb\u7684\uff09\u3002\n        \u8fd4\u56de\uff1a\u5982\u679cnum_classes=None\uff0c\u8fd4\u56delen(y) * [max(y)+1]\uff08\u7ef4\u5ea6\uff0cm*n\u8868\u793am\u884cn\u5217\u77e9\u9635\uff0c\u4e0b\u540c\uff09\uff0c\u5426\u5219\u4e3alen(y) * num_classes\u3002\n    '''\n    return x_pad, y_pad\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))", "execution_count": 12, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"Loading training and validation and testing data...\")\nstart_time = time.time()\nx_train, y_train = process_file(contents_train, labels_train, word_to_id, num_classes, seq_length)  # seq_length = 600\nx_dev, y_dev = process_file(contents_dev, labels_dev, word_to_id, num_classes, seq_length)\nx_test, y_test = process_file(contents_test, labels_test, word_to_id, num_classes, seq_length)\ntime_dif = get_time_dif(start_time)\nprint(\"Loading data Time usage:\", time_dif)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Loading training and validation and testing data...\nLoading data Time usage: 0:00:00\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "x_train[0], y_train[0]", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,  102,   14,    0,  485,    3, 4791,   13,  375, 2693,\n         812,  188], dtype=int32),\n array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "contents_train[0]", "execution_count": 15, "outputs": [{"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "['what',\n 'is',\n 'the',\n 'population',\n 'of',\n 'arkansas',\n 'on',\n 'july',\n '1st',\n '2002',\n '?']"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "word_to_id[\"arkansas\"]", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "4791"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "embedding[3]", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "array([ 0.70853  ,  0.57088  , -0.4716   ,  0.18048  ,  0.54449  ,\n        0.72603  ,  0.18157  , -0.52393  ,  0.10381  , -0.17566  ,\n        0.078852 , -0.36216  , -0.11829  , -0.83336  ,  0.11917  ,\n       -0.16605  ,  0.061555 , -0.012719 , -0.56623  ,  0.013616 ,\n        0.22851  , -0.14396  , -0.067549 , -0.38157  , -0.23698  ,\n       -1.7037   , -0.86692  , -0.26704  , -0.2589   ,  0.1767   ,\n        3.8676   , -0.1613   , -0.13273  , -0.68881  ,  0.18444  ,\n        0.0052464, -0.33874  , -0.078956 ,  0.24185  ,  0.36576  ,\n       -0.34727  ,  0.28483  ,  0.075693 , -0.062178 , -0.38988  ,\n        0.22902  , -0.21617  , -0.22562  , -0.093918 , -0.80375  ])"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efaadversarailLSTM\u6a21\u578b\nclass AdversarailLSTM(object):\n\n    def __init__(self, wordEmbedding):\n        # \u5b9a\u4e49\u8f93\u5165\n        self.inputX = tf.placeholder(tf.int32, [None, seq_length], name='inputX')\n        self.inputY = tf.placeholder(tf.int32, [None, num_classes], name='inputY')\n\n        self.dropoutKeepProb = tf.placeholder(tf.float64, name='keep_prob')\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        with tf.name_scope(\"wordEmbedding\"):\n            self.embeddedWords = tf.nn.embedding_lookup(wordEmbedding, self.inputX)\n\n        # \u8ba1\u7b97softmax\u4ea4\u53c9\u71b5\u635f\u5931\n        with tf.name_scope(\"loss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n                # self.y_pred_cls = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n                self.y_pred_cls = tf.argmax(tf.nn.softmax(self.predictions),1)  # \u9884\u6d4b\u7c7b\u522b tf.argmax\uff1a\u8fd4\u56de\u6bcf\u4e00\u884c\u6216\u6bcf\u4e00\u5217\u7684\u6700\u5927\u503c 1\u4e3a\u91cc\u9762\uff08\u6bcf\u4e00\u884c\uff09\uff0c0\u4e3a\u5916\u9762\uff08\u6bcf\u4e00\u5217\uff09\n                # losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n                loss = tf.reduce_mean(losses)\n\n        '''\n        with tf.name_scope(\"perturloss\"):\n            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n                print(\"perturbSize:{}\".format(perturWordEmbedding))\n                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n                # perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLosses = tf.nn.softmax_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n                perturLoss = tf.reduce_mean(perturLosses)\n\n        self.loss = loss + perturLoss\n        '''\n        self.loss = loss\n        \n        \n    def _Bi_LSTMAttention(self, embeddedWords):\n        # \u5b9a\u4e49\u4e24\u5c42\u53cc\u5411LSTM\u7684\u6a21\u578b\u7ed3\u6784\n        with tf.name_scope(\"Bi-LSTM\"):\n            fwHiddenLayers = []\n            bwHiddenLayers = []\n            for idx, hiddenSize in enumerate(hiddenSizes):\n                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n                    # \u5b9a\u4e49\u524d\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                    # \u5b9a\u4e49\u53cd\u5411\u7f51\u7edc\u7ed3\u6784\n                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(\n                        tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n                        output_keep_prob=self.dropoutKeepProb)\n\n                fwHiddenLayers.append(lstmFwCell)\n                bwHiddenLayers.append(lstmBwCell)\n\n            # \u5b9e\u73b0\u591a\u5c42\u7684LSTM\u7ed3\u6784\uff0c state_is_tuple=True\uff0c\u5219\u72b6\u6001\u4f1a\u4ee5\u5143\u7956\u7684\u5f62\u5f0f\u7ec4\u5408(h, c)\uff0c\u5426\u5219\u5217\u5411\u62fc\u63a5\n            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n            # \u91c7\u7528\u52a8\u6001rnn\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u82e5\u6ca1\u6709\u8f93\u5165\uff0c\u5219\u53d6\u5e8f\u5217\u7684\u5168\u957f\n            # outputs\u662f\u4e00\u4e2a\u5143\u7ec4(output_fw, output_bw), \u5176\u4e2d\u4e24\u4e2a\u5143\u7d20\u7684\u7ef4\u5ea6\u90fd\u662f[batch_size, max_time, hidden_size], fw\u548cbw\u7684hiddensize\u4e00\u6837\n            # self.current_state\u662f\u6700\u7ec8\u7684\u72b6\u6001\uff0c\u4e8c\u5143\u7ec4(state_fw, state_bw), state_fw=[batch_size, s], s\u662f\u4e00\u4e2a\u5143\u7ec4(h, c)\n            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm,\n                                                                          self.embeddedWords, dtype=tf.float64,\n                                                                          scope=\"bi-lstm\" + str(idx))\n\n        # \u5728bi-lstm+attention\u8bba\u6587\u4e2d\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u7684\u8f93\u51fa\u76f8\u52a0\n        with tf.name_scope(\"Attention\"):\n            H = outputs[0] + outputs[1]\n\n            # \u5f97\u5230attention\u7684\u8f93\u51fa\n            output = self.attention(H)\n            outputSize = hiddenSizes[-1]\n            print(\"outputSize:{}\".format(outputSize))\n\n        # \u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\n        with tf.name_scope(\"output\"):\n            outputW = tf.get_variable(\n                \"outputW\", dtype=tf.float64,\n                shape=[outputSize, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            outputB = tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=[num_classes]), name=\"outputB\")\n\n            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n\n            return predictions\n\n    def attention(self, H):\n        \"\"\"\n        \u5229\u7528Attention\u673a\u5236\u5f97\u5230\u53e5\u5b50\u7684\u5411\u91cf\u8868\u793a\n        \"\"\"\n        # \u83b7\u5f97\u6700\u540e\u4e00\u5c42lstm\u795e\u7ecf\u5143\u7684\u6570\u91cf\n        hiddenSize = hiddenSizes[-1]\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6743\u91cd\u5411\u91cf\uff0c\u662f\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\n        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1, dtype=tf.float64))\n\n        # \u5bf9bi-lstm\u7684\u8f93\u51fa\u7528\u6fc0\u6d3b\u51fd\u6570\u505a\u975e\u7ebf\u6027\u8f6c\u6362\n        M = tf.tanh(H)\n\n        # \u5bf9W\u548cM\u505a\u77e9\u9635\u8fd0\u7b97\uff0cW=[batch_size, time_step, hidden_size], \u8ba1\u7b97\u524d\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size * time_step, hidden_size]\n        # newM = [batch_size, time_step, 1], \u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u7531\u5411\u91cf\u8f6c\u6362\u6210\u4e00\u4e2a\u6570\u5b57\n        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n\n        # \u5bf9newM\u505a\u7ef4\u5ea6\u8f6c\u6362\u6210[batch_size, time_step]\n        restoreM = tf.reshape(newM, [-1, seq_length])\n\n        # \u7528softmax\u505a\u5f52\u4e00\u5316\u5904\u7406[batch_size, time_step]\n        self.alpha = tf.nn.softmax(restoreM)\n\n        # \u5229\u7528\u6c42\u5f97\u7684alpha\u7684\u503c\u5bf9H\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u7528\u77e9\u9635\u8fd0\u7b97\u76f4\u63a5\u64cd\u4f5c\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, seq_length, 1]))\n\n        # \u5c06\u4e09\u7ef4\u538b\u7f29\u6210\u4e8c\u7ef4sequeezeR = [batch_size, hissen_size]\n        sequeezeR = tf.squeeze(r)\n\n        sentenceRepren = tf.tanh(sequeezeR)\n\n        # \u5bf9attention\u7684\u8f93\u51fa\u53ef\u4ee5\u505adropout\u5904\u7406\n        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n\n        return output\n\n    def _normalize(self, wordEmbedding, weights):\n        \"\"\"\n        \u5bf9word embedding \u7ed3\u5408\u6743\u91cd\u505a\u6807\u51c6\u5316\u5904\u7406\n        \"\"\"\n        mean = tf.matmul(weights, wordEmbedding)\n        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n\n        var = tf.matmul(weights, powWordEmbedding)\n        stddev = tf.sqrt(1e-6 + var)\n\n        return (wordEmbedding - mean) / stddev\n\n    def _addPerturbation(self, embedded, loss):\n        \"\"\"\n        \u6dfb\u52a0\u6ce2\u52a8\u5230word embedding\n        \"\"\"\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = self._scaleL2(grad, epsilon)\n        # print(\"perturbSize:{}\".format(embedded+perturb))\n        return embedded + perturb\n\n    def _scaleL2(self, x, norm_length):\n        # shape(x) = [batch, num_step, d]\n        # divide x by max(abs(x)) for a numerically stable L2 norm\n        # 2norm(x) = a * 2norm(x/a)\n        # scale over the full sequence, dim(1, 2)\n        alpha = tf.reduce_max(tf.abs(x), (1, 2), keep_dims=True) + 1e-12\n        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keep_dims=True) + 1e-6)\n        x_unit = x / l2_norm\n        return norm_length * x_unit", "execution_count": 18, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "hiddenSizes = [128]  # \u5b9a\u4e49LSTM\u7684\u9690\u85cf\u5c42\uff08\u4e00\u5c42\uff0c128\u4e2a\u795e\u7ecf\u5143\uff09\nepsilon = 5\n\nnum_filters = 256\nkernel_size = 5\nhidden_dim = 128\nlearning_rate = 1e-3\ndropout_keep_prob = 0.5\n\nnum_epochs = 30\nbatch_size = 128\nprint_per_batch = 20  # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u4e00\u6b21\u7ed3\u679c\n\nsess = tf.Session()\nlstm = AdversarailLSTM(embedding)\nsaver = tf.train.Saver()", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "outputSize:128\nWARNING:tensorflow:From <ipython-input-18-376b46bbca3c>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stdout"}, {"output_type": "stream", "text": "WARNING:tensorflow:From <ipython-input-18-376b46bbca3c>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n# \u5b9a\u4e49\u4f18\u5316\u51fd\u6570\uff0c\u4f20\u5165\u5b66\u4e60\u901f\u7387\u53c2\u6570\noptimizer = tf.train.AdamOptimizer(learning_rate)\n# \u8ba1\u7b97\u68af\u5ea6,\u5f97\u5230\u68af\u5ea6\u548c\u53d8\u91cf\ngradsAndVars = optimizer.compute_gradients(lstm.loss)\n# \u5c06\u68af\u5ea6\u5e94\u7528\u5230\u53d8\u91cf\u4e0b\uff0c\u751f\u6210\u8bad\u7ec3\u5668\ntrainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n\n# \u51c6\u786e\u7387\ncorrect_pred = tf.equal(tf.argmax(lstm.inputY, 1), lstm.y_pred_cls)\nacc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# \u521d\u59cb\u5316\u6240\u6709\u53d8\u91cf\nsess.run(tf.global_variables_initializer())", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def batch_iter(x_pad, y_pad, batch_size):\n    \"\"\"\u751f\u6210\u6279\u6b21\u6570\u636e\"\"\"\n    data_len = len(x_pad)\n    num_batch = int((data_len - 1) / batch_size) + 1\n    # np.arange()\u751f\u62100\u5230data_len\u7684\u7b49\u5dee\u6570\u5217\uff0c\u9ed8\u8ba4\u7b49\u5dee\u4e3a1\uff1bnp.random.permutation()\u6253\u4e71\u751f\u6210\u7684\u7b49\u5dee\u5e8f\u5217\u7684\u987a\u5e8f\n    # \u4e0b\u9762\u4e09\u53e5\u8bed\u53e5\u662f\u4e3a\u4e86\u5c06\u8bad\u7ec3\u6216\u6d4b\u8bd5\u6587\u672c\u7684\u987a\u5e8f\u6253\u4e71\uff0c\u56e0\u4e3a\u539f\u6587\u672c\u4e2d\u6bcf\u4e2a\u5206\u7c7b\u7684\u6837\u672c\u5168\u90e8\u6328\u5728\u4e00\u8d77\uff0c\u8fd9\u6837\u6bcf\u4e2abatch\u8bad\u7ec3\u7684\u90fd\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\uff0c\u4e0d\u592a\u597d\uff0c\u6253\u4e71\u540e\u6bcf\u4e2abatch\u53ef\u5305\u542b\u4e0d\u540c\u5206\u7c7b\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x_pad[indices]\n    y_shuffle = y_pad[indices]\n\n    # \u8fd4\u56de\u6240\u6709batch\u7684\u6570\u636e\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n        \n        \ndef evaluate(sess, x_pad, y_pad, loss1, acc1):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_pad)\n    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch1, y_batch1 in batch_eval:\n        batch_len = len(x_batch1)\n        feed_dict1 = {lstm.inputX: x_batch1, lstm.inputY: y_batch1, lstm.dropoutKeepProb: 1.0}\n        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n        total_loss += lossTmp * batch_len\n        total_acc += accTmp * batch_len\n\n    return total_loss / data_len, total_acc / data_len", "execution_count": 21, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print('Training and evaluating...')\nstart_time = time.time()\ntotal_batch = 0  # \u603b\u6279\u6b21\nbest_acc_val = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\nlast_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\nrequire_improvement = 300  # \u5982\u679c\u8d85\u8fc7500\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\nflag = False\n\nfor epoch in range(num_epochs):\n    print('Epoch:', epoch + 1)\n    batch_train = batch_iter(x_train, y_train, batch_size)\n    for x_batch, y_batch in batch_train:\n        feed_dict = {lstm.inputX: x_batch, lstm.inputY: y_batch, lstm.dropoutKeepProb: dropout_keep_prob}\n        sess.run(trainOp, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n        total_batch += 1\n\n        if total_batch % print_per_batch == 0:\n            # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n            feed_dict[lstm.dropoutKeepProb] = 1.0\n            loss_train, acc_train = sess.run([lstm.loss, acc], feed_dict=feed_dict)\n            loss_val, acc_val = evaluate(sess, x_dev, y_dev, lstm.loss, acc)\n            if acc_val > best_acc_val:\n                # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                best_acc_val = acc_val\n                last_improved = total_batch\n                saver.save(sess=sess, save_path=savePath)\n                improved_str = '*'  # \u5bf9\u6700\u597d\u7ed3\u679c\u8fdb\u884c\u6807\u8bb0\n            else:\n                improved_str = ''\n\n            time_dif = get_time_dif(start_time)\n            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n                  + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n            print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n\n        if total_batch - last_improved > require_improvement:\n            # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n            print(\"No optimization for a long time, auto-stopping...\")\n            flag = True\n            break  # \u8df3\u51fa\u5faa\u73af\n    if flag:  # \u540c\u4e0a\n        break", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "Training and evaluating...\nEpoch: 1\nIter:     20, Train Loss:    2.2, Train Acc:  18.75%, Val Loss:    2.3, Val Acc:  16.78%, Time: 0:00:33 *\nIter:     40, Train Loss:    2.2, Train Acc:  17.19%, Val Loss:    2.1, Val Acc:  22.38%, Time: 0:01:00 *\nEpoch: 2\nIter:     60, Train Loss:    2.0, Train Acc:  27.34%, Val Loss:    2.0, Val Acc:  33.15%, Time: 0:01:26 *\nIter:     80, Train Loss:    1.8, Train Acc:  35.94%, Val Loss:    1.8, Val Acc:  38.88%, Time: 0:01:53 *\nEpoch: 3\nIter:    100, Train Loss:    1.6, Train Acc:  49.22%, Val Loss:    1.7, Val Acc:  41.26%, Time: 0:02:20 *\nIter:    120, Train Loss:    1.5, Train Acc:  51.56%, Val Loss:    1.5, Val Acc:  49.79%, Time: 0:02:47 *\nEpoch: 4\nIter:    140, Train Loss:    1.4, Train Acc:  56.25%, Val Loss:    1.5, Val Acc:  52.73%, Time: 0:03:13 *\nIter:    160, Train Loss:    1.4, Train Acc:  53.12%, Val Loss:    1.4, Val Acc:  53.01%, Time: 0:03:36 *\nIter:    180, Train Loss:    1.4, Train Acc:  50.51%, Val Loss:    1.3, Val Acc:  59.02%, Time: 0:04:02 *\nEpoch: 5\nIter:    200, Train Loss:   0.98, Train Acc:  74.22%, Val Loss:    1.3, Val Acc:  59.30%, Time: 0:04:29 *\nIter:    220, Train Loss:    1.1, Train Acc:  66.41%, Val Loss:    1.2, Val Acc:  60.00%, Time: 0:04:56 *\nEpoch: 6\nIter:    240, Train Loss:    1.1, Train Acc:  64.06%, Val Loss:    1.2, Val Acc:  62.66%, Time: 0:05:09 *\nIter:    260, Train Loss:    1.2, Train Acc:  66.41%, Val Loss:    1.1, Val Acc:  65.17%, Time: 0:05:23 *\nEpoch: 7\nIter:    280, Train Loss:   0.95, Train Acc:  70.31%, Val Loss:    1.1, Val Acc:  65.87%, Time: 0:05:36 *\nIter:    300, Train Loss:   0.74, Train Acc:  78.12%, Val Loss:    1.0, Val Acc:  68.11%, Time: 0:05:49 *\nEpoch: 8\nIter:    320, Train Loss:   0.89, Train Acc:  73.44%, Val Loss:    1.0, Val Acc:  67.55%, Time: 0:05:59 \nIter:    340, Train Loss:   0.67, Train Acc:  81.25%, Val Loss:   0.97, Val Acc:  70.21%, Time: 0:06:12 *\nIter:    360, Train Loss:   0.86, Train Acc:  69.70%, Val Loss:   0.95, Val Acc:  70.77%, Time: 0:06:26 *\nEpoch: 9\nIter:    380, Train Loss:   0.57, Train Acc:  82.03%, Val Loss:   0.92, Val Acc:  70.49%, Time: 0:06:36 \nIter:    400, Train Loss:   0.77, Train Acc:  74.22%, Val Loss:   0.94, Val Acc:  69.23%, Time: 0:06:48 \nEpoch: 10\nIter:    420, Train Loss:   0.66, Train Acc:  79.69%, Val Loss:   0.91, Val Acc:  71.33%, Time: 0:07:14 *\nIter:    440, Train Loss:   0.67, Train Acc:  82.03%, Val Loss:   0.86, Val Acc:  73.15%, Time: 0:07:43 *\nEpoch: 11\nIter:    460, Train Loss:   0.66, Train Acc:  82.03%, Val Loss:   0.85, Val Acc:  72.59%, Time: 0:08:05 \nIter:    480, Train Loss:   0.65, Train Acc:  82.81%, Val Loss:   0.83, Val Acc:  74.41%, Time: 0:08:35 *\nEpoch: 12\nIter:    500, Train Loss:   0.39, Train Acc:  87.50%, Val Loss:   0.81, Val Acc:  72.31%, Time: 0:08:57 \nIter:    520, Train Loss:    0.5, Train Acc:  86.72%, Val Loss:    0.8, Val Acc:  75.66%, Time: 0:09:25 *\nIter:    540, Train Loss:   0.64, Train Acc:  80.81%, Val Loss:   0.76, Val Acc:  76.36%, Time: 0:09:52 *\nEpoch: 13\nIter:    560, Train Loss:   0.35, Train Acc:  91.41%, Val Loss:   0.71, Val Acc:  77.62%, Time: 0:10:14 *\nIter:    580, Train Loss:   0.55, Train Acc:  85.94%, Val Loss:   0.75, Val Acc:  76.92%, Time: 0:10:37 \nEpoch: 14\nIter:    600, Train Loss:   0.48, Train Acc:  85.94%, Val Loss:   0.75, Val Acc:  79.30%, Time: 0:11:05 *\nIter:    620, Train Loss:   0.42, Train Acc:  89.84%, Val Loss:   0.71, Val Acc:  77.34%, Time: 0:11:27 \nEpoch: 15\nIter:    640, Train Loss:   0.27, Train Acc:  90.62%, Val Loss:   0.73, Val Acc:  77.34%, Time: 0:11:50 \nIter:    660, Train Loss:    0.4, Train Acc:  89.06%, Val Loss:   0.73, Val Acc:  76.92%, Time: 0:12:13 \nEpoch: 16\nIter:    680, Train Loss:   0.34, Train Acc:  88.28%, Val Loss:   0.72, Val Acc:  78.46%, Time: 0:12:35 \nIter:    700, Train Loss:   0.41, Train Acc:  89.84%, Val Loss:   0.68, Val Acc:  80.56%, Time: 0:12:56 *\nIter:    720, Train Loss:   0.25, Train Acc:  91.92%, Val Loss:   0.61, Val Acc:  81.54%, Time: 0:13:23 *\nEpoch: 17\nIter:    740, Train Loss:   0.28, Train Acc:  92.97%, Val Loss:   0.65, Val Acc:  78.88%, Time: 0:13:44 \nIter:    760, Train Loss:   0.27, Train Acc:  91.41%, Val Loss:   0.62, Val Acc:  81.26%, Time: 0:14:03 \nEpoch: 18\nIter:    780, Train Loss:   0.31, Train Acc:  88.28%, Val Loss:   0.66, Val Acc:  80.84%, Time: 0:14:24 \nIter:    800, Train Loss:    0.2, Train Acc:  93.75%, Val Loss:   0.65, Val Acc:  80.84%, Time: 0:14:48 \nEpoch: 19\nIter:    820, Train Loss:    0.2, Train Acc:  96.09%, Val Loss:   0.64, Val Acc:  81.40%, Time: 0:15:10 \nIter:    840, Train Loss:   0.24, Train Acc:  92.97%, Val Loss:   0.63, Val Acc:  81.26%, Time: 0:15:32 \nEpoch: 20\nIter:    860, Train Loss:   0.28, Train Acc:  90.62%, Val Loss:   0.64, Val Acc:  81.82%, Time: 0:15:59 *\nIter:    880, Train Loss:   0.31, Train Acc:  92.19%, Val Loss:   0.63, Val Acc:  81.68%, Time: 0:16:21 \nIter:    900, Train Loss:   0.25, Train Acc:  91.92%, Val Loss:   0.67, Val Acc:  82.52%, Time: 0:16:50 *\nEpoch: 21\nIter:    920, Train Loss:   0.24, Train Acc:  92.97%, Val Loss:   0.66, Val Acc:  81.68%, Time: 0:17:13 \nIter:    940, Train Loss:   0.26, Train Acc:  92.19%, Val Loss:   0.65, Val Acc:  80.56%, Time: 0:17:35 \nEpoch: 22\nIter:    960, Train Loss:   0.16, Train Acc:  96.88%, Val Loss:   0.61, Val Acc:  83.22%, Time: 0:18:01 *\nIter:    980, Train Loss:   0.26, Train Acc:  89.84%, Val Loss:   0.62, Val Acc:  82.80%, Time: 0:18:23 \nEpoch: 23\nIter:   1000, Train Loss:   0.23, Train Acc:  92.19%, Val Loss:   0.65, Val Acc:  81.68%, Time: 0:18:45 \nIter:   1020, Train Loss:   0.15, Train Acc:  96.09%, Val Loss:   0.62, Val Acc:  82.66%, Time: 0:18:56 \nEpoch: 24\nIter:   1040, Train Loss:   0.18, Train Acc:  96.88%, Val Loss:   0.53, Val Acc:  84.76%, Time: 0:19:09 *\nIter:   1060, Train Loss:   0.09, Train Acc:  97.66%, Val Loss:   0.58, Val Acc:  84.34%, Time: 0:19:19 \nIter:   1080, Train Loss:    0.2, Train Acc:  93.94%, Val Loss:   0.62, Val Acc:  83.36%, Time: 0:19:29 \nEpoch: 25\nIter:   1100, Train Loss:   0.21, Train Acc:  95.31%, Val Loss:   0.58, Val Acc:  83.92%, Time: 0:19:39 \nIter:   1120, Train Loss:   0.14, Train Acc:  96.09%, Val Loss:   0.63, Val Acc:  83.08%, Time: 0:19:49 \nEpoch: 26\nIter:   1140, Train Loss:   0.13, Train Acc:  98.44%, Val Loss:   0.56, Val Acc:  83.78%, Time: 0:19:59 \nIter:   1160, Train Loss:   0.21, Train Acc:  92.97%, Val Loss:    0.6, Val Acc:  83.08%, Time: 0:20:11 \nEpoch: 27\nIter:   1180, Train Loss:  0.075, Train Acc:  99.22%, Val Loss:   0.56, Val Acc:  84.06%, Time: 0:20:22 \nIter:   1200, Train Loss:   0.13, Train Acc:  97.66%, Val Loss:   0.61, Val Acc:  82.38%, Time: 0:20:32 \nEpoch: 28\nIter:   1220, Train Loss:   0.16, Train Acc:  96.09%, Val Loss:   0.63, Val Acc:  83.50%, Time: 0:20:43 \nIter:   1240, Train Loss:   0.17, Train Acc:  96.09%, Val Loss:   0.64, Val Acc:  83.22%, Time: 0:20:53 \nIter:   1260, Train Loss:   0.11, Train Acc:  96.97%, Val Loss:   0.58, Val Acc:  84.62%, Time: 0:21:03 \nEpoch: 29\nIter:   1280, Train Loss:   0.12, Train Acc:  96.88%, Val Loss:   0.64, Val Acc:  84.20%, Time: 0:21:15 \nIter:   1300, Train Loss:  0.086, Train Acc:  97.66%, Val Loss:   0.65, Val Acc:  82.52%, Time: 0:21:25 \nEpoch: 30\nIter:   1320, Train Loss:  0.066, Train Acc:  96.88%, Val Loss:   0.62, Val Acc:  83.92%, Time: 0:21:35 \nIter:   1340, Train Loss:  0.075, Train Acc:  98.44%, Val Loss:   0.57, Val Acc:  84.48%, Time: 0:21:49 \nNo optimization for a long time, auto-stopping...\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\ndef evaluate_model():\n    # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n    saver.restore(sess=sess, save_path=savePath)\n    start_time = time.time()\n    print('Testing...')\n    loss_test, acc_test = evaluate(sess, x_test, y_test, lstm.loss, acc)\n    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n    print(msg.format(loss_test, acc_test))\n\n    test_data_len = len(x_test)\n    test_num_batch = int((test_data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)  # \u83b7\u5f97\u7c7b\u522b\n    y_test_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c  len(x_test) \u8868\u793a\u6709\u591a\u5c11\u4e2a\u6587\u672c\n\n    for i in range(test_num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, test_data_len)\n        feed_dict = {\n            lstm.inputX: x_test[start_id:end_id],\n            lstm.dropoutKeepProb: 1.0\n        }\n        y_test_pred_cls[start_id:end_id] = sess.run(lstm.y_pred_cls, feed_dict=feed_dict)\n\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_test_cls, y_test_pred_cls, target_names=categories))\n    '''\n    sklearn\u4e2d\u7684classification_report\u51fd\u6570\u7528\u4e8e\u663e\u793a\u4e3b\u8981\u5206\u7c7b\u6307\u6807\u7684\u6587\u672c\u62a5\u544a\uff0e\u5728\u62a5\u544a\u4e2d\u663e\u793a\u6bcf\u4e2a\u7c7b\u7684\u7cbe\u786e\u5ea6\uff0c\u53ec\u56de\u7387\uff0cF1\u503c\u7b49\u4fe1\u606f\u3002\n        y_true\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u76ee\u6807\u503c\u3002 \n        y_pred\uff1a1\u7ef4\u6570\u7ec4\uff0c\u6216\u6807\u7b7e\u6307\u793a\u5668\u6570\u7ec4/\u7a00\u758f\u77e9\u9635\uff0c\u5206\u7c7b\u5668\u8fd4\u56de\u7684\u4f30\u8ba1\u503c\u3002 \n        labels\uff1aarray\uff0cshape = [n_labels]\uff0c\u62a5\u8868\u4e2d\u5305\u542b\u7684\u6807\u7b7e\u7d22\u5f15\u7684\u53ef\u9009\u5217\u8868\u3002 \n        target_names\uff1a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u4e0e\u6807\u7b7e\u5339\u914d\u7684\u53ef\u9009\u663e\u793a\u540d\u79f0\uff08\u76f8\u540c\u987a\u5e8f\uff09\u3002 \n        \u539f\u6587\u94fe\u63a5\uff1ahttps://blog.csdn.net/akadiao/article/details/78788864\n    '''\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_test_cls, y_test_pred_cls)\n    '''\n    \u6df7\u6dc6\u77e9\u9635\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u60c5\u5f62\u5206\u6790\u8868\uff0c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u6309\u7167\u771f\u5b9e\u7684\u7c7b\u522b\u4e0e\u5206\u7c7b\u6a21\u578b\u4f5c\u51fa\u7684\u5206\u7c7b\u5224\u65ad\u4e24\u4e2a\u6807\u51c6\u8fdb\u884c\u6c47\u603b\u3002\n    \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u5b83\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u8868\u660e\u591a\u4e2a\u7c7b\u522b\u662f\u5426\u6709\u6df7\u6dc6\uff08\u4e5f\u5c31\u662f\u4e00\u4e2aclass\u88ab\u9884\u6d4b\u6210\u53e6\u4e00\u4e2aclass\uff09\n    https://blog.csdn.net/u011734144/article/details/80277225\n    '''\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)", "execution_count": 25, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "evaluate_model()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/c1_biLSTM/c1_biLSTM\n", "name": "stdout"}, {"output_type": "stream", "text": "INFO:tensorflow:Restoring parameters from s3://corpus-text-classification1/c1_biLSTM/c1_biLSTM\n", "name": "stderr"}, {"output_type": "stream", "text": "Testing...\nTest Loss:   0.67, Test Acc:  81.31%\nPrecision, Recall and F1-Score...\n                           precision    recall  f1-score   support\n\n           Retrieve Value       0.90      0.77      0.83        71\n                   Filter       0.83      0.71      0.77        77\n    Compute Derived Value       0.69      0.78      0.73        79\n            Find Extremum       0.90      0.88      0.89        84\n                     Sort       0.80      0.83      0.81        63\n          Determine Range       0.73      0.81      0.77        64\nCharacterize Distribution       0.83      0.85      0.84        68\n           Find Anomalies       0.84      0.78      0.81        69\n                  Cluster       0.78      0.86      0.82        65\n                Correlate       0.86      0.84      0.85        77\n\n                micro avg       0.81      0.81      0.81       717\n                macro avg       0.82      0.81      0.81       717\n             weighted avg       0.82      0.81      0.81       717\n\nConfusion Matrix...\n[[55  0  9  1  0  1  1  2  0  2]\n [ 0 55  0  2  2 10  2  4  1  1]\n [ 3  1 62  1  4  4  1  0  2  1]\n [ 2  3  3 74  2  0  0  0  0  0]\n [ 0  2  1  0 52  1  0  0  6  1]\n [ 0  0  5  2  1 52  2  0  2  0]\n [ 0  1  3  0  1  1 58  0  3  1]\n [ 0  3  4  1  0  2  2 54  1  2]\n [ 1  1  0  0  3  0  0  1 56  3]\n [ 0  0  3  1  0  0  4  3  1 65]]\nTime usage: 0:00:03\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(predict_sentences, word_to_id, pad_max_length):\n    \"\"\"\n    \u5c06\u6587\u4ef6\u8f6c\u6362\u4e3aid\u8868\u793a,\u5e76\u4e14\u5c06\u6bcf\u4e2a\u5355\u72ec\u7684\u6837\u672c\u957f\u5ea6\u56fa\u5b9a\u4e3apad_max_lengtn\n    \"\"\"\n    \n    data_id = []\n    # \u5c06\u6587\u672c\u5185\u5bb9\u8f6c\u6362\u4e3a\u5bf9\u5e94\u7684id\u5f62\u5f0f\n    for i in range(len(predict_sentences)):\n        data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n        \n    # \u4f7f\u7528keras\u63d0\u4f9b\u7684pad_sequences\u6765\u5c06\u6587\u672cpad\u4e3a\u56fa\u5b9a\u957f\u5ea6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n    pad_sequences(sequences, maxlen=None, dtype=\u2019int32\u2019, padding=\u2019pre\u2019, truncating=\u2019pre\u2019, value=0.) \n        sequences\uff1a\u6d6e\u70b9\u6570\u6216\u6574\u6570\u6784\u6210\u7684\u4e24\u5c42\u5d4c\u5957\u5217\u8868\n        maxlen\uff1aNone\u6216\u6574\u6570\uff0c\u4e3a\u5e8f\u5217\u7684\u6700\u5927\u957f\u5ea6\u3002\u5927\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u88ab\u622a\u77ed\uff0c\u5c0f\u4e8e\u6b64\u957f\u5ea6\u7684\u5e8f\u5217\u5c06\u5728\u540e\u90e8\u586b0.\n        dtype\uff1a\u8fd4\u56de\u7684numpy array\u7684\u6570\u636e\u7c7b\u578b\n        padding\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u88650\u65f6\uff0c\u5728\u5e8f\u5217\u7684\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u8865\n        truncating\uff1a\u2018pre\u2019\u6216\u2018post\u2019\uff0c\u786e\u5b9a\u5f53\u9700\u8981\u622a\u65ad\u5e8f\u5217\u65f6\uff0c\u4ece\u8d77\u59cb\u8fd8\u662f\u7ed3\u5c3e\u622a\u65ad\n        value\uff1a\u6d6e\u70b9\u6570\uff0c\u6b64\u503c\u5c06\u5728\u586b\u5145\u65f6\u4ee3\u66ff\u9ed8\u8ba4\u7684\u586b\u5145\u503c0\n    '''\n    feed_dict = {\n        input_x: x_pad,\n        keep_prob: 1.0\n    }\n    predict_result = session.run(y_pred_cls, feed_dict=feed_dict)\n    predict_result = [i+1 for i in predict_result]\n    return predict_result", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  #  7\n                    \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n                    \"List center - backs by what their transfer _ fee was .\"]  # 5\npredict(predict_sentences, word_to_id, seq_length)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "tensorflow-1.8", "display_name": "TensorFlow-1.8", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}