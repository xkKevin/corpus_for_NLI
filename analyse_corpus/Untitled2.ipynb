{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe!\n",
      "Completed!\n",
      "embedding: (?, 41, 100)\n",
      "conv: (?, 39, 300)\n",
      "lstm: (?, 150)\n",
      "WARNING:tensorflow:From <ipython-input-1-a9c39a8f62da>:143: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from D:\\lab_data\\之江lab\\AIforVis\\model\\model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "vocabPath = r\"D:\\lab_data\\之江lab\\AIforVis\\model\\glove.6B.100d.txt\"\n",
    "savePath = r\"D:\\lab_data\\之江lab\\AIforVis\\model\\model\"\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    print('Loading GloVe!')\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append([float(ei) for ei in row[1:]])\n",
    "    file.close()\n",
    "    print('Completed!')\n",
    "    return vocab, embd\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "seq_length = 41\n",
    "vocab, embd = loadGloVe(vocabPath)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "# embedding = np.asarray(embd)\n",
    "embedding = embd\n",
    "word_to_id = dict(zip(vocab, range(vocab_size)))\n",
    "\n",
    "\n",
    "class ZhouCLSTMModel:\n",
    "    '''\n",
    "    Implementation proposal of: https://arxiv.org/pdf/1511.08630\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embedding,\n",
    "                 conv_size=3,\n",
    "                 conv_filters=300,\n",
    "                 drop_rate=0.5,\n",
    "                 lstm_units=150):\n",
    "        '''Constructor.\n",
    "        # Parameters:\n",
    "        conv_size: Size of the convolutions. Number of words that takes each\n",
    "            convolution step.\n",
    "        conv_filters: Number of convolution filters.\n",
    "        drop_rate: Drop rate for the final output of the LSTM layer.\n",
    "        lstm_units: Size of the states of the LSTM layer.\n",
    "        '''\n",
    "        self._embedding = embedding\n",
    "        self._conv_size = conv_size\n",
    "        self._conv_filters = conv_filters\n",
    "        self._drop_rate = drop_rate\n",
    "        self._lstm_units = lstm_units\n",
    "\n",
    "    def __call__(self, x_input):\n",
    "        self._embedding_tf = self._create_embedding_layer(\n",
    "            self._embedding, x_input)\n",
    "\n",
    "        self._convolution_tf = self._create_convolutional_layers(\n",
    "            self._conv_size,\n",
    "            self._conv_filters,\n",
    "            self._drop_rate,\n",
    "            self._embedding_tf)\n",
    "\n",
    "        self._lstm_tf = self._create_lstm_layer(\n",
    "            self._lstm_units,\n",
    "            self._convolution_tf)\n",
    "\n",
    "        return self._lstm_tf\n",
    "\n",
    "    def summary(self):\n",
    "        print('embedding:', str(self._embedding_tf.shape))\n",
    "        print('conv:', str(self._convolution_tf.shape))\n",
    "        print('lstm:', str(self._lstm_tf.shape))\n",
    "\n",
    "    def _create_embedding_layer(self, embedding, input_x):\n",
    "        embedding = tf.Variable(initial_value=embedding)\n",
    "\n",
    "        embedded_chars = tf.nn.embedding_lookup(\n",
    "            embedding, tf.cast(input_x, 'int32'))\n",
    "\n",
    "        return embedded_chars\n",
    "\n",
    "    def _create_convolutional_layers(self,\n",
    "                                     conv_size, num_filters, drop_rate, embedding):\n",
    "        filter_height = conv_size\n",
    "        filter_width = embedding.shape[2].value\n",
    "\n",
    "        filter_shape = [filter_height, filter_width, 1, num_filters]\n",
    "\n",
    "        W = tf.Variable(\n",
    "            initial_value=tf.truncated_normal(\n",
    "                shape=filter_shape,\n",
    "                stddev=0.1))\n",
    "        b = tf.Variable(\n",
    "            initial_value=tf.truncated_normal(\n",
    "                shape=[num_filters]))\n",
    "\n",
    "        embedding_expanded = tf.expand_dims(embedding, -1)\n",
    "        conv = tf.nn.conv2d(\n",
    "            input=embedding_expanded,\n",
    "            filter=W,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='VALID')\n",
    "        conv_reduced = tf.reshape(\n",
    "            tensor=conv,\n",
    "            shape=[-1, conv.shape[1], conv.shape[3]])\n",
    "\n",
    "        bias = tf.nn.bias_add(conv_reduced, b)\n",
    "        c = tf.nn.relu(bias)\n",
    "\n",
    "        d = tf.nn.dropout(c, keep_prob=drop_rate)\n",
    "        return d\n",
    "\n",
    "    def _create_lstm_layer(self, lstm_units, conv_input):\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_units)\n",
    "        sequence = tf.unstack(conv_input, axis=1)\n",
    "        (_, (h, _)) = tf.nn.static_rnn(lstm_cell, sequence, dtype=tf.float32)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "# 输入内容及对应的标签\n",
    "input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "model = ZhouCLSTMModel(embedding, drop_rate = keep_prob)\n",
    "fc = model(input_x)\n",
    "model.summary()\n",
    "\n",
    "# 分类器\n",
    "logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # 预测类别 tf.argmax：返回每一行或每一列的最大值 1为里面（每一行），0为外面（每一列）\n",
    "\n",
    "# 损失函数，交叉熵\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# 优化器\n",
    "optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 准确率\n",
    "correct_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "session = tf.Session()\n",
    "saver.restore(sess=session, save_path=savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    new_sent = ''\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] in string.punctuation:\n",
    "            if i > 0 and i < len(sent) - 1:\n",
    "                if sent[i] in \",.\" and sent[i-1].isdigit() and sent[i+1].isdigit():\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i] == \"%\" and sent[i-1].isdigit():\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i] == \"$\" and (sent[i-1].isdigit() or sent[i+1].isdigit()):\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i-1] != ' ':\n",
    "                    new_sent += ' ' + sent[i]\n",
    "                elif sent[i+1] != ' ':\n",
    "                    new_sent += sent[i] + ' '\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "            elif i == 0:\n",
    "                if sent[i] == \"$\" and sent[i+1].isdigit():\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i+1] != ' ':\n",
    "                    new_sent += sent[i] + ' '\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "            else:\n",
    "                if sent[i] == \"%\" and sent[i-1].isdigit():\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i] == \"$\" and sent[i-1].isdigit():\n",
    "                    new_sent += sent[i]\n",
    "                    continue\n",
    "                if sent[i-1] != ' ':\n",
    "                    new_sent += ' ' + sent[i]\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "        else:\n",
    "            new_sent += sent[i]\n",
    "    return new_sent.strip().lower()\n",
    "\n",
    "\n",
    "def predict11(predict_sentences):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "    data_id = []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for psi in predict_sentences:\n",
    "\n",
    "        data_id.append([word_to_id[x] for x in preprocess_sentence(psi).split() if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, seq_length)\n",
    "    feed_dict = {\n",
    "        input_x: x_pad,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "    predict_result = session.run(tf.nn.softmax(logits), feed_dict=feed_dict)\n",
    "    # print(predict_result)\n",
    "    result = []\n",
    "    for i in predict_result:\n",
    "        result.append([max(i), i.argmax()+1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7058551e-02 4.5138117e-02 9.2223249e-02 6.8682201e-02 1.6280174e-02\n",
      "  9.2392340e-02 6.6164529e-01 1.5889378e-03 4.8943777e-03 9.6813492e-05]]\n"
     ]
    }
   ],
   "source": [
    "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  # 7\n",
    "                     \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n",
    "                     \"List center - backs by what their transfer _ fee was .\",  # 5\n",
    "                     \"can you tell me what is arkansas 's population on the date july 1st of 2002 ?\",  # 1\n",
    "                     \"show the way the number of likes were distributed .\",  # 7\n",
    "                     \"is it true that people living on average depends on higher gdp of a country\"  # 10\n",
    "                     ]\n",
    "\n",
    "result = predict11([\"In the sixtieth ceremony , where were all of the winners from ?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'int64' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1e4dc18c08f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[1;32m--> 180\u001b[1;33m                         o.__class__.__name__)\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type 'int64' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "int(json.dumps(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = int(json.dumps(result[0],cls=NpEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split():\n",
    "    tasks_sentences = []\n",
    "    labels = []\n",
    "    with open(\"corpus最终版\\corpus_5.txt\", \"r\", encoding='utf-8') as fp:\n",
    "        for line in fp.readlines():\n",
    "            lsp = line.split()\n",
    "            sentence = \" \".join(lsp[1:])\n",
    "            tasks_sentences.append(sentence)\n",
    "            label = int(lsp[0].split(\":\")[0])\n",
    "            labels.append(label)\n",
    "            \n",
    "    return tasks_sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_sentences,labels = random_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict11(tasks_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24597807228565216"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(result[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:,0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6002466, 4]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[812]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[813][1],labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9569647310295689"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_num = len(labels)\n",
    "right_array = []\n",
    "for i in range(all_num):\n",
    "    if result[i][1] == labels[i]:\n",
    "        right_array.append(result[i][0])\n",
    "\n",
    "len(right_array)/all_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_result = np.asarray(right_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99642915, 0.9995763 , 0.9992624 ], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26189747, 0.26189747)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(right_result), min(right_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2616"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_result.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26189747"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_result[2616]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99642915, 0.99957627, 0.99926239, ..., 0.99830377, 0.57879895,\n",
       "       0.99996567])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.79089814, 7]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11([\"distribution\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf18",
   "language": "python",
   "name": "ts_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
