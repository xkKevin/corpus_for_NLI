{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainDataPath = \"s3://corpus-text-classification1/data/c1_train.txt\"\n",
    "devDataPath = \"s3://corpus-text-classification1/data/c1_dev.txt\"\n",
    "testDataPath = \"s3://corpus-text-classification1/data/c1_test.txt\"\n",
    "vocabPath = \"s3://corpus-text-classification1/data/glove.6B.50d.txt\"\n",
    "savePath = \"s3://corpus-text-classification1/c1_cnn/c1_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"c1_train.txt\"\n",
    "devDataPath = \"c1_dev.txt\"\n",
    "testDataPath = \"c1_test.txt\"\n",
    "vocabPath = r'D:\\lab_data\\之江lab\\实验模型\\glove.6B\\glove.6B.50d.txt'\n",
    "savePath = \"c1_cnn\\c1_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n",
    "              'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n",
    "cat_to_id = dict(zip(categories, range(1,len(categories)+1)))\n",
    "id_to_cat = dict(zip(range(1,len(categories)+1), categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Retrieve Value',\n",
       " 2: 'Filter',\n",
       " 3: 'Compute Derived Value',\n",
       " 4: 'Find Extremum',\n",
       " 5: 'Sort',\n",
       " 6: 'Determine Range',\n",
       " 7: 'Characterize Distribution',\n",
       " 8: 'Find Anomalies',\n",
       " 9: 'Cluster',\n",
       " 10: 'Correlate'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retrieve Value': 1,\n",
       " 'Filter': 2,\n",
       " 'Compute Derived Value': 3,\n",
       " 'Find Extremum': 4,\n",
       " 'Sort': 5,\n",
       " 'Determine Range': 6,\n",
       " 'Characterize Distribution': 7,\n",
       " 'Find Anomalies': 8,\n",
       " 'Cluster': 9,\n",
       " 'Correlate': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filePath):\n",
    "    \"\"\"读取文件内容，返回文本和标签列表\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open(filePath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                word = line.lower().strip().split()\n",
    "                label = word[0].split(\":\")[0]\n",
    "                content = word[1:]\n",
    "                \n",
    "                contents.append(content)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def readCategory():\n",
    "    \"\"\"读取分类目录，固定id\"\"\"\n",
    "    '''\n",
    "    Retrieve Value\n",
    "    Filter\n",
    "    Compute Derived Value\n",
    "    Find Extremum\n",
    "    Sort\n",
    "    Determine Range\n",
    "    Characterize Distribution\n",
    "    Find Anomalies\n",
    "    Cluster\n",
    "    Correlate\n",
    "    '''\n",
    "    categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort', \n",
    "                  'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n",
    "    cat_to_id = dict(zip(categories, range(1,len(categories)+1)))\n",
    "    id_to_cat = dict(zip(range(1,len(categories)+1), categories))\n",
    "    return id_to_cat, cat_to_id\n",
    "\n",
    "\n",
    "def loadGloVe(filename, emb_size=50):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    print('Loading GloVe!')\n",
    "    vocab.append('unk') #装载不认识的词\n",
    "    embd.append([0] * emb_size) #这个emb_size可能需要指定\n",
    "    file = open(filename,'r',encoding='utf-8')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    file.close()\n",
    "    print('Completed!')\n",
    "    return vocab,embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_train, labels_train = readfile(trainDataPath)\n",
    "contents_dev, labels_dev = readfile(devDataPath)\n",
    "contents_test, labels_test = readfile(testDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_cat, cat_to_id = readCategory()\n",
    "num_classes = len(id_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, '1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes, labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_all = contents_train + contents_dev + contents_test\n",
    "seq_length = 0\n",
    "for content in contents_all:\n",
    "    if seq_length < len(content):\n",
    "        seq_length = len(content)   # seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe!\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "vocab, embd = loadGloVe(vocabPath)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.013441', '0.23682', '-0.16899', '0.40951', '0.63812', '0.47709',\n",
       "       '-0.42852', '-0.55641', '-0.364', '-0.23938', '0.13001',\n",
       "       '-0.063734', '-0.39575', '-0.48162', '0.23291', '0.090201',\n",
       "       '-0.13324', '0.078639', '-0.41634', '-0.15428', '0.10068',\n",
       "       '0.48891', '0.31226', '-0.1252', '-0.037512', '-1.5179', '0.12612',\n",
       "       '-0.02442', '-0.042961', '-0.28351', '3.5416', '-0.11956',\n",
       "       '-0.014533', '-0.1499', '0.21864', '-0.33412', '-0.13872',\n",
       "       '0.31806', '0.70358', '0.44858', '-0.080262', '0.63003', '0.32111',\n",
       "       '-0.46765', '0.22786', '0.36034', '-0.37818', '-0.56657',\n",
       "       '0.044691', '0.30392'], dtype='<U11')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.418',\n",
       " '0.24968',\n",
       " '-0.41242',\n",
       " '0.1217',\n",
       " '0.34527',\n",
       " '-0.044457',\n",
       " '-0.49688',\n",
       " '-0.17862',\n",
       " '-0.00066023',\n",
       " '-0.6566',\n",
       " '0.27843',\n",
       " '-0.14767',\n",
       " '-0.55677',\n",
       " '0.14658',\n",
       " '-0.0095095',\n",
       " '0.011658',\n",
       " '0.10204',\n",
       " '-0.12792',\n",
       " '-0.8443',\n",
       " '-0.12181',\n",
       " '-0.016801',\n",
       " '-0.33279',\n",
       " '-0.1552',\n",
       " '-0.23131',\n",
       " '-0.19181',\n",
       " '-1.8823',\n",
       " '-0.76746',\n",
       " '0.099051',\n",
       " '-0.42125',\n",
       " '-0.19526',\n",
       " '4.0071',\n",
       " '-0.18594',\n",
       " '-0.52287',\n",
       " '-0.31681',\n",
       " '0.00059213',\n",
       " '0.0074449',\n",
       " '0.17778',\n",
       " '-0.15897',\n",
       " '0.012041',\n",
       " '-0.054223',\n",
       " '-0.29871',\n",
       " '-0.15749',\n",
       " '-0.34758',\n",
       " '-0.045637',\n",
       " '-0.44251',\n",
       " '0.18785',\n",
       " '0.0027849',\n",
       " '-0.18411',\n",
       " '-0.11514',\n",
       " '-0.78581']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eee = embd[1:5]\n",
    "eee[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = dict(zip(vocab, range(vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 50, 400001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding),embedding_dim,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(contents, labels, word_to_id, num_classes, pad_max_length):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "    # contents, labels = readfile(filePath)\n",
    "    data_id, label_id = [], []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(int(labels[i])-1)\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n",
    "    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n",
    "    pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        sequences：浮点数或整数构成的两层嵌套列表\n",
    "        maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "        dtype：返回的numpy array的数据类型\n",
    "        padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "        truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "        value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    '''\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=num_classes)  # 将标签转换为one-hot表示\n",
    "    ''' https://blog.csdn.net/nima1994/article/details/82468965\n",
    "    to_categorical(y, num_classes=None, dtype='float32')\n",
    "        将整型标签转为onehot。y为int数组，num_classes为标签类别总数，大于max(y)（标签从0开始的）。\n",
    "        返回：如果num_classes=None，返回len(y) * [max(y)+1]（维度，m*n表示m行n列矩阵，下同），否则为len(y) * num_classes。\n",
    "    '''\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation and testing data...\n",
      "Loading data Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training and validation and testing data...\")\n",
    "start_time = time.time()\n",
    "x_train, y_train = process_file(contents_train, labels_train, word_to_id, num_classes, seq_length)  # seq_length = 600\n",
    "x_dev, y_dev = process_file(contents_dev, labels_dev, word_to_id, num_classes, seq_length)\n",
    "x_test, y_test = process_file(contents_test, labels_test, word_to_id, num_classes, seq_length)\n",
    "time_dif = get_time_dif(start_time)\n",
    "print(\"Loading data Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  103,   15,    1,  486,    4, 4792,   14,  376, 2694,\n",
       "         813,  189]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'population',\n",
       " 'of',\n",
       " 'arkansas',\n",
       " 'on',\n",
       " 'july',\n",
       " '1st',\n",
       " '2002',\n",
       " '?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4792"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id[\"arkansas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_2/Reshape_1:0\", shape=(1, 1), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eee=[[  7, -10,  -7],\n",
    "       [  7,  -1,  -2],\n",
    "       [  3,  -9,  -2],\n",
    "       [ -6,  -6,   5],\n",
    "       [ -6,   0,  -6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, -10, -7], [7, -1, -2], [3, -9, -2], [-6, -6, 5], [-6, 0, -6]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7, -10,  -7],\n",
       "       [  7,  -1,  -2],\n",
       "       [  3,  -9,  -2],\n",
       "       [ -6,  -6,   5],\n",
       "       [ -6,   0,  -6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeenp = np.array(eee)\n",
    "eeenp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7, -10,  -7],\n",
       "       [  7,  -1,  -2],\n",
       "       [  3,  -9,  -2],\n",
       "       [ -6,  -6,   5],\n",
       "       [ -6,   0,  -6]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeenp2 = np.asarray(eee)\n",
    "eeenp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_inputs = tf.nn.embedding_lookup(eee, [[1]])\n",
    "embedding_inputs_eeenp = tf.nn.embedding_lookup(eeenp, [[1]])\n",
    "embedding_inputs_eeenp2 = tf.nn.embedding_lookup(eeenp2, [[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7]]\n",
      "[[[ 7 -1 -2]]]\n",
      "[[[ 7 -1 -2]]]\n"
     ]
    }
   ],
   "source": [
    "print(session.run(embedding_inputs))\n",
    "print(session.run(embedding_inputs_eeenp))\n",
    "print(session.run(embedding_inputs_eeenp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12492\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py:115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if params is None or params in ((), []):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8b53d13c3e8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 若使用训练好的词向量，或许训练此次文本分类的模型时会更快，更好。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0membedding_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[0;32m    306\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m       transform_fn=None)\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[1;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[0;32m    125\u001b[0m     if not any(\n\u001b[0;32m    126\u001b[0m         isinstance(p, resource_variable_ops.ResourceVariable) for p in params):\n\u001b[1;32m--> 127\u001b[1;33m       \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor_or_indexed_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ids\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_n_to_tensor_or_indexed_slices\u001b[1;34m(values, dtype, name)\u001b[0m\n\u001b[0;32m   1330\u001b[0m   \"\"\"\n\u001b[0;32m   1331\u001b[0m   return internal_convert_n_to_tensor_or_indexed_slices(\n\u001b[1;32m-> 1332\u001b[1;33m       values=values, dtype=dtype, name=name, as_ref=False)\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_n_to_tensor_or_indexed_slices\u001b[1;34m(values, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1301\u001b[0m       ret.append(\n\u001b[0;32m   1302\u001b[0m           internal_convert_to_tensor_or_indexed_slices(\n\u001b[1;32m-> 1303\u001b[1;33m               value, dtype=dtype, name=n, as_ref=as_ref))\n\u001b[0m\u001b[0;32m   1304\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor_or_indexed_slices\u001b[1;34m(value, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1260\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     return internal_convert_to_tensor(\n\u001b[1;32m-> 1262\u001b[1;33m         value, dtype=dtype, name=name, as_ref=as_ref)\n\u001b[0m\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                          as_ref=False):\n\u001b[0;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 214\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    530\u001b[0m     raise TypeError(\n\u001b[0;32m    531\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[1;32m--> 532\u001b[1;33m   \u001b[0mappend_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mSlowAppendObjectArrayToTensorProto\u001b[1;34m(tensor_proto, proto_values)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSlowAppendObjectArrayToTensorProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSlowAppendBoolArrayToTensorProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSlowAppendObjectArrayToTensorProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSlowAppendBoolArrayToTensorProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda2\\envs\\ts_py36\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \"\"\"\n\u001b[1;32m---> 61\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================CNN Model Start===============================================\n",
    "# 输入内容及对应的标签\n",
    "input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "# dropout的损失率\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# 词向量映射;实际上此处的词向量并不是用的预训练好的词向量，而是未经任何训练直接生成了一个矩阵，将此矩阵作为词向量矩阵使用，效果也还不错。\n",
    "# 若使用训练好的词向量，或许训练此次文本分类的模型时会更快，更好。\n",
    "# embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding, input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 256\n",
    "kernel_size = 5\n",
    "hidden_dim = 128\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "print_per_batch = 100  # 每多少轮输出一次结果\n",
    "save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "\n",
    "# CNN layer\n",
    "conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 这是个啥\n",
    "''' https://blog.csdn.net/khy19940520/article/details/89934335\n",
    "tf.layers.conv1d：一维卷积一般用于处理文本数据，常用语自然语言处理中，输入一般是文本经过embedding的二维数据。\n",
    "    inputs： 输入tensor， 维度(batch_size, seq_length, embedding_dim) 是一个三维的tensor；其中，\n",
    "        batch_size指每次输入的文本数量；\n",
    "        seq_length指每个文本的词语数或者单字数；\n",
    "        embedding_dim指每个词语或者每个字的向量长度；\n",
    "        例如每次训练输入2篇文本，每篇文本有100个词，每个词的向量长度为20，那input维度即为(2, 100, 20)。\n",
    "    filters：过滤器（卷积核）的数目\n",
    "    kernel_size：卷积核的大小，卷积核本身应该是二维的，这里只需要指定一维，因为第二个维度即长度与词向量的长度一致，卷积核只能从上往下走，不能从左往右走，即只能按照文本中词的顺序，也是列的顺序。\n",
    "'''\n",
    "# global max pooling layer\n",
    "gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n",
    "\n",
    "# 全连接层，后面接dropout以及relu激活\n",
    "fc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim：128\n",
    "''' https://blog.csdn.net/yangfengling1023/article/details/81774580\n",
    "dense ：全连接层  inputs：输入该网络层的数据；units：输出的维度大小，改变inputs的最后一维\n",
    "'''\n",
    "fc = tf.nn.dropout(fc, keep_prob)\n",
    "fc = tf.nn.relu(fc)\n",
    "\n",
    "# 分类器\n",
    "logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # 预测类别 tf.argmax：返回每一行或每一列的最大值 1为里面（每一行），0为外面（每一列）\n",
    "\n",
    "# 损失函数，交叉熵\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# 优化器\n",
    "optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 准确率\n",
    "correct_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# ======================================================CNN Model End============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建session\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================Train Start===============================================\n",
    "# 训练模型的代码，如果要重新训练则打开注释即可。因为后面调用了已训练好的模型，所以此处先注释掉。\n",
    "print('Training and evaluating...')\n",
    "start_time = time.time()\n",
    "total_batch = 0  # 总批次\n",
    "best_acc_val = 0.0  # 最佳验证集准确率\n",
    "last_improved = 0  # 记录上一次提升批次\n",
    "require_improvement = 500  # 如果超过1000轮未提升，提前结束训练\n",
    "flag = False\n",
    "\n",
    "for epoch in range(num_epochs):  # 20\n",
    "    print('Epoch:', epoch + 1)\n",
    "    batch_train = batch_iter(x_train, y_train, batch_size)\n",
    "    for x_batch, y_batch in batch_train:\n",
    "        feed_dict = {input_x: x_batch, input_y: y_batch, keep_prob: dropout_keep_prob}\n",
    "        session.run(optim, feed_dict=feed_dict)  # 运行优化\n",
    "        total_batch += 1\n",
    "\n",
    "        if total_batch % print_per_batch == 0:\n",
    "            # 每多少轮次输出在训练集和验证集上的性能\n",
    "            feed_dict[keep_prob] = 1.0\n",
    "            loss_train, acc_train = session.run([loss, acc], feed_dict=feed_dict)\n",
    "            loss_val, acc_val = evaluate(session, x_val, y_val, loss, acc)\n",
    "            if acc_val > best_acc_val:\n",
    "                # 保存最好结果\n",
    "                best_acc_val = acc_val\n",
    "                last_improved = total_batch\n",
    "                saver.save(sess=session, save_path=savePath)\n",
    "                improved_str = '*'\n",
    "            else:\n",
    "                improved_str = ''\n",
    "\n",
    "            time_dif = get_time_dif(start_time)\n",
    "            msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                  + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "            print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "        if total_batch - last_improved > require_improvement:\n",
    "            # 验证集正确率长期不提升，提前结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            flag = True\n",
    "            break  # 跳出循环\n",
    "    if flag:  # 同上\n",
    "        break\n",
    "# ======================================================Train End==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x_pad, y_pad, batch_size):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x_pad)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    # np.arange()生成0到data_len的等差数列，默认等差为1；np.random.permutation()打乱生成的等差序列的顺序\n",
    "    # 下面三句语句是为了将训练或测试文本的顺序打乱，因为原文本中每个分类的样本全部挨在一起，这样每个batch训练的都是同一个分类，不太好，打乱后每个batch可包含不同分类\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x_pad[indices]\n",
    "    y_shuffle = y_pad[indices]\n",
    "\n",
    "    # 返回所有batch的数据\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "        \n",
    "        \n",
    "def evaluate(sess, x_pad, y_pad, loss1, acc1):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_pad)\n",
    "    batch_eval = batch_iter(x_pad, y_pad, batch_size)  # 128\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    # print(dropout_keep_prob)\n",
    "    for x_batch1, y_batch1 in batch_eval:\n",
    "        batch_len = len(x_batch1)\n",
    "        feed_dict1 = {input_x: x_batch1, input_y: y_batch1, keep_prob: 1.0}\n",
    "        lossTmp, accTmp = sess.run([loss1, acc1], feed_dict=feed_dict1)\n",
    "        total_loss += lossTmp * batch_len\n",
    "        total_acc += accTmp * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf18",
   "language": "python",
   "name": "ts_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
