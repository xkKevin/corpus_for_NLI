{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe!\n",
      "Completed!\n",
      "WARNING:tensorflow:From <ipython-input-1-706d0c86dcf2>:88: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from D:\\lab_data\\之江lab\\AIforVis\\model\\c1_cnn_2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "vocabPath = r\"D:\\lab_data\\之江lab\\AIforVis\\model\\glove.6B.100d.txt\"\n",
    "savePath = r\"D:\\lab_data\\之江lab\\AIforVis\\model\\c1_cnn_2\"\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    print('Loading GloVe!')\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append([float(ei) for ei in row[1:]])\n",
    "    file.close()\n",
    "    print('Completed!')\n",
    "    return vocab, embd\n",
    "\n",
    "\n",
    "categories = ['Retrieve Value', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Sort',\n",
    "              'Determine Range', 'Characterize Distribution', 'Find Anomalies', 'Cluster', 'Correlate']\n",
    "num_classes = len(categories)\n",
    "seq_length = 35\n",
    "\n",
    "vocab, embd = loadGloVe(vocabPath)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "word_to_id = dict(zip(vocab, range(vocab_size)))\n",
    "\n",
    "# ======================================================CNN Model Start===============================================\n",
    "# 输入内容及对应的标签\n",
    "input_x = tf.placeholder(tf.int32, [None, seq_length], name='input_x')\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "# dropout的损失率\n",
    "keep_prob = tf.placeholder(tf.float64, name='keep_prob')\n",
    "\n",
    "# 词向量映射;实际上此处的词向量并不是用的预训练好的词向量，而是未经任何训练直接生成了一个矩阵，将此矩阵作为词向量矩阵使用，效果也还不错。\n",
    "# 若使用训练好的词向量，或许训练此次文本分类的模型时会更快，更好。\n",
    "# embedding = tf.get_variable('embedding', [vocab_size, embedding_dim])\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding, input_x)\n",
    "\n",
    "num_filters = 256\n",
    "kernel_size = 5\n",
    "hidden_dim = 128\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "print_per_batch = 20  # 每多少轮输出一次结果\n",
    "# save_per_batch = 5  # 每多少轮存入tensorboard\n",
    "\n",
    "\n",
    "# CNN layer\n",
    "conv = tf.layers.conv1d(embedding_inputs, num_filters, kernel_size, name='conv')  # num_filters = 256 这是个啥\n",
    "''' https://blog.csdn.net/khy19940520/article/details/89934335\n",
    "tf.layers.conv1d：一维卷积一般用于处理文本数据，常用语自然语言处理中，输入一般是文本经过embedding的二维数据。\n",
    "    inputs： 输入tensor， 维度(batch_size, seq_length, embedding_dim) 是一个三维的tensor；其中，\n",
    "        batch_size指每次输入的文本数量；\n",
    "        seq_length指每个文本的词语数或者单字数；\n",
    "        embedding_dim指每个词语或者每个字的向量长度；\n",
    "        例如每次训练输入2篇文本，每篇文本有100个词，每个词的向量长度为20，那input维度即为(2, 100, 20)。\n",
    "    filters：过滤器（卷积核）的数目\n",
    "    kernel_size：卷积核的大小，卷积核本身应该是二维的，这里只需要指定一维，因为第二个维度即长度与词向量的长度一致，卷积核只能从上往下走，不能从左往右走，即只能按照文本中词的顺序，也是列的顺序。\n",
    "'''\n",
    "# global max pooling layer\n",
    "gmp = tf.reduce_max(conv, reduction_indices=[1],\n",
    "                    name='gmp')  # https://blog.csdn.net/lllxxq141592654/article/details/85345864\n",
    "\n",
    "# 全连接层，后面接dropout以及relu激活\n",
    "fc = tf.layers.dense(gmp, hidden_dim, name='fc1')  # hidden_dim：128\n",
    "''' https://blog.csdn.net/yangfengling1023/article/details/81774580\n",
    "dense ：全连接层  inputs：输入该网络层的数据；units：输出的维度大小，改变inputs的最后一维\n",
    "'''\n",
    "fc = tf.nn.dropout(fc, keep_prob)\n",
    "fc = tf.nn.relu(fc)\n",
    "\n",
    "# 分类器\n",
    "logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)  # 预测类别 tf.argmax：返回每一行或每一列的最大值 1为里面（每一行），0为外面（每一列）\n",
    "\n",
    "# 损失函数，交叉熵\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# 优化器\n",
    "optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 准确率\n",
    "correct_pred = tf.equal(tf.argmax(input_y, 1), y_pred_cls)\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# ======================================================CNN Model End============================================\n",
    "\n",
    "# 创建session\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver.restore(sess=session, save_path=savePath)\n",
    "\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    new_sent = ''\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] in string.punctuation:\n",
    "            if i > 0 and i < len(sent) - 1:\n",
    "                if sent[i - 1] != ' ':\n",
    "                    new_sent += ' ' + sent[i]\n",
    "                elif sent[i + 1] != ' ':\n",
    "                    new_sent += sent[i] + ' '\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "            elif i == 0:\n",
    "                if sent[i + 1] != ' ':\n",
    "                    new_sent += sent[i] + ' '\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "            else:\n",
    "                if sent[i - 1] != ' ':\n",
    "                    new_sent += ' ' + sent[i]\n",
    "                else:\n",
    "                    new_sent += sent[i]\n",
    "        else:\n",
    "            new_sent += sent[i]\n",
    "    return new_sent.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentences, pad_max_length=seq_length):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "\n",
    "    data_id = []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for i in range(len(predict_sentences)):\n",
    "        data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n",
    "    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n",
    "    pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        sequences：浮点数或整数构成的两层嵌套列表\n",
    "        maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "        dtype：返回的numpy array的数据类型\n",
    "        padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "        truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "        value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    '''\n",
    "    feed_dict = {\n",
    "        input_x: x_pad,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "    predict_result = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "    predict_result = [i + 1 for i in predict_result]\n",
    "    return predict_result\n",
    "\n",
    "\n",
    "def predict11(predict_sentences, probability_threshold=0.3, pad_max_length=seq_length):\n",
    "    \"\"\"\n",
    "    将文件转换为id表示,并且将每个单独的样本长度固定为pad_max_lengtn\n",
    "    \"\"\"\n",
    "\n",
    "    data_id = []\n",
    "    # 将文本内容转换为对应的id形式\n",
    "    for i in range(len(predict_sentences)):\n",
    "        data_id.append([word_to_id[x] for x in predict_sentences[i].lower().strip().split() if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, pad_max_length)\n",
    "    ''' https://blog.csdn.net/TH_NUM/article/details/80904900\n",
    "    pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) \n",
    "        sequences：浮点数或整数构成的两层嵌套列表\n",
    "        maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "        dtype：返回的numpy array的数据类型\n",
    "        padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "        truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "        value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    '''\n",
    "    feed_dict = {\n",
    "        input_x: x_pad,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "    predict_result = session.run(tf.nn.softmax(logits), feed_dict=feed_dict)\n",
    "    print(predict_result)\n",
    "    result = []\n",
    "    for i in predict_result:\n",
    "        if max(i) > probability_threshold:\n",
    "            result.append(i.argmax()+1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentences = [\"In the sixtieth ceremony , where were all of the winners from ?\",  # 7\n",
    "                     \"On how many devices has the app \\\" CF SHPOP ! \\\" been installed ?\",  # 1\n",
    "                     \"List center - backs by what their transfer _ fee was .\"]  # 5\n",
    "\n",
    "# print(predict(predict_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.45090617e-02, 1.23237253e-01, 7.14768966e-04, 1.88490579e-02,\n",
       "        1.20412807e-01, 5.54553983e-02, 5.85205611e-01, 8.03466736e-04,\n",
       "        3.26596466e-04, 4.85979236e-04],\n",
       "       [9.99991166e-01, 2.53736569e-07, 7.45170653e-06, 3.93062304e-07,\n",
       "        5.27571381e-10, 1.18413987e-07, 9.59816961e-08, 5.07930054e-10,\n",
       "        2.01731920e-07, 3.17914399e-07],\n",
       "       [3.32020655e-02, 8.11915295e-03, 9.67328274e-03, 6.49981063e-03,\n",
       "        7.19423535e-01, 1.74614078e-01, 6.27792668e-04, 3.57554671e-04,\n",
       "        4.74083149e-02, 7.44134907e-05]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11(predict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array([[9.45090617e-02, 1.23237253e-01, 7.14768966e-04, 1.88490579e-02,\n",
    "        1.20412807e-01, 5.54553983e-02, 5.85205611e-01, 8.03466736e-04,\n",
    "        3.26596466e-04, 4.85979236e-04],\n",
    "       [9.99991166e-01, 2.53736569e-07, 7.45170653e-06, 3.93062304e-07,\n",
    "        5.27571381e-10, 1.18413987e-07, 9.59816961e-08, 5.07930054e-10,\n",
    "        2.01731920e-07, 3.17914399e-07],\n",
    "       [3.32020655e-02, 8.11915295e-03, 9.67328274e-03, 6.49981063e-03,\n",
    "        7.19423535e-01, 1.74614078e-01, 6.27792668e-04, 3.57554671e-04,\n",
    "        4.74083149e-02, 7.44134907e-05]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([12,17,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7d29b227aad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "max(result[0]),result[0].index(max(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    if max(i) > 0.7:\n",
    "        print(i.argmax()+1)\n",
    "    else:\n",
    "        print(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the sixtieth ceremony , where were all of the winners from ?',\n",
       " 'On how many devices has the app \" CF SHPOP ! \" been installed ?',\n",
       " 'List center - backs by what their transfer _ fee was .']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.09907288e-04 2.04830967e-01 4.59961172e-04 6.39203623e-01\n",
      "  8.33358360e-02 2.98951811e-02 3.76134953e-02 9.60751354e-04\n",
      "  5.64579871e-04 2.62569765e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11([\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.20806431e-02 2.44805163e-02 3.71637749e-03 6.49520190e-03\n",
      "  2.03403166e-05 8.72867751e-01 8.55812662e-05 6.47232636e-05\n",
      "  1.66088055e-04 2.27769668e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11([\"good bad R I mm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.08122986e-01 5.46142299e-01 1.09633785e-04 2.12000632e-02\n",
      "  2.54067842e-04 1.13652993e-02 1.23952700e-02 3.14969231e-05\n",
      "  5.30361397e-05 3.25848737e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11([\"i love china !\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 1, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11(predict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 1, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict11(predict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2*x if x else -1 for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'UNK'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d81bedeb7936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_to_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UNK'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sandberger'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'UNK'"
     ]
    }
   ],
   "source": [
    "word_to_id['UNK'], word_to_id['sandberger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.258,\n",
       " -0.068239,\n",
       " -0.1293,\n",
       " 0.40934,\n",
       " 0.24704,\n",
       " -0.30138,\n",
       " 0.64817,\n",
       " -0.39733,\n",
       " 0.098767,\n",
       " 0.27254,\n",
       " -0.22292,\n",
       " 0.29851,\n",
       " 0.38641,\n",
       " -0.20509,\n",
       " 0.13445,\n",
       " 0.1114,\n",
       " -0.065329,\n",
       " 0.60735,\n",
       " 0.42454,\n",
       " -0.16439,\n",
       " -0.42364,\n",
       " -0.014459,\n",
       " -0.49806,\n",
       " -0.084324,\n",
       " -0.53053,\n",
       " -0.14708,\n",
       " -0.14706,\n",
       " -0.19774,\n",
       " -0.065882,\n",
       " 0.32439,\n",
       " 0.55762,\n",
       " 0.3363,\n",
       " 0.35643,\n",
       " -0.16911,\n",
       " 0.29504,\n",
       " -0.41179,\n",
       " -0.033898,\n",
       " -0.34218,\n",
       " 0.0972,\n",
       " -0.14092,\n",
       " -0.063052,\n",
       " -0.080234,\n",
       " 0.059456,\n",
       " -0.050595,\n",
       " -0.42402,\n",
       " 0.41918,\n",
       " -0.0025027,\n",
       " 0.35303,\n",
       " -0.070322,\n",
       " 0.43291,\n",
       " -0.26104,\n",
       " 0.04959,\n",
       " -0.30767,\n",
       " 0.19803,\n",
       " 0.41325,\n",
       " 1.0292,\n",
       " -0.3959,\n",
       " -0.014833,\n",
       " -0.3658,\n",
       " -0.47339,\n",
       " 0.12888,\n",
       " -0.45944,\n",
       " 0.27612,\n",
       " 0.11627,\n",
       " -0.40329,\n",
       " 0.21118,\n",
       " -0.38505,\n",
       " -0.1359,\n",
       " -0.36774,\n",
       " 0.013439,\n",
       " 0.81402,\n",
       " 0.23368,\n",
       " 0.080804,\n",
       " 0.54235,\n",
       " 0.39422,\n",
       " -0.18939,\n",
       " 0.34036,\n",
       " -0.37009,\n",
       " 0.71362,\n",
       " -0.46628,\n",
       " -0.50555,\n",
       " -0.16086,\n",
       " 0.26761,\n",
       " -0.49872,\n",
       " 0.61128,\n",
       " -0.029987,\n",
       " 0.11944,\n",
       " -0.23577,\n",
       " -0.45469,\n",
       " 0.032653,\n",
       " 0.035162,\n",
       " 0.26197,\n",
       " 0.42505,\n",
       " -0.032472,\n",
       " 0.64324,\n",
       " -0.1462,\n",
       " -0.076223,\n",
       " 0.23424,\n",
       " -0.42436,\n",
       " 0.15058]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd[299999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sandberger'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"sdf\":12, \"2\":[1,3]}\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'int64' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b282dd701304>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'int64' is not defined"
     ]
    }
   ],
   "source": [
    "aa = np.array([2],dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1\n",
    "aa = int(json.dumps(a,cls=NpEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf18",
   "language": "python",
   "name": "ts_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
