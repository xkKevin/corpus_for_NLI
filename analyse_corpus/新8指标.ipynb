{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "# import Levenshtein"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Indices(object):\n",
    "    def __init__(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "        self.sentence_len = []\n",
    "\n",
    "        self.levenshtein = []\n",
    "        self.bleu_score = []\n",
    "        \n",
    "        self.uni_overlap = []\n",
    "        self.bi_overlap = []\n",
    "        self.tri_overlap = []\n",
    "\n",
    "        self.uni_set = []\n",
    "        self.bi_set = []\n",
    "        self.tri_set = []\n",
    "        \n",
    "    \n",
    "    # caluclate the ngram of the words\n",
    "    #\n",
    "    # @param    words       list{str}\n",
    "    # @return   none\n",
    "    def ngram(self, words):\n",
    "        wlen = len(words)\n",
    "                  \n",
    "        for i in range(wlen):\n",
    "            word = words[i]\n",
    "                  \n",
    "            if word not in self.unigram:\n",
    "                self.unigram[word] = 1\n",
    "            else:\n",
    "                self.unigram[word] = self.unigram[word] + 1\n",
    "            \n",
    "            if i < (wlen - 1):\n",
    "                bi_words = ' '.join([word, words[i+1]])\n",
    "                if bi_words not in self.bigram:\n",
    "                    self.bigram[bi_words] = 1\n",
    "                else:\n",
    "                    self.bigram[bi_words] = self.bigram[bi_words] + 1\n",
    "                \n",
    "                if i < (wlen - 2):\n",
    "                    tri_words = ' '.join([word, words[i+1], words[i+2]])\n",
    "                    if tri_words not in self.trigram:\n",
    "                        self.trigram[tri_words] = 1\n",
    "                    else:\n",
    "                        self.trigram[tri_words] = self.trigram[tri_words] + 1\n",
    "                        \n",
    "        \n",
    "    def lev_dist(self, source, target):\n",
    "        '''\n",
    "            Compute Levenshtein Distance, Edit Distance\n",
    "        '''\n",
    "        len_source = len(source)\n",
    "        len_target = len(target)\n",
    "        edit = [[i + j for j in range(len_target + 1)] for i in range(len_source + 1)]\n",
    "        for i in range(1, len_source + 1):\n",
    "            for j in range(1, len_target + 1):\n",
    "                if source[i - 1] == target[j - 1]:\n",
    "                    d = 0\n",
    "                else:\n",
    "                    d = 1\n",
    "                edit[i][j] = min(edit[i - 1][j] + 1, edit[i][j - 1] + 1, edit[i - 1][j - 1] + d)\n",
    "\n",
    "        max_len = max(len_source, len_target)\n",
    "        edit_score = edit[len_source][len_target]\n",
    "        return (max_len - edit_score)/ max_len\n",
    "\n",
    "\n",
    "    def overlap(self, s_num):\n",
    "        for si in range(s_num - 1):\n",
    "            for sj in range(si + 1, s_num):\n",
    "                try:\n",
    "                    self.uni_overlap.append(len(self.uni_set[si] & self.uni_set[sj]) / len(self.uni_set[si] | self.uni_set[sj]))\n",
    "                    self.bi_overlap.append(len(self.bi_set[si] & self.bi_set[sj]) / len(self.bi_set[si] | self.bi_set[sj]))\n",
    "                    self.tri_overlap.append(len(self.tri_set[si] & self.tri_set[sj]) / len(self.tri_set[si] | self.tri_set[sj]))\n",
    "                except Exception as e:\n",
    "                    print(self.uni_set[si], self.uni_set[sj])\n",
    "                    print(self.bi_set[si], self.bi_set[sj])\n",
    "                    print(self.tri_set[si], self.tri_set[sj])\n",
    "                    \n",
    "                    \n",
    "class AnalyseCorpus(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus  # corpus: {1:[], 2:[]}\n",
    "        self.result = {}\n",
    "\n",
    "\n",
    "    def analyse(self):\n",
    "        for taski, ci in self.corpus.items():  # 每一个task的sentences：ci\n",
    "            indices = Indices()\n",
    "            \n",
    "            s_num = len(ci)   # 此task下共有多少句sentence\n",
    "            for si in range(s_num):\n",
    "                words = ci[si].split()\n",
    "                indices.sentence_len.append(len(words))\n",
    "\n",
    "                indices.uni_set.append(set(nltk.ngrams(words,1)))\n",
    "                indices.bi_set.append(set(nltk.ngrams(words,2)))\n",
    "                indices.tri_set.append(set(nltk.ngrams(words,3)))\n",
    "\n",
    "                indices.ngram(words)\n",
    "                if si < s_num - 1:\n",
    "                    for sj in range(si + 1, s_num):\n",
    "                        words2 = ci[sj].split()\n",
    "                        indices.levenshtein.append(indices.lev_dist(words, words2))\n",
    "                        indices.bleu_score.append(nltk.translate.bleu_score.sentence_bleu([words], words2, weights = (0.5, 0.5)))\n",
    "\n",
    "            indices.overlap(s_num)\n",
    "            self.result[taski] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyseCorpus(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus  # corpus: {1:[], 2:[]}\n",
    "        self.analyse = {}\n",
    "        self.result = {}\n",
    "\n",
    "    def analyse_task(self):\n",
    "        for taski, ci in self.corpus.items():  # 每一个task的sentences：ci\n",
    "            self.analyse[taski] = {\"unigram\":set(), \"bigram\":set(), \"trigram\":set(), \"sentence_len\":[], \"sentence_num\":0, \"bleu_score\":[]}\n",
    "            \n",
    "            s_num = len(ci)   # 此task下共有多少句sentence\n",
    "            self.analyse[taski][\"sentence_num\"] = s_num\n",
    "            \n",
    "            for si in range(s_num):\n",
    "                words = ci[si].split()\n",
    "                self.analyse[taski][\"sentence_len\"].append(len(words))\n",
    "\n",
    "                self.analyse[taski][\"unigram\"] = self.analyse[taski][\"unigram\"].union(set(nltk.ngrams(words,1)))\n",
    "                self.analyse[taski][\"bigram\"] = self.analyse[taski][\"bigram\"].union(set(nltk.ngrams(words,2)))\n",
    "                self.analyse[taski][\"trigram\"] = self.analyse[taski][\"trigram\"].union(set(nltk.ngrams(words,3)))\n",
    "\n",
    "                if si < s_num - 1:\n",
    "                    for sj in range(si + 1, s_num):\n",
    "                        words2 = ci[sj].split()\n",
    "                        self.analyse[taski][\"bleu_score\"].append(nltk.translate.bleu_score.sentence_bleu([words], words2, weights = (0.5, 0.5)))\n",
    "                        \n",
    "            self.result[taski] = {\n",
    "                \"unigram\": len(self.analyse[taski][\"unigram\"]),\n",
    "                \"bigram\": len(self.analyse[taski][\"bigram\"]),\n",
    "                \"trigram\": len(self.analyse[taski][\"trigram\"]),\n",
    "                \"sentence_len\": self.analyse[taski][\"sentence_len\"],\n",
    "                \"sentence_num\": self.analyse[taski][\"sentence_num\"],\n",
    "                \"bleu_score\": sum(self.analyse[taski][\"bleu_score\"])/len(self.analyse[taski][\"bleu_score\"])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: []}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_sentences = {i:[] for i in range(1,11)}\n",
    "tasks_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus最终版\\corpus_5.txt\", \"r\", encoding='utf-8') as fp:\n",
    "    for line in fp.readlines():\n",
    "        lsp = line.split()\n",
    "        info = lsp[0]\n",
    "        sentence = \" \".join(lsp[1:])\n",
    "        task = int(info.split(\":\")[0])\n",
    "        if (int(info.split(\":\")[2])):\n",
    "            tasks_sentences[task].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the population of arkansas on july 1st 2002 ?',\n",
       " 'where is harvard university located ?']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_sentences[1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\study\\python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "analyse_corpus = AnalyseCorpus(tasks_sentences)\n",
    "analyse_corpus.analyse_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Corpus_ExpertAnalysisResult920.json','w',encoding='utf-8') as fObj:\n",
    "    json.dump(analyse_corpus.result,fObj,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 651 705\n",
      "353 761 832\n"
     ]
    }
   ],
   "source": [
    "print(len(analyse_corpus.result[1][\"unigram\"]),len(analyse_corpus.result[1][\"bigram\"]),len(analyse_corpus.result[1][\"trigram\"]))\n",
    "print(len(analyse_corpus.result[2][\"unigram\"]),len(analyse_corpus.result[2][\"bigram\"]),len(analyse_corpus.result[2][\"trigram\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for taski, ri in analyse_corpus.result.items():\n",
    "    result[taski] = {\"unigram\": ri.unigram, \"bigram\": ri.bigram, \"trigram\": ri.trigram, \"sentences_len\": ri.sentence_len, \n",
    "                     \"levenshtein\": ri.levenshtein, \"bleu_score\": ri.bleu_score, \n",
    "                     \"uni_overlap\": ri.uni_overlap, \"bi_overlap\": ri.bi_overlap, \"tri_overlap\": ri.tri_overlap}\n",
    "with open('./CorpusAnalysisResult_.json','w',encoding='utf-8') as fObj:\n",
    "    json.dump(result,fObj,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = set(nltk.ngrams(\"what is the population of arkansas on july 1st 2002 ?\".split(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 52}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "aa = aa.union({1,52})\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [1,2,3]\n",
    "sum(aa) / len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa={1,2,3}\n",
    "bb = {1,5,3}\n",
    "aa.union(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1, 2, 3}, {1, 3, 5})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa,bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"sd\": 12}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(\"what is the population of arkansas on july 1st 2002 ?\".split(),1))\n",
    "json.dumps({\"sd\":12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1585333431.0908127"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585333488.6645074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=58)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(end_time)\n",
    "timedelta(seconds=int(round(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:57.573695\n",
      "0:00:58\n"
     ]
    }
   ],
   "source": [
    "print(timedelta(seconds = end_time - start_time))\n",
    "print(timedelta(seconds=int(round(end_time - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:13.320638\n",
      "0:03:13\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(timedelta(seconds = end_time - start_time))\n",
    "print(timedelta(seconds=int(round(end_time - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.32063817977905"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
