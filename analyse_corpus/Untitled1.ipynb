{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_v1 = pd.read_excel(\"corpus_v1.0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interviewer</th>\n",
       "      <th>Interviewee</th>\n",
       "      <th>Task</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Data Related</th>\n",
       "      <th>Table</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xiong Kai</td>\n",
       "      <td>WJC</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the popularity of Arkansas on July 1st...</td>\n",
       "      <td>popularity, Arkansas, July 1st 2002</td>\n",
       "      <td>co-est2002-01-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xiong Kai</td>\n",
       "      <td>WJC</td>\n",
       "      <td>1</td>\n",
       "      <td>Where is Harvard University?</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>top-50-us-universities</td>\n",
       "      <td>where 表示 location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xiong Kai</td>\n",
       "      <td>WJC</td>\n",
       "      <td>1</td>\n",
       "      <td>How many faculties are there in Harvard Univer...</td>\n",
       "      <td>faculties, Harvard Universtiy</td>\n",
       "      <td>top-50-us-universities</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xiong Kai</td>\n",
       "      <td>WJC</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the type of Harvard University?</td>\n",
       "      <td>Harvard University, type</td>\n",
       "      <td>top-50-us-universities</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xiong Kai</td>\n",
       "      <td>WJC</td>\n",
       "      <td>2</td>\n",
       "      <td>which county has more than 30000 popularity?</td>\n",
       "      <td>county, 30000 popularity</td>\n",
       "      <td>co-est2002-01-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Ge Xiaodong</td>\n",
       "      <td>Tang Junxiu</td>\n",
       "      <td>8</td>\n",
       "      <td>Show me the outlier of trip duration between t...</td>\n",
       "      <td>trip duration, stations</td>\n",
       "      <td>bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Ge Xiaodong</td>\n",
       "      <td>Tang Junxiu</td>\n",
       "      <td>9</td>\n",
       "      <td>Cluster the trip duration according to start s...</td>\n",
       "      <td>trip duration, start stations</td>\n",
       "      <td>bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Ge Xiaodong</td>\n",
       "      <td>Tang Junxiu</td>\n",
       "      <td>9</td>\n",
       "      <td>Is there any clusters of trip durations?</td>\n",
       "      <td>trip durations</td>\n",
       "      <td>bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Ge Xiaodong</td>\n",
       "      <td>Tang Junxiu</td>\n",
       "      <td>10</td>\n",
       "      <td>Is there any trend of the using frequency of b...</td>\n",
       "      <td>using frequency, bikes</td>\n",
       "      <td>bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Ge Xiaodong</td>\n",
       "      <td>Tang Junxiu</td>\n",
       "      <td>10</td>\n",
       "      <td>How about the correlation of age and trip dura...</td>\n",
       "      <td>age, duration</td>\n",
       "      <td>bike</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>895 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Interviewer  Interviewee Task  \\\n",
       "0      Xiong Kai          WJC    1   \n",
       "1      Xiong Kai          WJC    1   \n",
       "2      Xiong Kai          WJC    1   \n",
       "3      Xiong Kai          WJC    1   \n",
       "4      Xiong Kai          WJC    2   \n",
       "..           ...          ...  ...   \n",
       "890  Ge Xiaodong  Tang Junxiu    8   \n",
       "891  Ge Xiaodong  Tang Junxiu    9   \n",
       "892  Ge Xiaodong  Tang Junxiu    9   \n",
       "893  Ge Xiaodong  Tang Junxiu   10   \n",
       "894  Ge Xiaodong  Tang Junxiu   10   \n",
       "\n",
       "                                              Sentence  \\\n",
       "0    What is the popularity of Arkansas on July 1st...   \n",
       "1                         Where is Harvard University?   \n",
       "2    How many faculties are there in Harvard Univer...   \n",
       "3              What is the type of Harvard University?   \n",
       "4         which county has more than 30000 popularity?   \n",
       "..                                                 ...   \n",
       "890  Show me the outlier of trip duration between t...   \n",
       "891  Cluster the trip duration according to start s...   \n",
       "892           Is there any clusters of trip durations?   \n",
       "893  Is there any trend of the using frequency of b...   \n",
       "894  How about the correlation of age and trip dura...   \n",
       "\n",
       "                            Data Related                   Table  \\\n",
       "0    popularity, Arkansas, July 1st 2002        co-est2002-01-05   \n",
       "1                     Harvard University  top-50-us-universities   \n",
       "2          faculties, Harvard Universtiy  top-50-us-universities   \n",
       "3               Harvard University, type  top-50-us-universities   \n",
       "4               county, 30000 popularity        co-est2002-01-05   \n",
       "..                                   ...                     ...   \n",
       "890              trip duration, stations                    bike   \n",
       "891        trip duration, start stations                    bike   \n",
       "892                       trip durations                    bike   \n",
       "893               using frequency, bikes                    bike   \n",
       "894                        age, duration                    bike   \n",
       "\n",
       "                  Note  \n",
       "0                  NaN  \n",
       "1    where 表示 location  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "..                 ...  \n",
       "890                NaN  \n",
       "891                NaN  \n",
       "892                NaN  \n",
       "893                NaN  \n",
       "894                NaN  \n",
       "\n",
       "[895 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_v1[(corpus_v1.Task == '1') | (corpus_v1.Task == 1) | corpus_v1.Task.str.contains('1,', case=False, na=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interviewer                                           Ge Xiaodong\n",
       "Interviewee                                           Deng Dazhen\n",
       "Task                                                            1\n",
       "Sentence        Retrive the neighborhood name with the boy bir...\n",
       "Data Related        neighborhood name, boy birth number, 56, 2017\n",
       "Table                                                      births\n",
       "Note                                                          NaN\n",
       "Name: 491, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_v1.loc[491]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_sentences = []\n",
    "tasks_len = []\n",
    "for i in range(1,11):\n",
    "    # tasks_sentences.append([i,str(i),str(i)+','])\n",
    "    tasks_sentences.append(dict(corpus_v1[(corpus_v1.Task == i) | (corpus_v1.Task == str(i)) | corpus_v1.Task.str.contains(str(i) + ',', case=False, na=False)].Sentence))\n",
    "    # print(len(tasks_sentences[i-1]))\n",
    "    tasks_len.append(len(tasks_sentences[i-1]))\n",
    "    \n",
    "sum(tasks_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 405)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_v1), len(corpus_v1[corpus_v1.Interviewer == 'Ge Xiaodong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'What is the popularity of Arkansas on July 1st 2002?',\n",
       " 1: 'Where is Harvard University?',\n",
       " 2: 'How many faculties are there in Harvard Universtiy?',\n",
       " 3: 'What is the type of Harvard University?',\n",
       " 42: 'What is the happiness rank of Iceland?',\n",
       " 43: 'How many dimensions to describe happiness score?',\n",
       " 44: 'How many students in the grade records?',\n",
       " 45: \"Show me Kibo Mcgee's mid-term score.\",\n",
       " 83: 'What is the revenue of \"Titanic\"?',\n",
       " 84: 'How many goals does Liverpool score in the game?',\n",
       " 85: 'Find the score of course A for student B.',\n",
       " 86: 'How long would it take from location A to location B?',\n",
       " 128: 'What region does Iceland belong to?',\n",
       " 129: 'Which country is the 15th most happy one?',\n",
       " 130: 'How would people in Iceland rate theirs happiness?',\n",
       " 131: 'Which country is the next target, if Iceland want to enhance the happiness score?',\n",
       " 155: 'Which publisher published The Queen of Nothing?',\n",
       " 156: 'Find me books written by Brian Tracy.',\n",
       " 157: 'What language is The Art of Super Mario Odyssey written in?',\n",
       " 158: \"Is Barbara Allan's book highly rated?\",\n",
       " 160: 'How long is Beautiful People?',\n",
       " 161: \"What's the length of Beautiful People?\",\n",
       " 162: 'What pop songs has Sam Simth released?',\n",
       " 163: 'Could you give me some pop songs by Sam Simth?',\n",
       " 244: 'what is the current version of \"Coloring book moana\"',\n",
       " 245: 'tell me the size of \"Kids Paint Free - Drawing Fun\"',\n",
       " 260: 'display a random reason',\n",
       " 261: \"What kind of jobs does students' mother have?\",\n",
       " 270: 'How many likes does the video titled \"PLUSH - Bad Unboxing Fan Mail\" have?',\n",
       " 271: 'Does the video \"PLUSH - Bad Unboxing Fan Mail\" allow comments?',\n",
       " 272: 'What time was \"PLUSH - Bad Unboxing Fan Mail\" published?',\n",
       " 273: 'Since when has \"PLUSH - Bad Unboxing Fan Mail\" been trending?',\n",
       " 274: 'How many users liked the video \"PLUSH - Bad Unboxing Fan Mail\"?',\n",
       " 275: 'What is the date that \"PLUSH - Bad Unboxing Fan Mail\" became trending?',\n",
       " 276: 'Tell me the number of likes that \"PLUSH - Bad Unboxing Fan Mail\" receives',\n",
       " 277: 'What channel does \"PLUSH - Bad Unboxing Fan Mail\" belong to?',\n",
       " 278: 'Which channel published the video \"PLUSH - Bad Unboxing Fan Mail\"?',\n",
       " 279: 'Describe the video \"PLUSH - Bad Unboxing Fan Mail\".',\n",
       " 324: 'How many times has \"Coloring book moana\" been installed?',\n",
       " 325: 'Is \"Coloring book moana\" free or paid?',\n",
       " 345: 'Where was the movie Transformers Prime produced?',\n",
       " 346: 'I would like to know the rating of the movie Transformers Prime.',\n",
       " 366: 'How many people from Albania at the age of 15-24 years suicide in 1987?',\n",
       " 367: 'What is the GDP of Albania in 1987?',\n",
       " 387: 'What is the number of undergraduate students of United Kingdom in 2001?',\n",
       " 388: 'What is the percentage of underguaduate students of all countries in 2001?',\n",
       " 389: 'What are the income of undergraduates and postgraduates of United Kingdom in 2001 respectively?',\n",
       " 390: 'Where was the match hold between England and Wales on 1879-01-08?',\n",
       " 391: 'Find the place where the match hold between England and Walse on 1879-01-08.',\n",
       " 453: 'I would like to inquire of you about the value of Percentage Attempted To Quit Smoking in Ohio.',\n",
       " 454: 'How many people live in Hawaii?',\n",
       " 455: 'How many people smoke in Maryland?',\n",
       " 479: \"I am dying to know Lucas Mcgee's Mid-term exams result.\",\n",
       " 490: 'what is the number of birth of the boys in el Raval in 2017?',\n",
       " 491: 'Retrive the neighborhood name with the boy birth number 56 in 2017?',\n",
       " 510: 'Get the Market value of Fernando Torres in season 2010-2011',\n",
       " 511: 'Which player is with 50000000 market value and get transfered with 58500000 in season 2010-2011?',\n",
       " 530: 'how many widowed people live in the Urban?',\n",
       " 531: 'what is the total number of the single person in the Tanga?',\n",
       " 550: \"what's the age of David when he was transfered in 2010?\",\n",
       " 551: 'which team was the david traded from when he came to Premier League?',\n",
       " 570: 'List all the results in the table about the film \"About Schmidt\".',\n",
       " 571: 'Who won the \"Best Performance by an Actor in a Motion Picture - Drama\" in the 60th ceremony?',\n",
       " 590: 'What rating score has the APP \"CF.lumen\" won?',\n",
       " 591: 'How many installations does the APP \"CF SHOP!\" has?',\n",
       " 610: 'How many hospitals does Arusha have?',\n",
       " 611: \"What's the number of clinics in the neibor of Mara?\",\n",
       " 630: \"What's the gdp of China in 2000?\",\n",
       " 631: \"What' the number of male's suicides in China in 2000?\",\n",
       " 650: \"return xx school's CHANGE_PRE in 2006\",\n",
       " 651: 'What is the CHANGE_PRE of xx school in 2016?',\n",
       " 670: 'How many installs of xx app?',\n",
       " 671: 'What is the price of xx app?',\n",
       " 690: 'How many of the Orange County residents are Asian?',\n",
       " 691: 'Show the percentage of White population for Blount County, Alabama.',\n",
       " 713: 'What is the price of CF Life?',\n",
       " 714: 'When was Unity CF last updated?',\n",
       " 733: 'What is the number of people exposed to ALBINO in GEITA',\n",
       " 734: 'How many people are AUTISM in LINDI',\n",
       " 753: 'what is the PM10 reading in Sants at 2018/1/11 0:00',\n",
       " 754: 'get the geo-location of Sants',\n",
       " 773: 'how many hospitals are there in Manyara',\n",
       " 774: 'show the population of Mara',\n",
       " 793: 'how is the air quality in Barcelona - Sants on 2018/1/11',\n",
       " 794: 'where is Barcelona - Sants',\n",
       " 815: 'Which movie win the nomination Best Original Score - Motion Picture in 1997',\n",
       " 816: 'I want to know the release year of the movie The Hours',\n",
       " 835: 'Tell me the pass rate of school WAJA SPRINGS PR. SCHOOL in 2016.',\n",
       " 836: 'in 2016，what is the pass rate of private school WAJA SPRINGS PR. SCHOOL',\n",
       " 855: 'What is the sale volume of 4046 on 2015/8/30?',\n",
       " 856: 'How many 4225 has been sold on 2015/7/5',\n",
       " 875: 'Find the user type of trip id 17536702',\n",
       " 876: 'Retrieve the trip duration of trip id 17536710'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # n is the order of n-gram language model\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "        self.sentence_len = []\n",
    "\n",
    "    # scan a sentence, extract the ngram and update their\n",
    "    # frequence.\n",
    "    #\n",
    "    # @param    sentence    list{str}\n",
    "    # @return   none\n",
    "    def scan(self, sentences):\n",
    "        fip = \"\"\n",
    "        # file your code here\n",
    "        for sentence in sentences.values():\n",
    "            self.sentence_len.append(len(sentence.split()))  # sentence.strip().split(' ')\n",
    "            self.ngram(sentence.split())\n",
    "        # unigram\n",
    "        try:\n",
    "            fip = open(\"uni\", \"w\", encoding='utf-8')\n",
    "        except:\n",
    "            print(sys.stderr, \"failed to open data.uni\")\n",
    "        tmp = zip(self.unigram.values(),self.unigram.keys())\n",
    "        for i in sorted(tmp,reverse=True):\n",
    "            fip.write(\"%s %d\\n\" % (i[1], i[0]))\n",
    "            \n",
    "        # bigram\n",
    "        try:\n",
    "            fip = open(\"bi\", \"w\", encoding='utf-8')\n",
    "        except:\n",
    "            print(sys.stderr, \"failed to open data.bi\")\n",
    "        tmp = zip(self.bigram.values(),self.bigram.keys())\n",
    "        for i in sorted(tmp,reverse=True):\n",
    "            fip.write(\"%s %d\\n\" % (i[1], i[0]))\n",
    "            \n",
    "        # bigram\n",
    "        try:\n",
    "            fip = open(\"tri\", \"w\", encoding='utf-8')\n",
    "        except:\n",
    "            print(sys.stderr, \"failed to open data.tri\")\n",
    "        tmp = zip(self.trigram.values(),self.trigram.keys())\n",
    "        for i in sorted(tmp,reverse=True):\n",
    "            fip.write(\"%s %d\\n\" % (i[1], i[0]))\n",
    "        \n",
    "        try:\n",
    "            fip = open(\"len\", \"w\", encoding='utf-8')\n",
    "        except:\n",
    "            print(sys.stderr, \"failed to open data.len\")\n",
    "        fip.write(\"Sentences Num: %d\\n\" % (len(self.sentence_len)))\n",
    "        for i in self.sentence_len:\n",
    "            fip.write(\"%d, \" % (i))\n",
    "\n",
    "\n",
    "    # caluclate the ngram of the words\n",
    "    #\n",
    "    # @param    words       list{str}\n",
    "    # @return   none\n",
    "    def ngram(self, words):\n",
    "        wlen = len(words)\n",
    "                  \n",
    "        for i in range(wlen):\n",
    "            word = words[i]\n",
    "                  \n",
    "            if word not in self.unigram:\n",
    "                self.unigram[word] = 1\n",
    "            else:\n",
    "                self.unigram[word] = self.unigram[word] + 1\n",
    "            \n",
    "            if i < (wlen - 1):\n",
    "                bi_words = ' '.join([word, words[i+1]])\n",
    "                if bi_words not in self.bigram:\n",
    "                    self.bigram[bi_words] = 1\n",
    "                else:\n",
    "                    self.bigram[bi_words] = self.bigram[bi_words] + 1\n",
    "                \n",
    "                if i < (wlen - 2):\n",
    "                    tri_words = ' '.join([word, words[i+1], words[i+2]])\n",
    "                    if tri_words not in self.trigram:\n",
    "                        self.trigram[tri_words] = 1\n",
    "                    else:\n",
    "                        self.trigram[tri_words] = self.trigram[tri_words] + 1\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
